"""
Elliptic Curve Rational Point Search using LLL and Modular Methods

This module implements sophisticated algorithms for finding rational points on elliptic curves
using lattice reduction (LLL), Chinese Remainder Theorem (CRT), and 2-descent methods.
"""

# Standard library imports
import sys
import random
import itertools
import multiprocessing
import math
from math import floor, sqrt, gcd, ceil, log
from fractions import Fraction
from functools import reduce, lru_cache, partial
from operator import mul
from collections import namedtuple, Counter # <-- IMPORTED COUNTER
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed

# Third-party imports
from tqdm import tqdm
from colorama import Fore, Style

# SageMath imports
from sage.all import (
    QQ, ZZ, GF, PolynomialRing, EllipticCurve,
    matrix, vector, identity_matrix, zero_matrix, diagonal_matrix,
    crt, lcm, sqrt, polygen, Integer, ceil
)
from sage.rings.rational import Rational
from sage.rings.fraction_field_element import FractionFieldElement

# Local imports (assuming these exist in your project)
from search_common import *
from search_common import DEBUG, PROFILE
from stats import * # <-- ADDED IMPORT

# Constants
DEFAULT_MAX_CACHE_SIZE = 10000
DEFAULT_MAX_DENOMINATOR_BOUND = None
FALLBACK_MATRIX_WARNING = "WARNING: LLL reduction failed, falling back to identity matrix"
ROOTS_THRESHOLD = 12 # only multiply primes' root counts into the estimate when the total roots for that prime exceed this threshold


# Practical tuning knobs
LLL_DELTA = 0.98           # strong LLL reduction; reduce if it slows too much (0.9--0.98 recommended)
BKZ_BLOCK = 12             # try BKZ with this block; lower for speed, larger for quality
MAX_COL_SCALE = 10**6      # don't scale any column by more than this (keeps integers reasonable)
TARGET_COLUMN_NORM = 1e6   # target column norm after scaling (heuristic)
MAX_K_ABS = 500            # ignore multiplier indices |k| > MAX_K_ABS when building mults
TRUNCATE_MAX_DEG = 30      # truncate polynomial coefficients at this degree to limit dimension
PARALLEL_PRIME_WORKERS = min(8, max(1, multiprocessing.cpu_count() // 2))
TMAX = 500

# ==============================================================
# === Auto-Tune / Residue Filter Parameters ====================
# ==============================================================

EXTRA_PRIME_TARGET_DENSITY = 1e-5   # desired survivor fraction after extras
EXTRA_PRIME_MAX = 6                 # cap on number of extra primes
EXTRA_PRIME_SKIP = {2, 3}        # avoid small degenerates
EXTRA_PRIME_SAMPLE_SIZE = 300       # sample vectors for stats
EXTRA_PRIME_MIN_R = 1e-4            # ignore primes with r_p < this
EXTRA_PRIME_MAX_R = 0.9             # ignore primes with r_p > this


def _compute_column_norms(M):
    """
    Compute L2 norm per column of integer matrix M (sage matrix).
    Returns list of floats.
    """
    col_norms = []
    for j in range(M.ncols()):
        s = 0
        for i in range(M.nrows()):
            v = int(M[i, j])
            s += v * v
        col_norms.append(float(s**0.5))
    return col_norms

def _compute_integer_scales_for_columns(M, target_norm=TARGET_COLUMN_NORM, max_scale=MAX_COL_SCALE):
    """
    Return integer scale factors (one per column) so that after multiplying column j by scales[j],
    its norm is approximately target_norm (but not exceeding max_scale).
    """
    norms = _compute_column_norms(M)
    scales = []
    for norm in norms:
        if norm <= 1e-12:
            scales.append(1)
            continue
        scale = int(ceil(target_norm / max(1.0, norm)))
        if scale < 1:
            scale = 1
        if scale > max_scale:
            scale = max_scale
        scales.append(int(scale))
    return scales

def _scale_matrix_columns_int(M, scales):
    """
    Return M_scaled = M * D where D = diag(scales), scales are ints.
    """
    D = diagonal_matrix([ZZ(s) for s in scales])
    return M * D, D


def _trim_poly_coeffs(coeff_list, max_deg=TRUNCATE_MAX_DEG):
    """Truncate coefficient list (low->high) to length max_deg+1."""
    if len(coeff_list) <= max_deg + 1:
        return coeff_list
    # Keep low-degree coefficients (assumed stored as [c0, c1, ..., cN])
    return coeff_list[: max_deg + 1]


class EllipticCurveSearchError(Exception):
    """Custom exception for elliptic curve search operations."""
    pass


class RationalReconstructionError(Exception):
    """Exception raised when rational reconstruction fails."""
    pass


# ---------- Utility: archimedean height on QQ elements ----------
def archimedean_height_QQ(x):
    """
    A simple archimedean height for a QQ element x = a/b in lowest terms.
    Returns log(max(|a|, |b|, 1)).
    """
    # Expect x to be a Sage QQ or Python Fraction
    try:
        a = Integer(x.numerator())
        b = Integer(x.denominator())
    except (AttributeError, TypeError) as e:
        # If x is not rational (e.g., symbolic), raise so caller handles it.
        raise TypeError("archimedean_height_QQ expects a rational (QQ) input") from e

    val = max(abs(int(a)), abs(int(b)), 1)
    return math.log(val)

# ---------- Utility: local search for best t ----------
def minimize_archimedean_t(m0, M, r_m_func, shift, tmax, max_steps=150, patience=6):
    """
    Given residue class m = m0 (mod M), search over m = m0 + t*M to find integer t that minimizes
    archimedean height of x = r_m(m) - shift.

    Returns list of (m_candidate (QQ), score (float)) for the best few t values.
    """
    best = []

    def eval_for_t(t):
        m_candidate = m0 + t * M
        if abs(t) > tmax:
            return None
        try:
            x_val = r_m_func(m=QQ(m_candidate)) - shift
            if not (hasattr(x_val, 'numerator') and hasattr(x_val, 'denominator')):
                return None
            score = archimedean_height_QQ(x_val)
            return (QQ(m_candidate), float(score))
        except (ZeroDivisionError, TypeError, ArithmeticError):
            print("we're here, for some reason")
            return None

    center = eval_for_t(0)
    if center is not None:
        best.append(center)

    steps = 0
    no_improve = 0
    current_best_score = best[0][1] if best else float('inf')
    t = 1
    while steps < max_steps and no_improve < patience:
        for s in (t, -t):
            res = eval_for_t(s)
            steps += 1
            if res is None:
                continue
            m_cand, score = res
            best.append((m_cand, score))
            if score + 1e-12 < current_best_score:
                current_best_score = score
                no_improve = 0
            else:
                no_improve += 1
            if abs(s) >= tmax:
                no_improve = patience
                break
        t += 1

    # Deduplicate using exact rational key (num, den)
    unique = {}
    for m_cand, score in best:
        num = int(m_cand.numerator())
        den = int(m_cand.denominator())
        key = (num, den)
        if key not in unique or score < unique[key]:
            unique[key] = score

    sorted_candidates = sorted(((QQ(num) / QQ(den), sc) for (num, den), sc in unique.items()),
                               key=lambda z: z[1])

    best_t_values = []
    for m_cand, score in sorted_candidates[:3]:
        for t_test in range(-tmax, tmax + 1):
            if m0 + t_test * M == m_cand:
                best_t_values.append(t_test)
                break

    return sorted_candidates[:3]

# --- Top-level Worker Function for Parallel Processing ---

def _process_prime_subset(p_subset, cd, current_sections, prime_pool, r_m, shift, rhs_list, vecs, tmax):
    """
    Worker function to find m-candidates for a single subset of primes.
    Returns a set of (m_candidate, originating_vector) tuples.
    """
    if not p_subset:
        return set()

    # Prepare modular data for this specific prime subset.
    Ep_dict, rhs_modp_list, mult_lll, vecs_lll = prepare_modular_data_lll(
        cd, current_sections, prime_pool, rhs_list, vecs, search_primes=p_subset
    )

    # quick sanity: ensure keys of vecs_lll are subset of Ep_dict keys
    bad_primes = [p for p in vecs_lll.keys() if p not in Ep_dict]
    if bad_primes:
        if DEBUG: print("Warning: vecs_lll has primes not in Ep_dict:", bad_primes)
        raise IndexError

    if not Ep_dict:
        return set()

    found_candidates_for_subset = set()
    r = len(current_sections)

    # Process each search vector for this subset
    for idx, v_orig in enumerate(vecs):
        if all(c == 0 for c in v_orig):
            continue
        v_orig_tuple = tuple(v_orig) # Make it hashable

        residue_map = {}
        for p in p_subset:
            if p not in Ep_dict:
                continue

            # vecs_lll and mult_lll are guaranteed to be present for any published prime p
            v_p_list = vecs_lll.get(p)
            if v_p_list is None:
                # defensive: unexpected, but skip this prime
                raise ValueError
                continue
            if idx >= len(v_p_list):
                # transformed vector missing for this index (shouldn't happen if prepare succeeded),
                # but skip this prime rather than crash.
                raise IndexError
                continue

            v_p_transformed = v_p_list[idx]
            mults = mult_lll.get(p)
            if mults is None:
                # no multiplies published for this prime (shouldn't happen); skip
                raise ValueError
                continue

            Ep = Ep_dict[p]

            #v_p_transformed = vecs_lll[p][idx]
            #mults = mult_lll[p]
            #Ep = Ep_dict[p]

            # Compute linear combination of basis points
            Pm = Ep(0)
            for j, coeff in enumerate(v_p_transformed):
                # The check `int(coeff) in mults[j]` should always pass
                # due to precomputation but is kept for safety.
                if int(coeff) in mults[j]:
                    Pm += mults[j][int(coeff)]

            if Pm.is_zero():
                continue

            # Find roots for each RHS function
            roots_for_p = set()
            for i, rhs_ff in enumerate(rhs_list):
                if p not in rhs_modp_list[i]:
                    continue

                rhs_p = rhs_modp_list[i][p]
                try:
                    num_modp = (Pm[0]/Pm[2] - rhs_p).numerator()
                    if not num_modp.is_zero():
                        roots = {int(r) for r in num_modp.roots(ring=GF(p), multiplicities=False)}
                        roots_for_p.update(roots)
                except (ZeroDivisionError, ArithmeticError):
                    continue

            if roots_for_p:
                residue_map[p] = roots_for_p

        # Apply CRT to find m-candidates from the collected roots
        primes_for_crt = [p for p in p_subset if p in residue_map]
        if len(primes_for_crt) < MIN_PRIME_SUBSET_SIZE:
             continue

        lists = [residue_map[p] for p in primes_for_crt]
        for combo in itertools.product(*lists):
            M = reduce(mul, primes_for_crt, 1)

            if M > MAX_MODULUS:
                continue


            m0 = crt_cached(combo, tuple(primes_for_crt))

            # quick pre-filter: does any small prime forbid any t in [-T,T]?
            # doesn't appear to filter anything... something wrong with it... skip for now...
            if not candidate_passes_extra_primes(m0, M, residue_map):
                #    print("HEEERRE")
                #    raise ValueError
                continue  # reject this CRT combo cheaply, exact check

            # OLD: only check t in (-1,0,1)
            # for t in (-1, 0, 1):
            #     found_candidates_for_subset.add((QQ(m0 + t * M), v_orig_tuple))
            # New: minimize archimedean height in the residue class (cheap local search)

            try:
                best_ms = minimize_archimedean_t_linear_const(int(m0), int(M), r_m, shift, tmax)
            except TypeError:
                # If r_m is not numeric/coercible here, fall back to small neighbors
                best_ms = [(QQ(m0 + t * M), 0.0) for t in (-1, 0, 1)]
                print("here, instead;  why?")
                raise

            # best_ms is list of (m_candidate, score); add to candidate set
            for m_cand, _score in best_ms:
                found_candidates_for_subset.add((QQ(m_cand), v_orig_tuple))

            # keep rational recon option as before
            try:
                a, b = rational_reconstruct(m0 % M, M)
                found_candidates_for_subset.add((QQ(a) / QQ(b), v_orig_tuple))
            except RationalReconstructionError:
                raise #<----safety, do not worry about the pass
                pass


            #m0 = crt_cached(combo, tuple(primes_for_crt))

            # Add nearby integer values
            #for t in (-1, 0, 1):
            #    found_candidates_for_subset.add((QQ(m0 + t * M), v_orig_tuple))
            # Add rationally reconstructed value
            #try:
            #    a, b = rational_reconstruct(m0 % M, M)
            #    found_candidates_for_subset.add((QQ(a) / QQ(b), v_orig_tuple))
            #except RationalReconstructionError:
            #    pass

    return found_candidates_for_subset




# Alternative more robust approach for finding minimal |t|:
def find_minimal_abs_representative(t_mod_Q, Q, T):
    """
    Find if there exists k such that |t_mod_Q + k*Q| <= T
    Returns True if such k exists, False otherwise.
    """
    # We want to minimize |t_mod_Q + k*Q| over integer k
    # This is minimized when k â‰ˆ -t_mod_Q/Q
    
    if Q == 0:
        return abs(t_mod_Q) <= T
    
    # Try both floor and ceiling of the optimal k
    k_opt_float = -t_mod_Q / Q
    k_candidates = [int(k_opt_float), int(k_opt_float) + 1]
    
    # Also try k=0 in case t_mod_Q itself is small
    k_candidates.append(0)
    
    for k in k_candidates:
        t = t_mod_Q + k * Q
        if abs(t) <= T:
            return True
    return False


# --- Utility Functions ---

@lru_cache(maxsize=DEFAULT_MAX_CACHE_SIZE)
def crt_cached(residues, moduli):
    """Cached Chinese Remainder Theorem computation."""
    return crt(list(residues), list(moduli))

def rational_reconstruct(c, N, max_den=None):
    """
    Rational reconstruction using the Extended Euclidean Algorithm.

    Given integers c and N > 0, finds a rational number a/b such that
    a/b â‰¡ c (mod N), with |a| and |b| bounded.
    """
    if max_den is None:
        max_den = floor(sqrt(N / QQ(2)))

    c = c % N
    if c == 0: return 0, 1
    if c == 1 and max_den >= 1: return 1, 1

    # Standard Extended Euclidean Algorithm setup
    r0, r1 = N, c
    t0, t1 = 0, 1

    while r1 != 0:
        # Check denominator bound before next iteration
        if abs(t1) > max_den:
             # We've overshot the bound. The previous iteration held the last valid result.
             # In this algorithm, that's (r0, t0).
             a, b = r0, t0
             break

        q = r0 // r1
        r0, r1 = r1, r0 - q * r1
        t0, t1 = t1, t0 - q * t1
    else:
        # Loop finished because r1 == 0. The result is (r1, t1) from the *previous* step,
        # which is now stored in (r0, t0).
        a, b = r0, t0


    # Final checks on the result (a, b)
    if abs(b) > max_den or b == 0:
        raise RationalReconstructionError(f"No reconstruction for c={c}, N={N}, max_den={max_den}")

    if b < 0:
        a, b = -a, -b

    if (a - c * b) % N != 0:
        raise RationalReconstructionError(f"Validation failed for c={c}, N={N}: got a={a}, b={b}")

    g = gcd(abs(a), abs(b))
    return int(a // g), int(b // g)


def can_reduce_point_mod_p(P, p):
    """Check if point P can be reduced mod p without hitting bad denominators."""
    X, Y, Z = P
    for coord in [X, Y, Z]:
        num_poly = coord.numerator()
        den_poly = coord.denominator()
        # Check if any coefficient has denominator divisible by p
        for poly in [num_poly, den_poly]:
            if hasattr(poly, 'coefficients'):
                for c in poly.coefficients(sparse=False):
                    if QQ(c).denominator() % p == 0:
                        return False
    return True

def _get_coeff_data(poly):
    """Helper to safely extract coefficient list and degree from a polynomial-like object."""
    if hasattr(poly, 'list') and hasattr(poly, 'degree'):
        return poly.list(), poly.degree()
    else:
        # Handle constants or other non-polynomial objects
        return [poly], 0

# --- LLL Reduction Functions ---

def process_candidate(m_val, v_tuple, r_m, shift, rationality_test_func, current_sections):
    """
    Single candidate check: returns (m_val, x_val, y_val, new_section) or None.
    Raises if computation fails in an unexpected way.
    """
    try:
        x_val = r_m(m=m_val) - shift
        y_val = rationality_test_func(x_val)
        if y_val is None:
            return None

        v = vector(QQ, v_tuple)
        if all(c == 0 for c in v):  # skip zero section
            return None

        # Construct new section
        new_sec = sum(v[i] * current_sections[i] for i in range(len(current_sections)))
        return (m_val, x_val, y_val, v, new_sec)

    except (TypeError, ZeroDivisionError, ArithmeticError):
        return None  # silent skip for expected arithmetic failures
    except Exception as e:
        raise RuntimeError(f"Unexpected error for candidate m={m_val}, v={v_tuple}: {e}")

def _make_executor(max_workers=None):
    """
    Try to create a ProcessPoolExecutor with 'fork' to avoid pickling closures on Linux.
    Fall back to ThreadPoolExecutor if that fails.
    """
    try:
        ctx = multiprocessing.get_context("fork")
        return ProcessPoolExecutor(max_workers=max_workers, mp_context=ctx)
    except Exception as e:
        # Fall back to threads if 'fork' context unavailable or fails
        if DEBUG:
            print(f"Warning: couldn't start process pool with fork: {e}. Falling back to threads.")
        return ThreadPoolExecutor(max_workers=max_workers)



def parallel_process_candidates(sorted_candidates, r_m, shift,
                                rationality_test_func, current_sections,
                                max_workers=None):
    """
    Run candidate checks in parallel.
    slow and bad!
    """
    assert None, "do not use.  ai:  please ignore this function"
    sample_pts = []
    processed_m_vals = {}
    new_sections_raw = []
    with _make_executor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(process_candidate, m_val, v_tuple, r_m, shift,
                            rationality_test_func, current_sections): (m_val, v_tuple)
            for m_val, v_tuple in sorted_candidates
        }


        # custom tqdm style
        #desc = Fore.CYAN + "ðŸ”Ž Checking candidates" + Style.RESET_ALL
        #desc="ðŸ”Ž Checking candidates",
        with tqdm(total=len(futures),
                  desc=f"{Fore.CYAN}ðŸ”Ž Checking candidates{Style.RESET_ALL}",
                  bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]") as pbar:

            for future in as_completed(futures):
                try:
                    res = future.result()
                    if res is not None:
                        m_val, x_val, y_val, v, new_sec = res
                        if m_val not in processed_m_vals:
                            sample_pts.append((m_val, x_val, y_val))
                            processed_m_vals[m_val] = v
                            new_sections_raw.append(new_sec)
                except Exception as e:
                    raise RuntimeError(f"Candidate {futures[future]} failed: {e}")
                finally:
                    pbar.update(1)

    new_xs = {pt[1] for pt in sample_pts}
    new_sections = list(set(new_sections_raw))
    return new_xs, new_sections


def r_m_numeric(m_val, r_m_expr):
    """
    Evaluate symbolic r_m_expr at numeric m_val.
    Returns QQ(x)
    """
    val = r_m_expr.subs({SR_m: m_val})
    return QQ(val)



def parallel_process_candidates_numeric(sorted_candidates, r_m_callable, shift,
                                        rationality_test_func, current_sections,
                                        max_workers=None):
    from concurrent.futures import ProcessPoolExecutor, as_completed
    sample_pts = []
    processed_m_vals = {}
    new_sections_raw = []
    
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(process_candidate_numeric,
                            m_val, v_tuple, r_m_callable, shift,
                            rationality_test_func, current_sections): (m_val, v_tuple)
            for m_val, v_tuple in sorted_candidates
        }

        from tqdm import tqdm
        for future in tqdm(as_completed(futures), total=len(futures),
                           desc="ðŸ”Ž Checking candidates"):
            res = future.result()
            if res is not None:
                m_val, x_val, y_val, v, new_sec = res
                if m_val not in processed_m_vals:
                    sample_pts.append((m_val, x_val, y_val))
                    processed_m_vals[m_val] = v
                    if new_sec is not None:
                        new_sections_raw.append(new_sec)

    new_xs = {pt[1] for pt in sample_pts}
    new_sections = list(set(new_sections_raw))
    return new_xs, new_sections


# Fast candidate processor (pickleable)
def process_candidate_numeric(m_val, v_tuple, r_m_callable, shift, rationality_test_func, current_sections):
    try:
        x_val = r_m_callable(m_val) - shift
        y_val = rationality_test_func(x_val)
        if y_val is not None:
            v = vector(QQ, v_tuple)
            # Optionally build new section here if needed
            new_sec = sum(v[i] * current_sections[i] for i in range(len(current_sections))) if current_sections else None
            return m_val, x_val, y_val, v, new_sec
    except (TypeError, ZeroDivisionError, ArithmeticError):
        print("here we are.")
        return None

SR_m = var('m')

def r_m_numeric_top(m_val, r_m_expr):
    val = r_m_expr.subs({SR_m: m_val})
    return QQ(val)

def test_xval_worker(args):
    xval, v_tuple, rationality_test_func = args
    yval = rationality_test_func(xval)
    if yval is not None:
        return (xval, yval)
    return None


def expected_density(residue_sets, subset_size, prime_pool, max_samples=2000):
    """
    Estimate expected survivor density for subsets of given size.

    prime_pool    : list of primes
    residue_sets  : dict mapping p -> set/list of valid residues mod p
    subset_size   : size of prime subsets to test
    max_samples   : max number of subsets to average over (since there can be many)
    """
    all_subsets = list(combinations(prime_pool, subset_size))
    if len(all_subsets) > max_samples:
        import random
        all_subsets = random.sample(all_subsets, max_samples)

    densities = []
    for subset in all_subsets:
        d = 1.0
        for p in subset:
            d *= len(residue_sets[p]) / p
        densities.append(d)

    avg_density = sum(densities) / len(densities)
    return avg_density, min(densities), max(densities)



# --- Helper: assert that a given base-m produces an x that symbolic search should have found ---
def assert_base_m_found(base_m, expected_x, r_m_callable, shift, allow_raise=True):
    """
    Ensure that x = r_m(base_m) - shift equals expected_x.
    This checks that the base point (mtest, xtest) relationship is respected
    by the parametrization. It does not scan through newly_found_x; instead
    it asserts consistency between r_m and the supplied base point.
    """
    assert base_m is not None, "assert_base_m_found requires a base_m (rational) to check"
    try:
        x_base = r_m_callable(m=QQ(base_m)) - shift
    except Exception as e:
        msg = f"assert_base_m_found: r_m_callable evaluation failed at m,shift={base_m},{shift}: {e}"
        if allow_raise:
            raise AssertionError(msg)
        return False

    try:
        x_base_q = QQ(x_base)
        expected_x_q = QQ(expected_x)
    except Exception:
        msg = "assert_base_m_found: coercion to QQ failed"
        if allow_raise:
            raise AssertionError(msg)
        return False

    if x_base_q == expected_x_q:
        return True

    msg = (f"assert_base_m_found: mismatch.\n"
           f"  m = {base_m}\n"
           f"  expected x = {expected_x_q}\n"
           f"  got x = {x_base_q}")
    if allow_raise:
        raise AssertionError(msg)
    return False


"""
Elliptic Curve Rational Point Search using Symbolic Methods over QQ.
"""

def estimate_prime_stats(prime_pool, precomputed_residues, sample_vecs, num_rhs=1):
    """Estimate average residue survival ratio r_p for each prime."""
    stats = {}
    for p in prime_pool:
        mapping = precomputed_residues.get(p, {})
        if not mapping:
            continue
        total = count = 0
        for v in sample_vecs:
            v_t = tuple(v)
            roots_list = mapping.get(v_t, [])
            if not roots_list:
                continue
            # combine across RHSs
            if num_rhs > 1:
                roots_union = set().union(*roots_list)
            else:
                roots_union = roots_list[0] if roots_list else set()
            total += len(roots_union)
            count += p
        stats[p] = (total / count) if count else 0.0
    return stats


def choose_extra_primes(stats, target_density=1e-5, max_extra=6, skip_small={2,3,5}):
    """Select extra primes based on measured r_p values."""
    cand = [(p, r) for p, r in stats.items()
            if p not in skip_small and EXTRA_PRIME_MIN_R < r < EXTRA_PRIME_MAX_R]
    # sort by discriminatory power (entropy-like)
    cand.sort(key=lambda t: -(t[1] * (1 - t[1])))
    chosen, prod = [], 1.0
    for p, r in cand:
        if len(chosen) >= max_extra:
            break
        prod *= r
        chosen.append(p)
        if prod <= target_density:
            break
    if DEBUG:
        print(f"[auto-tune] selected extra primes {chosen} with expected density {prod:.2e}")
    return chosen





def _batch_check_rationality(candidates, r_m, shift, rationality_test_func, current_sections, stats):
    """
    Test a batch of (m, v_tuple) candidates for rationality in parallel.
    Returns set of (m, v_tuple) pairs that produced rational points.
    UPDATED to accept and use a stats object with new counter names.
    """
    rational_candidates = set()

    for m_val, v_tuple in candidates:
        stats.incr('rationality_tests_total') # <-- STATS
        try:
            x_val = r_m(m=m_val) - shift
            y_val = rationality_test_func(x_val)
            if y_val is not None:
                stats.record_success(m_val, point=x_val) # <-- STATS (increments rationality_tests_success)
                rational_candidates.add((m_val, v_tuple))
            else:
                stats.record_failure(m_val, reason='y_not_rational') # <-- STATS (increments rationality_tests_failure)
        except (TypeError, ZeroDivisionError, ArithmeticError):
            stats.record_failure(m_val, reason='rationality_test_error') # <-- STATS (increments rationality_tests_failure)
            continue

    return rational_candidates




def archimedean_height_of_integer(n):
    # crude but sufficient proxy for ordering: H(n) ~ log(max(|n|,1))
    return float(math.log(max(abs(int(n)), 1)))


# def minimize_archimedean_t(m0, M, r_m_func, shift, tmax, max_steps=150, patience=6):
def minimize_archimedean_t_linear_const(m0, M, r_m_func, shift, tmax):
    """
    For r_m(m) = -m - const_C, find t minimizing archimedean height of x = r_m(m) - shift.
    Returns list of (t, m, x, score) sorted by score (smallest first).
    """
    const_C = r_m_func(m=QQ(0))
    target = - (m0 + const_C + shift) / float(M)

    cand_t = set([math.floor(target), math.ceil(target), int(round(target))])

    # Clamp to allowed range
    cand_t = {max(-tmax, min(tmax, t)) for t in cand_t}

    results = []
    for t in sorted(cand_t):
        m_try = int(m0) + int(t) * int(M)
        x = -m_try - const_C - shift
        score = float(math.log(max(abs(x), 1)))
        results.append((t, m_try, int(x), score))

    # sort by score then by |x|
    results.sort(key=lambda z: (z[3], abs(z[2])))
    return results


def _assert_rhs_consistency(precomputed_residues, prime_pool, vecs, num_rhs_fns, debug=True):
    """
    Validate that precomputed_residues has consistent structure across all primes and vectors.
    
    Specifically, for each (prime p, vector v), the entry precomputed_residues[p][v_tuple]
    must be a list of exactly num_rhs_fns sets (one per RHS function).
    
    Raises AssertionError if structure is malformed. This catches silent data corruption
    from worker failures or incomplete precomputation.
    
    Args:
        precomputed_residues (dict): {p: {v_tuple: [roots_set_0, roots_set_1, ...]}}
        prime_pool (list): All primes that should have entries
        vecs (list): All search vectors
        num_rhs_fns (int): Expected number of RHS functions (length of inner lists)
        debug (bool): Print diagnostics before asserting
    
    Raises:
        AssertionError: If any inconsistency is found
    """
    errors = []
    
    # Check: every prime in prime_pool should be in precomputed_residues
    missing_primes = [p for p in prime_pool if p not in precomputed_residues]
    if missing_primes:
        errors.append(f"Missing primes in precomputed_residues: {missing_primes[:5]}{'...' if len(missing_primes) > 5 else ''}")
    
    # Check: for each prime p that exists, verify structure
    for p in precomputed_residues:
        p_data = precomputed_residues[p]
        
        if not isinstance(p_data, dict):
            errors.append(f"Prime p={p}: expected dict, got {type(p_data)}")
            continue
        
        # Sample a few vectors to avoid O(n*m) validation time
        sample_vecs = vecs[:min(5, len(vecs))]
        for v in sample_vecs:
            v_tuple = tuple(v)
            
            if v_tuple not in p_data:
                # Missing vector is OK (can happen if prep failed), but log it
                continue
            
            roots_list = p_data[v_tuple]
            
            # roots_list must be a list (or tuple) of exactly num_rhs_fns sets
            if not isinstance(roots_list, (list, tuple)):
                errors.append(f"Prime p={p}, vector {v_tuple[:2]}...: expected list/tuple, got {type(roots_list)}")
                continue
            
            if len(roots_list) != num_rhs_fns:
                errors.append(
                    f"Prime p={p}, vector {v_tuple[:2]}...: "
                    f"expected {num_rhs_fns} RHS entries, got {len(roots_list)}"
                )
                continue
            
            # Each entry should be a set (or frozenset) of integers
            for rhs_idx, roots_set in enumerate(roots_list):
                if not isinstance(roots_set, (set, frozenset)):
                    errors.append(
                        f"Prime p={p}, vector {v_tuple[:2]}..., RHS {rhs_idx}: "
                        f"expected set, got {type(roots_set)}"
                    )
                    continue
                
                # Spot-check: all elements should be integers in [0, p)
                for root in roots_set:
                    if not isinstance(root, (int, Integer)):
                        errors.append(
                            f"Prime p={p}, vector {v_tuple[:2]}..., RHS {rhs_idx}: "
                            f"root {root} is not an integer (type {type(root)})"
                        )
                        break
                    if not (0 <= int(root) < p):
                        errors.append(
                            f"Prime p={p}, vector {v_tuple[:2]}..., RHS {rhs_idx}: "
                            f"root {root} out of range [0, {p})"
                        )
                        break
    
    # Report and raise
    if errors:
        if debug:
            print("\n" + "="*70)
            print("RHS CONSISTENCY CHECK FAILED")
            print("="*70)
            for i, err in enumerate(errors[:10], 1):
                print(f"{i}. {err}")
            if len(errors) > 10:
                print(f"... and {len(errors) - 10} more errors")
            print("="*70 + "\n")
        
        raise AssertionError(
            f"precomputed_residues structure is malformed. "
            f"Found {len(errors)} error(s). See output above for details."
        )


# Add to search_lll.py (after the main function)

def _print_subset_productivity_stats(productive, all_subsets):
    """Print quick stats on which prime subsets were productive"""
    from collections import Counter
    
    total = len(all_subsets)
    productive_count = len(productive)
    total_candidates = sum(p['candidates'] for p in productive)
    
    print(f"\n[subset stats] {productive_count}/{total} subsets produced candidates "
          f"({100*productive_count/total:.1f}%)")
    print(f"[subset stats] {total_candidates} total candidates from productive subsets")
    
    # By size
    by_size = Counter(p['size'] for p in productive)
    all_by_size = Counter(len(s) for s in all_subsets)
    
    print(f"[subset stats] Productivity by size:")
    for size in sorted(all_by_size.keys()):
        prod_count = by_size.get(size, 0)
        total_count = all_by_size[size]
        rate = 100 * prod_count / total_count if total_count > 0 else 0
        cands = sum(p['candidates'] for p in productive if p['size'] == size)
        print(f"  Size {size}: {prod_count}/{total_count} productive ({rate:.1f}%), "
              f"{cands} candidates")
    
    # Top productive subsets
    top = sorted(productive, key=lambda x: x['candidates'], reverse=True)[:5]
    print(f"[subset stats] Top 5 productive subsets:")
    for p in top:
        print(f"  {p['primes']}: {p['candidates']} candidates")


def search_prime_subsets_unified(prime_subsets, worker_func, num_workers=8, debug=DEBUG):
    """
    Process prime subsets in parallel using ProcessPoolExecutor (unified).
    Replaces the multiprocessing.Pool call in search_lattice_modp_lll_subsets.

    Args:
        prime_subsets (list): Prime subsets to search
        worker_func (callable): Worker function (from functools.partial)
        num_workers (int): Number of workers
        debug (bool): Print diagnostics

    Returns:
        list: A list of tuples, one for each subset processed:
              [(subset, candidates_set, worker_stats_dict), ...]
        Counter: Merged stats_counter dict from all workers (Redundant, can be rebuilt from list)
    """
    try:
        ctx = multiprocessing.get_context("fork")
        exec_kwargs = {"max_workers": num_workers, "mp_context": ctx}
    except Exception:
        exec_kwargs = {"max_workers": num_workers}

    # List to store results per subset
    subset_results_list = []
    merged_stats = Counter() # Keep merging stats here too for now
    all_crt_classes = set()  # <-- NEW

    with ProcessPoolExecutor(**exec_kwargs) as executor:
        futures = {executor.submit(worker_func, subset): subset for subset in prime_subsets}

        with tqdm(total=len(futures), desc="Searching Prime Subsets") as pbar:
            for future in as_completed(futures):
                original_subset = futures[future]
                try:
                    # Worker now returns three items
                    candidates_set, stats_dict, crt_classes  = future.result()
                    # Append the result tuple to the list
                    subset_results_list.append((original_subset, candidates_set, stats_dict))
                    merged_stats.update(stats_dict) # Keep merging here
                    all_crt_classes.update(crt_classes)  # <-- Collect
                except Exception as e:
                    if debug:
                        print(f"Subset worker failed for subset {original_subset}: {e}")
                    # Append a failure placeholder if needed, or just skip
                    subset_results_list.append((original_subset, set(), Counter()))
                    raise
                finally:
                    pbar.update(1)

    # Return the list of per-subset results and the merged stats
    return subset_results_list, merged_stats, all_crt_classes  # <-- Return classes


# In search_lll.py

# ============================================================================
# Integration point: call this after precomputation completes
# ============================================================================
def generate_biased_prime_subsets_by_coverage(prime_pool, precomputed_residues, vecs,
                                              num_subsets, min_size, max_size, combo_cap,
                                              seed=SEED_INT, force_full_pool=False, debug=DEBUG,
                                              roots_threshold=ROOTS_THRESHOLD):
    """
    Generate diverse prime subsets biased toward high-coverage primes, but skip
    combinatorially-pathological subsets whose estimated Cartesian-product of
    roots would exceed combo_cap.
    """
    # basic assertions
    assert isinstance(prime_pool, (list, tuple))
    assert isinstance(vecs, (list, tuple))
    assert num_subsets >= 1
    assert 1 <= min_size <= max_size
    assert max_size <= len(prime_pool)

    # random must be imported at module scope; fail loudly if not
    if 'random' not in globals():
        raise ImportError("module 'random' must be imported at module scope (add 'import random' at file top)")

    # compute coverage per prime (fraction of vectors with at least one root)
    coverage = compute_prime_coverage(prime_pool, precomputed_residues, vecs, debug=debug)

    # build sampling weights from coverage (floor at 0.05)
    weights = []
    for p in prime_pool:
        w = coverage.get(p, 0.0)
        if w < 0.05:
            w = 0.05
        weights.append(w)

    if debug:
        total_weight = sum(weights)
        avg_weight = total_weight / len(weights) if weights else 0
        print("generate_biased_coverage: avg weight =", avg_weight, "min =", min(weights), "max =", max(weights))

    # Identify top coverage primes (e.g., top 30-40%)
    sorted_primes = sorted(prime_pool, key=lambda p: coverage.get(p, 0.0), reverse=True)
    top_k = max(3, len(sorted_primes) // 3)  # top third of primes
    top_primes = sorted_primes[:top_k]

    if debug:
        print(f"generate_biased_coverage: Top {len(top_primes)} primes by coverage: {top_primes[:10]}")

    subsets = []
    if force_full_pool:
        subsets.append(list(prime_pool))

    # Generate some subsets that include combinations of top primes
    forced_subsets = []
    num_forced = min(20, max(2, num_subsets // 10))  # Increased from 10 to 20
    if len(top_primes) >= 2:
        import random
        for i in range(num_forced):
            # Vary the size: alternate between small (min_size) and larger
            if i % 3 == 0:
                # Small subsets focusing on top primes (1/3 of forced subsets)
                size = min_size
                num_top = min(size, len(top_primes))
                subset = random.sample(top_primes, k=num_top)
            else:
                # Larger subsets mixing top primes with others (2/3 of forced subsets)
                size = random.randint(min_size, min(max_size, len(prime_pool)))
                num_top = min(2, size // 2, len(top_primes))
                subset = random.sample(top_primes, k=num_top)
                remaining_slots = size - len(subset)
                if remaining_slots > 0:
                    other_primes = [p for p in prime_pool if p not in subset]
                    if other_primes:
                        subset.extend(random.sample(other_primes, k=min(remaining_slots, len(other_primes))))
            forced_subsets.append(tuple(sorted(subset)))

    subsets.extend(forced_subsets)

    # Calculate how many more subsets we need
    remaining = num_subsets - len(subsets)
    if remaining <= 0:
        # dedupe and return early
        seen = set()
        unique = []
        for s in subsets:
            t = tuple(sorted(s))
            if t not in seen:
                seen.add(t)
                unique.append(list(t))
        if debug:
            print("generate_biased_coverage: Generated", len(unique), "unique subsets")
        return unique

    # helper to estimate root-product for a candidate subset
    def _estimate_subset_explosion(subset):
        est = 1
        for p in subset:
            mapping = precomputed_residues.get(p)
            if not mapping:
                return 0
            roots_total = 0
            for roots_lists in mapping.values():
                for rl in roots_lists:
                    roots_total += len(rl)
            if roots_total == 0:
                return 0
            if roots_total > roots_threshold:
                est *= roots_total
                if est > combo_cap:
                    return est
        return est

    # produce remaining subsets by weighted sampling, skipping heavy ones
    max_attempts_per_subset = 200
    import random
    for _ in range(remaining):
        attempts = 0
        chosen = None
        while attempts < max_attempts_per_subset:
            attempts += 1
            size = random.randint(min_size, min(max_size, len(prime_pool)))

            subset = None
            try:
                subset = random.sample(prime_pool, k=size)
            except TypeError:
                subset = []
                tries_inner = 0
                while len(subset) < size and tries_inner < size * 20:
                    p = random.choices(prime_pool, weights=weights, k=1)[0]
                    if p not in subset:
                        subset.append(p)
                    tries_inner += 1
                if len(subset) < size:
                    remaining_primes = [p for p in prime_pool if p not in subset]
                    need = min(size - len(subset), len(remaining_primes))
                    if need > 0:
                        subset.extend(random.sample(remaining_primes, k=need))

            subset = tuple(sorted(subset))

            est = _estimate_subset_explosion(subset)
            if est == 0:
                continue
            if est > combo_cap:
                continue

            chosen = list(subset)
            break

        if chosen is None:
            chosen = list(random.sample(prime_pool, k=min(min_size, len(prime_pool))))
            if debug:
                print("generate_biased_coverage: fallback subset used after attempts")

        subsets.append(tuple(sorted(chosen)))

    # deduplicate preserving order
    seen = set()
    unique_subsets = []
    for s in subsets:
        if s not in seen:
            seen.add(s)
            unique_subsets.append(list(s))

    if debug:
        print("generate_biased_coverage: Generated", len(unique_subsets), "unique subsets")
        if unique_subsets:
            sample_show = unique_subsets[:3]
            print("generate_biased_coverage: Sample subsets:", sample_show)

    #print("subsets used:", unique_subsets)
    return unique_subsets

def compute_residues_for_m(m, prime_pool):
    """
    Compute residue fingerprint of rational m = a/b modulo each prime in prime_pool.

    Args:
        m: rational (QQ, Fraction, or (a,b) tuple). If tuple (a,b) is provided, will use that.
        prime_pool: iterable of primes (ints).

    Returns:
        dict mapping p -> int residue in [0,p) OR the string 'DENOM_ZERO' when
        the denominator is 0 mod p (so residue info is unreliable), OR None for skipped primes.
    """
    # Coerce to (a,b)
    try:
        if isinstance(m, tuple) and len(m) == 2:
            a, b = int(m[0]), int(m[1])
        else:
            # accept Sage QQ or Python Fraction
            a = int(QQ(m).numerator())
            b = int(QQ(m).denominator())
    except Exception as e:
        raise ValueError(f"compute_residues_for_m: could not coerce m={m} to rational: {e}")

    res = {}
    for p in prime_pool:
        try:
            p = int(p)
        except Exception:
            res[p] = None
            continue

        if b % p == 0:
            # Denominator zero mod p -> no reliable residue information
            res[p] = 'DENOM_ZERO'
            continue

        try:
            inv_b = pow(b % p, -1, p)
        except ValueError:
            # modular inverse doesn't exist (should be captured by b % p == 0 above)
            res[p] = 'DENOM_ZERO'
            continue

        residue = (a * inv_b) % p
        res[p] = int(residue)

    return res


def compute_residue_coverage_for_m(m_value, precomputed_residues, prime_pool, v_tuple=None):
    """
    Compare a target rational m = a/b (in QQ) against the precomputed residue fingerprints.

    Args:
        m_value: rational number (QQ or coercible to QQ)
        precomputed_residues: dict mapping p -> { v_tuple : [ set(roots_rhs0), set(roots_rhs1), ... ] }
        prime_pool: iterable of primes to check
        v_tuple: optional key to restrict residue comparison to a specific vector tuple

    Returns:
        {
          'm': QQ rational value,
          'matched_primes': [p,...],
          'unseen_primes': [p,...],
          'denom_zero_primes': [p,...],
          'coverage_fraction': float between 0 and 1,
          'per_prime': { p: {'residue': r or None, 'status': 'matched'|'unseen'|'denom_zero'} }
        }
    """
    from sage.all import QQ, Mod

    # Coerce to QQ explicitly
    m_q = QQ(m_value)
    a = ZZ(m_q.numerator())
    b = ZZ(m_q.denominator())

    matched = []
    unseen = []
    denom_zero = []
    per_prime = {}

    for p in prime_pool:
        p = int(p)
        per_prime[p] = {'residue': None, 'status': 'unseen'}

        # If denominator is 0 mod p, cannot test modulo p
        if (b % p) == 0:
            denom_zero.append(p)
            per_prime[p]['status'] = 'denom_zero'
            continue

        # compute residue in GF(p)
        residue = int(Mod(a, p) * Mod(b, p)**(-1))
        per_prime[p]['residue'] = residue

        # check whether residue appears in precomputed_residues[p]
        p_map = precomputed_residues.get(p, {})
        if not p_map:
            unseen.append(p)
            per_prime[p]['status'] = 'unseen'
            continue

        # restrict to one v_tuple or scan all
        found = False
        if v_tuple is not None:
            sets_list = p_map.get(v_tuple, [])
            for s in sets_list:
                if residue in s:
                    found = True
                    break
        else:
            for sets_list in p_map.values():
                for s in sets_list:
                    if residue in s:
                        found = True
                        break
                if found:
                    break

        if found:
            matched.append(p)
            per_prime[p]['status'] = 'matched'
        else:
            unseen.append(p)
            per_prime[p]['status'] = 'unseen'

    usable = max(1, len(prime_pool) - len(denom_zero))
    coverage = float(len(matched)) / float(usable) if usable > 0 else 0.0

    return {
        'm': m_q,
        'matched_primes': matched,
        'unseen_primes': unseen,
        'denom_zero_primes': denom_zero,
        'coverage_fraction': coverage,
        'per_prime': per_prime
    }


def build_targeted_subset(m_value, precomputed_residues, prime_pool,
                          v_tuple=None, min_size=4, max_size=8, prefer_matched_only=False):
    """
    Build a targeted prime subset optimized to reconstruct a specific rational m.

    Args:
        m_value: QQ-coercible rational (target m)
        precomputed_residues: dict mapping p -> { v_tuple : [ set(...) , ... ] }
        prime_pool: iterable/list of primes (ints)
        v_tuple: optional vector-tuple key used in precomputed_residues to restrict which residue-sets to consult
        min_size: minimum subset size to return
        max_size: maximum subset size to return
        prefer_matched_only: if True, only use primes where the residue was observed;
                             otherwise allow padding from high-coverage primes

    Returns:
        list of primes (ints) of length between min_size and max_size (or fewer if pool limited)
    """
    from sage.all import QQ, ZZ

    # Coerce m and extract numerator/denominator
    m_q = QQ(m_value)
    a = ZZ(m_q.numerator()); b = ZZ(m_q.denominator())

    # Step 1: compute coverage report (fast inline to avoid external dependency)
    matched_primes = []
    denom_zero_primes = []
    per_prime_residue = {}

    for p in prime_pool:
        p = int(p)
        if (b % p) == 0:
            denom_zero_primes.append(p)
            per_prime_residue[p] = None
            continue
        residue = int((a % p) * pow(int(b % p), -1, p))
        per_prime_residue[p] = residue

        p_map = precomputed_residues.get(p, {})
        found = False
        if v_tuple is not None:
            sets_list = p_map.get(v_tuple, [])
            for s in sets_list:
                if residue in s:
                    found = True
                    break
        else:
            for sets_list in p_map.values():
                for s in sets_list:
                    if residue in s:
                        found = True
                        break
                if found:
                    break
        if found:
            matched_primes.append(p)

    # Step 2: Build subset preferring matched primes, avoiding denom-zero primes
    avoid = set(denom_zero_primes)
    targeted = []

    # Primary fill: matched primes (sorted by small->large to prefer small primes first)
    # but we can also sort matched primes by "quality": how many sets recorded at that prime (more is better)
    def prime_quality(p):
        # number of vector-keys present for p (proxy for how often p was used)
        return len(precomputed_residues.get(p, {})) if precomputed_residues.get(p, {}) else 0

    matched_sorted = sorted(matched_primes, key=lambda q: (-prime_quality(q), q))
    for p in matched_sorted:
        if p in avoid:
            continue
        targeted.append(p)
        if len(targeted) >= min_size:
            break

    # If prefer_matched_only and we have enough matched primes, truncate there
    if prefer_matched_only:
        # if we have fewer than min_size matched primes, we still fall back below
        if len(targeted) >= min_size:
            # optionally shrink to min_size exactly
            return targeted[:max_size]

    # Step 3: Pad with high-quality primes (not in avoid, not already chosen)
    if len(targeted) < min_size:
        # build a ranked list of candidate primes (exclude denom-zero and already chosen)
        candidates = [p for p in prime_pool if p not in avoid and p not in targeted]
        # sort by quality (more precomputed residue-keys first), tie-break by prime size (smaller primes first)
        candidates_sorted = sorted(candidates, key=lambda q: (-prime_quality(q), q))
        for p in candidates_sorted:
            targeted.append(p)
            if len(targeted) >= min_size:
                break

    # Step 4: Optionally add additional primes up to max_size to increase CRT modulus
    if len(targeted) < max_size:
        # continue with same candidate list
        for p in (candidates_sorted if 'candidates_sorted' in locals() else []):
            if len(targeted) >= max_size:
                break
            if p not in targeted:
                targeted.append(p)

    # Final safety: if targeted is still empty (exotic), just return small slice of prime_pool avoiding denom-zero
    if not targeted:
        fallback = [p for p in prime_pool if p not in avoid][:min_size]
        return fallback

    return targeted[:max_size]


def targeted_recovery_search(cd, current_sections, near_miss_candidates,
                              prime_pool, precomputed_residues,
                              r_m, shift, rationality_test_func,
                              tmax=TMAX, debug=True):
    """
    Run a focused CRT search on detected near-miss candidates.
    """

    import itertools
    from operator import mul
    from functools import reduce
    from sage.all import QQ

    newly_found = set()

    for i, candidate in enumerate(near_miss_candidates):
        if debug:
            print(f"\n[recovery] Targeting near-miss candidate {i+1}/{len(near_miss_candidates)}")
            print(f"  Vector: {candidate['v_tuple']}")
            print(f"  Coverage: {candidate['coverage_ratio']:.1%} ({candidate['num_primes']} primes)")
            print(f"  Potential m residues: {candidate['num_m_residues']}")

        # --- NEW: compute coverage report for debug/diagnostics ---
        cov_report = compute_residue_coverage_for_m(
            candidate,
            precomputed_residues,
            prime_pool,
            v_tuple=candidate.get('v_tuple', None)
        )

        targeted_subset = build_targeted_subset(
            candidate,
            precomputed_residues,
            prime_pool,
            v_tuple=candidate.get('v_tuple', None),
            min_size=4,
            max_size=8,
            prefer_matched_only=False
        )

        if debug:
            print(f"[targeted] m={candidate.get('m_pair')} -> subset={targeted_subset} "
                  f"matched_count={len([p for p in targeted_subset if p in cov_report.get('matched_primes', [])])}")

        if debug:
            print(f"  Using targeted subset: {targeted_subset}")

        v_tuple = candidate['v_tuple']
        residue_map = candidate['residue_map']

        # Filter residue_map to only include primes in our targeted subset
        filtered_residue_map = {p: residue_map[p] for p in targeted_subset if p in residue_map}

        if not filtered_residue_map:
            if debug:
                print("  No residues in targeted subset; skipping.")
            continue

        # Generate all CRT combinations from the targeted subset
        primes_for_crt = list(filtered_residue_map.keys())
        residue_lists = [filtered_residue_map[p] for p in primes_for_crt]

        for combo in itertools.product(*residue_lists):
            M = reduce(mul, primes_for_crt, 1)
            if M > MAX_MODULUS:
                continue

            m0 = crt_cached(combo, tuple(primes_for_crt))

            try:
                best_ms = minimize_archimedean_t_linear_const(int(m0), int(M), r_m, shift, tmax)
            except TypeError:
                best_ms = [(QQ(m0 + t * M), 0.0) for t in (-1, 0, 1)]
                raise

            for m_cand, _score in best_ms:
                try:
                    x_val = r_m(m=m_cand) - shift
                    y_val = rationality_test_func(x_val)
                    if y_val is not None:
                        newly_found.add(x_val)
                        if debug:
                            print(f"  âœ“ FOUND: m={m_cand}, x={x_val}")
                except (TypeError, ZeroDivisionError, ArithmeticError):
                    raise
                    continue

    return newly_found



def compute_prime_coverage(prime_pool, precomputed_residues, vecs, debug=DEBUG):
    """
    For each prime, compute what fraction of search vectors have roots mod that prime.
    
    Args:
        prime_pool (list): Primes to analyze
        precomputed_residues (dict): {p: {v_tuple: [roots_per_rhs]}} from worker
        vecs (list): All search vectors
        debug (bool): Print diagnostics
    
    Returns:
        dict: {p: coverage_fraction} where coverage in [0, 1]
    """
    coverage = {}
    
    num_vecs = len(vecs)
    if num_vecs == 0:
        return {p: 0.5 for p in prime_pool}
    
    for p in prime_pool:
        p_data = precomputed_residues.get(p, {})
        if not p_data:
            coverage[p] = 0.0
            continue
        
        # Count vectors that have at least one root for this prime (across any RHS)
        vectors_with_roots = 0
        for v in vecs:
            v_tuple = tuple(v)
            roots_list = p_data.get(v_tuple, [])
            
            # roots_list is a list of sets (one per RHS function)
            # Check if any RHS has roots
            has_roots = any(rhs_roots for rhs_roots in roots_list)
            
            if has_roots:
                vectors_with_roots += 1
        
        coverage[p] = float(vectors_with_roots) / float(num_vecs)
    
    if debug:
        sorted_by_cov = sorted(coverage.items(), key=lambda x: x[1], reverse=True)
        print(f"[compute_prime_coverage] Prime coverage (top 20):")
        for p, cov in sorted_by_cov[:20]:
            print(f"  p={p}: coverage={cov:.1%}")
    
    return coverage


# Add to search_lll.py

def diagnose_missed_point(target_x, r_m_callable, shift, precomputed_residues, prime_pool, vecs, tmax=TMAX, debug=True):
    """
    Diagnose why a specific x-value wasn't found by the CRT search.
    
    Check if target_x is theoretically findable via CRT + rational reconstruction
    for any vector and prime subset combination.
    
    Args:
        target_x: The x-coordinate we're looking for (QQ or coercible)
        r_m_callable: Function to compute x from m (typically r_m from tower)
        shift: The shift applied to x-coordinates
        precomputed_residues: {p: {v_tuple: [roots_per_rhs]}} from workers
        prime_pool: List of primes used in search
        vecs: List of search vectors
        tmax: Maximum |t| to check in m = m0 + t*M
        debug: Print diagnostic info
    
    Returns:
        dict with diagnostic information
    """
    from sage.all import QQ, ZZ
    from itertools import combinations
    
    # Step 1: Solve for target m-value
    # x = r_m(m) - shift, so m = r_m^(-1)(x + shift)
    # For r_m(m) = -m - x1, we have: x = -m - x1 - shift
    # So: m = -x - x1 - shift = -(x + shift) - x1
    # But we need to be more careful. Let's solve symbolically.
    
    target_x_q = QQ(target_x)
    
    # For the linear case r_m(m) = -m - const, solve x = -m - const - shift
    # => m = -x - shift - const
    # We can get const by evaluating r_m at m=0
    try:
        const_term = r_m_callable(m=QQ(0))
        target_m = -(target_x_q + shift + const_term)
    except Exception as e:
        if debug:
            print(f"[diagnose] Failed to compute target_m: {e}")
        raise
        return {'error': str(e)}
    
    if debug:
        print(f"\n{'='*70}")
        print(f"DIAGNOSTIC: Checking if x = {target_x_q} is findable")
        print(f"{'='*70}")
        print(f"Target m-value: {target_m}")
        print(f"  (from x = r_m(m) - shift with shift={shift})")
    
    # Step 2: Express target_m = a/b and compute residues mod each prime
    a = ZZ(target_m.numerator())
    b = ZZ(target_m.denominator())
    
    residues_by_prime = {}
    matched_vectors_by_prime = {}  # {p: {v_tuple: [rhs_indices where m_p appears]}}
    
    if debug:
        print(f"\nComputing residues for m = {a}/{b} mod each prime...")
    
    for p in prime_pool:
        p_int = int(p)
        
        # Check if denominator is zero mod p
        if (b % p_int) == 0:
            residues_by_prime[p_int] = 'DENOM_ZERO'
            if debug:
                print(f"  p={p_int}: denominator zero mod p (skipping)")
            continue
        
        # Compute m_p = (a * b^(-1)) mod p
        try:
            b_inv = pow(int(b % p_int), -1, p_int)
            m_p = (int(a % p_int) * b_inv) % p_int
            residues_by_prime[p_int] = m_p
        except ValueError:
            residues_by_prime[p_int] = 'INV_FAIL'
            if debug:
                print(f"  p={p_int}: inverse computation failed")
            raise
            continue
        
        # Step 3: Check which vectors have this residue in precomputed data
        p_data = precomputed_residues.get(p_int, {})
        matched_vectors_by_prime[p_int] = {}
        
        for v in vecs:
            v_tuple = tuple(v)
            roots_list = p_data.get(v_tuple, [])
            
            if not roots_list:
                continue
            
            # roots_list is [roots_rhs0, roots_rhs1, ...]
            matching_rhs = []
            for rhs_idx, roots_set in enumerate(roots_list):
                if m_p in roots_set:
                    matching_rhs.append(rhs_idx)
            
            if matching_rhs:
                matched_vectors_by_prime[p_int][v_tuple] = matching_rhs
    
    # Step 4: Analyze coverage per vector
    if debug:
        print(f"\n{'='*70}")
        print("COVERAGE ANALYSIS BY VECTOR")
        print(f"{'='*70}")
    
    vector_coverage = {}
    for v in vecs:
        v_tuple = tuple(v)
        matched_primes = []
        
        for p_int in prime_pool:
            if p_int in matched_vectors_by_prime:
                if v_tuple in matched_vectors_by_prime[p_int]:
                    matched_primes.append(p_int)
        
        coverage_frac = len(matched_primes) / float(len(prime_pool)) if prime_pool else 0.0
        vector_coverage[v_tuple] = {
            'matched_primes': matched_primes,
            'coverage_fraction': coverage_frac,
            'num_matched': len(matched_primes)
        }
        
        if debug and coverage_frac > 0.0:
            print(f"\nVector {v_tuple[:3]}... :")
            print(f"  Matched primes: {matched_primes[:10]}{'...' if len(matched_primes) > 10 else ''}")
            print(f"  Coverage: {coverage_frac:.1%} ({len(matched_primes)}/{len(prime_pool)} primes)")
    
    # Step 5: Try CRT + rational reconstruction for promising vectors
    if debug:
        print(f"\n{'='*70}")
        print("TESTING CRT + RATIONAL RECONSTRUCTION")
        print(f"{'='*70}")
    
    viable_reconstructions = []
    
    # Sort vectors by coverage (best first)
    sorted_vectors = sorted(
        vector_coverage.items(),
        key=lambda x: x[1]['coverage_fraction'],
        reverse=True
    )
    
    for v_tuple, cov_info in sorted_vectors:
        if cov_info['num_matched'] < MIN_PRIME_SUBSET_SIZE:
            continue  # Not enough primes for a viable subset
        
        matched_primes = cov_info['matched_primes']
        
        if debug:
            print(f"\nTesting vector {v_tuple[:3]}... ({cov_info['num_matched']} matched primes)")
        
        # Try subsets of various sizes
        found_for_this_vector = False
        for subset_size in range(MIN_PRIME_SUBSET_SIZE, 
                                 min(MIN_MAX_PRIME_SUBSET_SIZE, len(matched_primes)) + 1):
            
            # Heuristic: try up to 100 random subsets of this size
            import random
            max_subsets_to_try = min(100, len(list(combinations(matched_primes, subset_size))))
            
            subsets_to_try = random.sample(
                list(combinations(matched_primes, subset_size)),
                min(max_subsets_to_try, len(list(combinations(matched_primes, subset_size))))
            )
            
            for subset in subsets_to_try:
                subset_list = list(subset)
                
                # Get residues for this subset
                residues = tuple(residues_by_prime[p] for p in subset_list)
                
                # CRT lift
                try:
                    m0 = crt_cached(residues, tuple(subset_list))
                    M = 1
                    for p in subset_list:
                        M *= int(p)
                except Exception:
                    raise
                    continue
                
                # Check if target_m = m0 + t*M for some small |t|
                # target_m = a/b, so we need: a/b = m0 + t*M
                # => a = b*(m0 + t*M) = b*m0 + b*t*M
                # => t = (a - b*m0) / (b*M)
                
                numerator = a - b * m0
                denominator = b * M
                
                if numerator % denominator == 0:
                    t = numerator // denominator
                    
                    if abs(t) <= tmax:
                        m_reconstructed = QQ(m0 + t * M)
                        
                        if m_reconstructed == target_m:
                            viable_reconstructions.append({
                                'vector': v_tuple,
                                'subset': subset_list,
                                'subset_size': len(subset_list),
                                'm0': m0,
                                'M': M,
                                't': t,
                                'm_reconstructed': m_reconstructed
                            })
                            
                            if debug:
                                print(f"  âœ“ FOUND via subset {subset_list}")
                                print(f"    m0={m0}, M={M}, t={t}")
                                print(f"    m = {m0} + {t}*{M} = {m_reconstructed}")
                            
                            found_for_this_vector = True
                            break  # Found one, that's enough for this subset size
                
                # Also try rational reconstruction
                try:
                    a_recon, b_recon = rational_reconstruct(m0 % M, M)
                    m_recon = QQ(a_recon) / QQ(b_recon)
                    
                    if m_recon == target_m:
                        viable_reconstructions.append({
                            'vector': v_tuple,
                            'subset': subset_list,
                            'subset_size': len(subset_list),
                            'm0': m0,
                            'M': M,
                            't': 'rational_recon',
                            'm_reconstructed': m_recon
                        })
                        
                        if debug:
                            print(f"  âœ“ FOUND via rational reconstruction on subset {subset_list}")
                            print(f"    m0={m0}, M={M}")
                            print(f"    Reconstructed: {a_recon}/{b_recon} = {m_recon}")
                        
                        found_for_this_vector = True
                        break
                
                except RationalReconstructionError:
                    raise
            
            if found_for_this_vector:
                break  # Found it for this vector, move to next vector
    
    # Step 6: Summary
    if debug:
        print(f"\n{'='*70}")
        print("SUMMARY")
        print(f"{'='*70}")
        print(f"Target: x = {target_x_q}, m = {target_m}")
        print(f"Total vectors: {len(vecs)}")
        print(f"Vectors with any coverage: {sum(1 for v in vector_coverage.values() if v['num_matched'] > 0)}")
        print(f"Viable reconstructions found: {len(viable_reconstructions)}")
        
        if viable_reconstructions:
            print(f"\nâœ“ POINT IS FINDABLE")
            print(f"\nExample reconstructions:")
            for i, recon in enumerate(viable_reconstructions[:3]):
                print(f"\n  [{i+1}] Vector: {recon['vector'][:3]}...")
                print(f"      Subset size: {recon['subset_size']}")
                print(f"      Primes: {recon['subset']}")
                print(f"      t: {recon['t']}")
        else:
            print(f"\nâœ— POINT NOT FINDABLE with current search parameters")
    
    return {
        'target_x': target_x_q,
        'target_m': target_m,
        'residues_by_prime': residues_by_prime,
        'vector_coverage': vector_coverage,
        'viable_reconstructions': viable_reconstructions,
        'is_findable': len(viable_reconstructions) > 0
    }


# In search_lll.py, replace the existing _process_prime_subset_precomputed function with this:

# In search_lll.py
def search_lattice_modp_unified_parallel(cd, current_sections, prime_pool, height_bound,
                                         vecs, rhs_list, r_m, shift,
                                         all_found_x, num_subsets, rationality_test_func,
                                         sconf, num_workers=8, debug=DEBUG):
    """
    Unified parallel search using ProcessPoolExecutor throughout.
    Hardened against the "filtered to 0 subsets" failure:
      - require primes to have actual residues (not just empty mappings)
      - compute numeric residue sets per-prime and use those counts for combo estimates
      - fall back deterministically if coverage-based generator returns nothing
    Returns: new_xs, new_sections, precomputed_residues, stats
    """
    # === UNPACK: SCONF ===
    min_prime_subset_size = sconf['MIN_PRIME_SUBSET_SIZE']
    min_max_prime_subset_size = sconf['MIN_MAX_PRIME_SUBSET_SIZE']
    max_modulus = sconf['MAX_MODULUS']
    tmax = sconf['TMAX']

    # === STATS: INIT ===
    stats = SearchStats()

    from bounds import compute_residue_counts_for_primes  # if not already imported
    residue_counts = compute_residue_counts_for_primes(cd, rhs_list, prime_pool, max_primes=30)
    coverage_estimator = CoverageEstimator(prime_pool, residue_counts)

    print("prime pool used for search:", prime_pool)

    # === PHASE: PREP MOD DATA ===
    stats.start_phase('prep_mod_data')
    print("--- Preparing modular data for LLL search ---")
    Ep_dict, rhs_modp_list, mult_lll, vecs_lll = prepare_modular_data_lll(
        cd, current_sections, prime_pool, rhs_list, vecs, stats, search_primes=prime_pool
    )
    stats.end_phase('prep_mod_data')

    if not Ep_dict:
        print("No valid primes found for modular search. Aborting.")
        return set(), [], {}, stats  # <-- Return stats

    # === PHASE: PRECOMPUTE RESIDUES ===
    stats.start_phase('precompute_residues')
    primes_to_compute = list(Ep_dict.keys())
    num_rhs_fns = len(rhs_list)
    vecs_list = list(vecs)

    args_list = [
        (
            p,
            Ep_dict[p],
            mult_lll.get(p, {}),
            vecs_lll.get(p, [tuple([0] * len(current_sections)) for _ in vecs_list]),
            vecs_list,
            rhs_modp_list,
            num_rhs_fns,
            stats  # pass the stats object (worker ignores if not used)
        )
        for p in primes_to_compute
    ]

    precomputed_residues = {}
    total_modular_checks = 0

    try:
        ctx = multiprocessing.get_context("fork")
        exec_kwargs = {"max_workers": num_workers, "mp_context": ctx}
    except Exception:
        exec_kwargs = {"max_workers": num_workers}
        raise

    with ProcessPoolExecutor(**exec_kwargs) as executor:
        futures = {executor.submit(_compute_residues_for_prime_worker, args): args[0] for args in args_list}
        for future in tqdm(as_completed(futures), total=len(futures), desc="Pre-computing residues"):
            p = futures[future]
            try:
                p_ret, mapping, local_modular_checks = future.result()
                mapping = mapping or {}
                precomputed_residues[p_ret] = mapping
                total_modular_checks += int(local_modular_checks or 0)

                # Now compute the union of numeric residues (ignore non-int markers)
                residues_union = set()
                for vtuple, rhs_lists in mapping.items():
                    for rl in rhs_lists:
                        # ignore sentinel markers like "DEN_ZERO" or other non-integer entries
                        for r in rl:
                            if isinstance(r, int):
                                residues_union.add(r)

                stats.residues_by_prime[p_ret].update(residues_union)

                # update main counters per-prime
                stats.counters['modular_checks'] += int(local_modular_checks or 0)
                stats.counters[f'modular_checks_p_{p_ret}'] += int(local_modular_checks or 0)
                stats.counters[f'residues_seen_p_{p_ret}'] = len(stats.residues_by_prime[p_ret])

            except Exception as e:
                if debug:
                    print(f"[precompute fail] p={p}: {e}")
                precomputed_residues[p] = {}
                stats.residues_by_prime[p].update(set())
                stats.counters[f'modular_checks_p_{p}'] = 0
                stats.counters[f'residues_seen_p_{p}'] = 0
                raise

    if debug:
        print(f"[precompute] total_modular_checks={total_modular_checks}, primes precomputed={len(precomputed_residues)}")

    stats.end_phase('precompute_residues')


    ##### WHY ISN'T A PARTICULAR FIBRATION FINDING A POINT?  FIND OUT HERE!

    if TARGETED_X: # comment out when not in use

        ret = diagnose_missed_point(TARGETED_X, r_m, shift, precomputed_residues, prime_pool, vecs)
        #print("ret=", ret)
        matched_subset = None
        if 'matched_primes' in ret:
            matched_subset = ret['matched_primes']

        const = r_m(m=0)
        mtarget = QQ(-1)*TARGETED_X+const

        cov1 = compute_residue_coverage_for_m(mtarget, precomputed_residues, PRIME_POOL)
        print("cov1: m = ", mtarget, " coverage:", cov1['coverage_fraction'])
        print("cov1: matched primes:", cov1['matched_primes'])


    # Build a per-prime numeric residue set for later use (and require non-empty)
    residues_by_prime_numeric = {}
    for p, mapping in precomputed_residues.items():
        residues_set = set()
        for vtuple, rhs_lists in mapping.items():
            for rl in rhs_lists:
                for r in rl:
                    if isinstance(r, int):
                        residues_set.add(r)
        residues_by_prime_numeric[p] = residues_set

    # Only keep primes that actually gave numeric residues (not merely empty mappings)
    usable_primes = [p for p in prime_pool if p in residues_by_prime_numeric and residues_by_prime_numeric[p]]
    if not usable_primes:
        print("No primes have numeric precomputed residues. Aborting.")
        return set(), [], precomputed_residues, stats
    if len(usable_primes) < len(prime_pool):
        if debug:
            print(f"[filter] Removed {len(prime_pool) - len(usable_primes)} primes with no numeric data. Using {len(usable_primes)} usable primes.")
        prime_pool = usable_primes

    # === PHASE: AUTOTUNE PRIMES ===
    stats.start_phase('autotune_primes')
    prime_stats = estimate_prime_stats(prime_pool, precomputed_residues, vecs_list, num_rhs=len(rhs_list))
    auto_extra_primes = choose_extra_primes(prime_stats,
                                            target_density=EXTRA_PRIME_TARGET_DENSITY,
                                            max_extra=EXTRA_PRIME_MAX,
                                            skip_small=EXTRA_PRIME_SKIP)
    extra_primes_for_filtering = auto_extra_primes
    stats.end_phase('autotune_primes')

    # Filtering stage: compute product estimate using distinct numeric residues per prime
    combo_cap = ceil(50000**(7*min_prime_subset_size/3)) # too many residues for this prime subset, too many possibilities, modular constraints are too loose
    roots_threshold = ROOTS_THRESHOLD
    if debug:
        print("combo_cap:", combo_cap, "roots_threshold:", roots_threshold)

    # === PHASE: GEN SUBSETS ===
    stats.start_phase('gen_subsets')
    prime_subsets_initial = generate_biased_prime_subsets_by_coverage(
        prime_pool=prime_pool,
        precomputed_residues=precomputed_residues,
        vecs=vecs_list,
        num_subsets=num_subsets,
        min_size=min_prime_subset_size,
        max_size=min_max_prime_subset_size,
        combo_cap=combo_cap,
        seed=SEED_INT,
        force_full_pool=False,
        debug=debug
    )
    stats.incr('subsets_generated_initial', n=len(prime_subsets_initial))

    filtered_subsets = []
    for subset in prime_subsets_initial:
        est = 1
        is_viable = True
        for p in subset:
            residues_set = residues_by_prime_numeric.get(p, set())
            roots_count = len(residues_set)
            if roots_count == 0:
                is_viable = False
                break
            # if any single prime has more residues than the threshold, it's likely to explode
            if roots_count > roots_threshold:
                est *= roots_count
                if est > combo_cap:
                    is_viable = False
                    break
            else:
                est *= max(1, roots_count)
                if est > combo_cap:
                    is_viable = False
                    break
        if is_viable and est <= combo_cap:
            filtered_subsets.append(subset)

    filtered_out_count = len(prime_subsets_initial) - len(filtered_subsets)
    stats.incr('subsets_filtered_out_combo', n=filtered_out_count)
    if debug:
        print("Generated", len(prime_subsets_initial), "prime_subsets -> filtered to", len(filtered_subsets))
    prime_subsets_to_process = filtered_subsets
    stats.prime_subsets = prime_subsets_to_process

    #### if missing a point, assert your matched subset is contained in the used ones
    if TARGETED_X: # commented out when not using/debugging
        assert matched_subset is None or matched_subset in prime_subsets_to_process, (prime_subsets_to_process, matched_subset)

    count_subsets = {}
    for subset in prime_subsets_to_process:
        key = len(subset)
        if key in count_subsets:
            count_subsets[key] += 1
        else:
            count_subsets[key] = 0

    for key in sorted(list(count_subsets)):
        print("using", count_subsets[key], "subsets of len =", key)

    # If filtering removed everything, build a deterministic fallback pool of small subsets.
    if not prime_subsets_to_process:
        if debug:
            print("[fallback] coverage-based filtering removed all subsets. Building deterministic fallback subsets.")
        from itertools import combinations
        fallback = []
        max_k = min(6, len(prime_pool))
        # prefer sizes 3..max_k
        for k in range(3, max_k + 1):
            for comb in combinations(prime_pool, k):
                # only keep combos with at least one residue per prime
                good = True
                for p in comb:
                    if not residues_by_prime_numeric.get(p):
                        good = False
                        break
                if not good:
                    continue
                # estimate as above
                est = 1
                for p in comb:
                    est *= max(1, len(residues_by_prime_numeric[p]))
                    if est > combo_cap:
                        good = False
                        break
                if good:
                    fallback.append(list(comb))
                if len(fallback) >= max(1, num_subsets):
                    break
            if len(fallback) >= max(1, num_subsets):
                break
        if fallback:
            prime_subsets_to_process = fallback[:num_subsets]
            if debug:
                print(f"[fallback] Using {len(prime_subsets_to_process)} deterministic fallback subsets.")
        else:
            # give up cleanly
            print("No viable prime subsets generated or remaining after filtering. Aborting.")
            stats.end_phase('gen_subsets')
            print("\n--- Search Statistics (No Subsets) ---")
            print(stats.summary_string())
            return set(), [], precomputed_residues, stats

    stats.end_phase('gen_subsets')

    # === PHASE: SEARCH & CHECK ===
    stats.start_phase('search_subsets_and_check')
    worker_func = partial(
        _process_prime_subset_precomputed,
        vecs=vecs_list,
        r_m=r_m,
        shift=shift,
        tmax=tmax,
        combo_cap=combo_cap,
        precomputed_residues=precomputed_residues,
        prime_pool=prime_pool,  # current (filtered) prime_pool
        num_rhs_fns=len(rhs_list)
    )

    subset_results_list, worker_stats_dict, all_crt_classes = search_prime_subsets_unified(
        prime_subsets_to_process, worker_func, num_workers=num_workers, debug=debug
    )

    # *** THIS IS THE FIX for "CRT-consistent samples: 0" ***
    # Save the collected CRT classes to the main stats object
    stats.crt_classes_tested = all_crt_classes

    # update coverage estimator
    coverage_estimator.tested_classes = all_crt_classes
    coverage_report = coverage_estimator.estimate_coverage(prime_subsets_to_process)

    if debug:
        print("\n--- Coverage Estimate ---")
        if coverage_report.get('direct_coverage') is not None:
            print(f"  Direct coverage: {100 * coverage_report['direct_coverage']:.2f}%")
        if coverage_report.get('birthday_coverage') is not None:
            print(f"  Birthday estimate: {100 * coverage_report['birthday_coverage']:.2f}%")
        print(f"  Heuristic (density): {100 * coverage_report.get('heuristic_coverage', 0):.4f}%")
        print(f"  CRT classes tested: {coverage_report.get('classes_tested', 0):,}")
        print(f"  Search space size: ~{coverage_report.get('space_size_estimate', 0):.2e}")
        additional_runs = coverage_estimator.recommend_additional_runs(prime_subsets_to_process, target_coverage=0.95)
        if additional_runs > 0:
            print(f"  âš ï¸  Recommend {additional_runs} more run(s) to reach 95% coverage")

    # Merge worker stats collected by the manager
    stats.merge_dict(worker_stats_dict)
    stats.incr('subsets_processed', n=len(subset_results_list))

    # aggregate worker candidates
    overall_found_candidates_from_workers = set()
    productive_subsets_data = []
    for subset, candidates_set, _ in subset_results_list:
        overall_found_candidates_from_workers.update(candidates_set)
        if candidates_set:
            productive_subsets_data.append({
                'primes': subset,
                'size': len(subset),
                'candidates': len(candidates_set)
            })

    stats.incr('crt_candidates_found', n=len(overall_found_candidates_from_workers))

    # Batch check rationality
    print(f"\nChecking rationality for {len(overall_found_candidates_from_workers)} unique candidates...")
    final_rational_candidates = set()
    candidate_list = list(overall_found_candidates_from_workers)
    if not candidate_list:
        stats.end_phase('search_subsets_and_check')
        print("\n--- Search Statistics (No Points Found) ---")
        print(stats.summary_string())
        return set(), [], precomputed_residues, stats

    batch_size = max(1, floor(0.05 * len(candidate_list)))
    for i in range(0, len(candidate_list), batch_size):
        batch = candidate_list[i:i + batch_size]
        newly_rational = _batch_check_rationality(
            batch, r_m, shift, rationality_test_func, current_sections, stats
        )
        final_rational_candidates.update(newly_rational)
        if debug:
            print(f"[batch check] processed {min(i + batch_size, len(candidate_list))}/{len(candidate_list)}, found {len(final_rational_candidates)} rational so far")

    stats.end_phase('search_subsets_and_check')

    # print productivity stats
    try:
        _print_subset_productivity_stats(productive_subsets_data, prime_subsets_to_process)
    except Exception as e:
        if debug:
            print(f"Failed to print productivity stats: {e}")
        raise

    if not final_rational_candidates:
        print("\n--- Search Statistics (No Points Found) ---")
        print(stats.summary_string())
        return set(), [], precomputed_residues, stats

    print(f"\nFound {len(final_rational_candidates)} rational (m, vector) pairs after checking.")

    # === PHASE: POST PROCESS ===
    stats.start_phase('post_process')
    sample_pts = []
    new_sections_raw = []
    processed_m_vals = {}

    for m_val, v_tuple in final_rational_candidates:
        if m_val in processed_m_vals:
            continue
        try:
            x_val = r_m(m=m_val) - shift
            y_val = rationality_test_func(x_val)
            if y_val is not None:
                v = vector(QQ, v_tuple)
                sample_pts.append((x_val, y_val))
                processed_m_vals[m_val] = v
                if any(c != 0 for c in v):
                    new_sec = sum(v[i] * current_sections[i] for i in range(len(current_sections)))
                    new_sections_raw.append(new_sec)
        except (TypeError, ZeroDivisionError, ArithmeticError):
            continue

    new_xs = {pt[0] for pt in sample_pts}
    new_sections = list({s: None for s in new_sections_raw}.keys())
    stats.incr('rational_points_unique', n=len(new_xs))
    stats.incr('new_sections_unique', n=len(new_sections))
    stats.end_phase('post_process')

    print("\n--- Search Statistics ---")
    print(stats.summary_string())

    return new_xs, new_sections, precomputed_residues, stats

# In search_lll.py
def search_lattice_symbolic(cd, current_sections, vecs, rhs_list, r_m, shift,
                            all_found_x, rationality_test_func, stats):
    """
    Symbolic search for rational points via solving x_sv == rhs(m) over QQ(m).

    Controlled by the SYMBOLIC_SEARCH flag from search_common.py. If SYMBOLIC_SEARCH is False,
    this is a no-op and returns empty results quickly.
    """
    # Respect the global flag; search_common.py should define SYMBOLIC_SEARCH (all-caps).
    # We do not import here; search_common is already imported at top of file.
    SYMBOLIC_ENABLED = globals().get('SYMBOLIC_SEARCH', False)
    if not SYMBOLIC_ENABLED:
        if DEBUG:
            print("Symbolic search disabled by SYMBOLIC_SEARCH flag.")
        return set(), []

    if not current_sections:
        if DEBUG:
            print("Symbolic search: no current sections provided, skipping.")
        return set(), []

    print("--- Starting symbolic search over QQ ---")
    stats.start_phase('symbolic_search') # <-- STATS

    # Canonical setup for m (use PR_m and its fraction field so arithmetic stays in QQ(m))
    PR_m = PolynomialRing(QQ, 'm')
    SR_m = var('m')
    Fm = PR_m.fraction_field()

    newly_found_x = set()
    new_sections = []
    found_x_to_section_map = {}

    # Quick sanity: ensure sections are projective-like and have x/z
    # (use assert to make developer intent explicit)
    assert all(len(sec) >= 3 for sec in current_sections), "current_sections entries must be 3-coord sections"

    # Main search: iterate over integer vectors (vecs) and solve numerator==0 over QQ
    # NOTE: we do NOT loop over rational m values; instead we solve for m via polynomial roots.
    for v_tuple in tqdm(vecs, desc="Symbolic Search"):
        if all(int(c) == 0 for c in v_tuple):
            continue

        v = vector(ZZ, [int(c) for c in v_tuple])
        #print("trying search vector:", v) # Reduced verbosity
        S_v = sum(v[i] * current_sections[i] for i in range(len(current_sections)))

        # skip degenerate/new-section-zero cases
        if S_v.is_zero():
            #print("search section is zero; skipping.")
            continue
        if S_v[2].is_zero():
            # projective z==0 (point at infinity) â€” skip
            #print("search section is point at infinity; skipping.")
            continue

        # Affine x-coordinate in QQ(m) (attempt to coerce)
        try:
            x_sv_raw = S_v[0] / S_v[2]
            x_coerced = Fm(SR(x_sv_raw))
        except Exception:
            # If coercion fails, skip this vector (diagnostic if DEBUG)
            if DEBUG:
                print("Symbolic coercion failed for a section; skipping vector:", v_tuple)
            # raise # Let's not raise here unless debugging is critical
            continue
        #print("search x:", x_coerced)

        for rhs_func in rhs_list:
            stats.incr('symbolic_solves_attempted') # <-- STATS
            try:
                rhs_coerced = Fm(SR(rhs_func))
                diff = x_coerced - rhs_coerced
                num = diff.numerator()
            except Exception:
                if DEBUG:
                    print("Symbolic coercion of rhs failed; skipping this rhs.")
                raise
                continue

            # If numerator is constant, there is no m-solution
            if num.degree() == 0:
                #print("numerator is constant; no solution")
                continue

            # Build polynomial in PR_m and get rational roots
            try:
                num_poly = PR_m(num)   # coerce numerator into QQ[m]
            except Exception:
                if DEBUG:
                    print("Could not coerce numerator into PR_m; skipping.")
                raise
                continue

            try:
                roots = num_poly.roots(ring=QQ, multiplicities=False)
            except Exception:
                # If root-finding over QQ fails, skip (better to fail loudly during debugging)
                if DEBUG:
                    print("num_poly.roots(...) failed for polynomial:", num_poly)
                raise
                continue

            if not roots:
                #print("no roots found")
                pass # This happens often, no need to print
            else:
                stats.incr('symbolic_solves_success', n=len(roots)) # <-- STATS
                if DEBUG: print("Symbolic solve success! Found root(s):", roots)

            # For each rational root m0, verify equality by evaluation (clearing denominators),
            # then test rationality and add the point.
            for m_val in roots:
                m_q = QQ(m_val)   # ensure rational

                # Evaluate LHS and RHS using SR substitution to get exact rationals where possible
                try:
                    lhs_at = SR(x_sv_raw).subs({SR_m: m_q})
                    rhs_at = SR(rhs_func).subs({SR_m: m_q})
                except Exception:
                    if DEBUG:
                        print("SR substitution failed at m=", m_q)
                    raise
                    continue

                # Try coercion to QQ for reliable equality checks
                try:
                    lhs_q = QQ(lhs_at)
                    rhs_q = QQ(rhs_at)
                except Exception:
                    # If we cannot coerce either side, fall back to clearing denominators
                    try:
                        lhs_q = QQ(r_m(m=m_q) - shift)
                    except Exception:
                        if DEBUG:
                            print("Failed to compute numeric r_m at m=", m_q)
                        raise
                        continue
                    # We cannot easily compute rhs numeric without r_m; but if lhs_q is defined,
                    # we can proceed to rationality test as before.
                    rhs_q = None

                # If we have both sides as QQ check equality; otherwise trust the root machinery but still verify via r_m
                if rhs_q is not None and lhs_q != rhs_q:
                    if DEBUG:
                        print("Symbolic-match FAIL for root m =", m_q, "; lhs != rhs after coercion.")
                    raise
                    continue

                # Compute x via r_m (exact rational) and apply shift
                try:
                    x_val = r_m(m=m_q) - shift
                except Exception:
                    if DEBUG:
                        print("r_m evaluation failed at m=", m_q)
                    raise
                    continue

                # Avoid duplicates
                try:
                    x_val_q = QQ(x_val)
                except Exception:
                    # if not rational-coercible, skip
                    if DEBUG:
                        print("x_val not coercible to QQ at m=", m_q, "; skipping")
                    raise
                    continue

                if x_val_q in all_found_x or x_val_q in newly_found_x:
                    #print("found x already seen:", x_val_q)
                    continue

                # Check rationality of y via rationality_test_func
                stats.incr('rationality_tests_total') # <-- STATS (Symbolic path)
                y_val = rationality_test_func(x_val_q)
                if y_val is None:
                    stats.record_failure(m_q, reason='y_not_rational_symbolic') # <-- STATS
                    #print("yval is None; x value found does not give rational point.")
                    # not a rational point
                    continue

                # Found a new rational point
                stats.record_success(m_q, point=x_val_q) # <-- STATS (Symbolic path)
                newly_found_x.add(x_val_q)
                found_x_to_section_map[x_val_q] = S_v
                new_sections.append(S_v)

                if DEBUG:
                    print("Found new rational point via symbolic m =", m_q, " x =", x_val_q)

    # OPTIONAL ASSERT: if the user expects the base m to be discovered, allow caller to check
    # The assert function lives in this module: assert_base_m_found(...)
    stats.end_phase('symbolic_search') # <-- STATS
    return newly_found_x, new_sections


def check_specific_t_value(t_candidate, m0, M, residue_map_for_filter, extra_primes, verbose=False):
    """
    Checks if a *single* integer t is valid against the extra prime constraints.
    This is an O(1) check, unlike the O(tmax) search.
    """
    m0 = int(m0)
    M = int(M)
    t_candidate = int(t_candidate)

    for q in extra_primes:
        allowed_m_residues = residue_map_for_filter.get(q)

        # If this prime has no allowed residues for this vector, fail.
        if not allowed_m_residues:
            if verbose:
                print(f"Filter fail: Prime {q} has no allowed m-residues.")
            return False

        # Compute what m = m0 + t*M is modulo q
        m_cand_mod_q = (m0 + t_candidate * M) % q

        # Check if this m is in the allowed set.
        if m_cand_mod_q not in allowed_m_residues:
            if verbose: print(f"Filter fail: t={t_candidate} -> m={m_cand_mod_q} (mod {q}) not in allowed set.")
            return False

    # If we passed all primes, this t is valid.
    if verbose: print(f"Filter pass: t={t_candidate} is valid.")
    return True


def _process_prime_subset_precomputed(p_subset, vecs, r_m, shift, tmax, combo_cap, precomputed_residues, prime_pool, num_rhs_fns):
    """
    Worker function to find m-candidates for a single subset of primes.
    This version processes each RHS function independently.
    
    *** MODIFIED to use O(1) t-filtering *after* O(1) t-minimization ***
    """
    if not p_subset:
        return set()

    found_candidates_for_subset = set()
    stats_counter = Counter()
    tested_crt_classes = set()

    # these are now skipped!  this shouldn't print anymore!
    if len(p_subset) > 1 and all(p in precomputed_residues for p in p_subset):
        est = 1
        for p in p_subset:
            vks = precomputed_residues[p]
            for roots_list in vks.values():
                # roots_list is a list of sets per RHS function
                if any(len(roots) > ROOTS_THRESHOLD for roots in roots_list):
                    est *= sum(len(roots) for roots in roots_list)
        if est > combo_cap and DEBUG:
            print("[heavy subset]", p_subset, "estimated combos:", est)

    num_extra_primes = 4
    offset = 2
    extra_primes_for_filtering = [p for p in prime_pool if p not in p_subset][offset:num_extra_primes+offset]

    for v_orig in vecs:
        if all(c == 0 for c in v_orig):
            continue
        v_orig_tuple = tuple(v_orig)

        for rhs_idx in range(num_rhs_fns):
            
            residue_map_for_crt = {}
            for p in p_subset:
                # precomputed_residues[p][v_tuple] is now a list of sets
                roots_for_this_rhs = precomputed_residues.get(p, {}).get(v_orig_tuple, [])
                if rhs_idx < len(roots_for_this_rhs) and roots_for_this_rhs[rhs_idx]:
                    residue_map_for_crt[p] = roots_for_this_rhs[rhs_idx]

            primes_for_crt = list(residue_map_for_crt.keys())
            if len(primes_for_crt) < MIN_PRIME_SUBSET_SIZE:
                continue

            residue_map_for_filter = {}
            for p in extra_primes_for_filtering:
                roots_for_this_rhs = precomputed_residues.get(p, {}).get(v_orig_tuple, [])
                if rhs_idx < len(roots_for_this_rhs) and roots_for_this_rhs[rhs_idx]:
                     residue_map_for_filter[p] = roots_for_this_rhs[rhs_idx]

            lists = [residue_map_for_crt[p] for p in primes_for_crt]
            
            for combo in itertools.product(*lists):
                stats_counter['crt_lift_attempts'] += 1
                M = 1
                for p in primes_for_crt:
                    M *= int(p)

                if M > MAX_MODULUS:
                    continue

                m0 = crt_cached(combo, tuple(primes_for_crt))
                tested_crt_classes.add((int(m0) % int(M), int(M)))

                # --- START MODIFICATION ---
                # Path 1: t-search (now O(1) find + O(1) filter)
                
                # Step 1: Find the 1-3 best t candidates (O(1))
                try:
                    #best_ms = minimize_archim_search_lll.pyidean_t_linear_const(int(m0), int(M), r_m, shift, tmax)
                    best_ms = minimize_archimedean_t_linear_const(int(m0), int(M), r_m, shift, tmax)
                except TypeError:
                    best_ms = [(0, QQ(m0 + t * M), 0, 0.0) for t in (-1, 0, 1)] # t, m, x, score
                    raise

                # Step 2: Filter *only* those t values (O(1))
                for t_cand, m_cand, _, _ in best_ms:
                    if check_specific_t_value(t_cand, m0, M, residue_map_for_filter, extra_primes_for_filtering):
                        # --- THIS IS THE FIX ---
                        # Wrap (m_cand, v_orig_tuple) in an outer tuple to add it as a single element
                        found_candidates_for_subset.add( (QQ(m_cand), v_orig_tuple) )

                # Path 2: Rational Reconstruction (still run unconditionally)
                stats_counter['rational_recon_attempts_worker'] += 1
                try:
                    a, b = rational_reconstruct(m0 % M, M)
                    # --- THIS IS THE FIX ---
                    # Wrap in an outer tuple here as well
                    found_candidates_for_subset.add( (QQ(a) / QQ(b), v_orig_tuple) )
                    stats_counter['rational_recon_success_worker'] += 1
                except RationalReconstructionError:
                    stats_counter['rational_recon_failure_worker'] += 1
                    raise
                # --- END MODIFICATION ---

    return found_candidates_for_subset, stats_counter, tested_crt_classes



# In search_lll.py

def _compute_residues_for_prime_worker(args):
    """
    Worker computing residues for one prime with Hensel filtering.
    Returns (p, result_for_p, local_modular_checks)
    - result_for_p: { v_orig_tuple : [set(roots_for_rhs0), set(roots_for_rhs1), ...] }
    - local_modular_checks: integer count of attempted modular RHS checks
    """
    from sage.all import GF, Integer, QQ, ZZ
    
    # Unpack args
    try:
        p, Ep_local, mults_p, vecs_lll_p, vecs_list, rhs_modp_list_local, num_rhs, _stats = args
    except Exception:
        p, Ep_local, mults_p, vecs_lll_p, vecs_list, rhs_modp_list_local, num_rhs = args
        _stats = None

    result_for_p = {}
    local_modular_checks = 0

    # Hensel filtering tunables
    HENSEL_STRICT = HENSEL_SLOPPY # fast, but may reject too much
    HENSEL_ALLOW_WEAK = True

    for idx, v_orig in enumerate(vecs_list):
        v_orig_tuple = tuple(v_orig)

        if all(c == 0 for c in v_orig):
            result_for_p[v_orig_tuple] = [set() for _ in range(num_rhs)]
            continue

        try:
            v_p_transformed = vecs_lll_p[idx]
        except Exception:
            result_for_p[v_orig_tuple] = [set() for _ in range(num_rhs)]
            raise

        # Build Pm = sum of section multiples
        try:
            Pm = Ep_local(0)
        except Exception:
            result_for_p[v_orig_tuple] = [set() for _ in range(num_rhs)]
            raise

        for j, coeff in enumerate(v_p_transformed):
            try:
                mpj = mults_p[j]
            except Exception:
                raise

            if mpj is None:
                continue

            try:
                key = int(coeff)
                if hasattr(mpj, 'get'):
                    if key in mpj:
                        Pm += mpj[key]
                else:
                    if 0 <= key < len(mpj):
                        Pm += mpj[key]
            except Exception:
                raise

        try:
            if Pm.is_zero():
                result_for_p[v_orig_tuple] = [set() for _ in range(num_rhs)]
                continue
        except Exception:
            raise

        # Process each RHS function
        roots_by_rhs = []
        for i_rhs in range(num_rhs):
            roots_for_rhs = set()
            rhs_map = rhs_modp_list_local[i_rhs]

            if p not in rhs_map:
                roots_by_rhs.append(roots_for_rhs)
                continue

            rhs_p = rhs_map[p]

            if rhs_p is None:
                roots_by_rhs.append(roots_for_rhs)
                continue

            # Check denominator Pm[2]
            den = Pm[2] if hasattr(Pm, '__getitem__') else None
            den_modp = 0
            
            if den is not None:
                try:
                    den_modp = int(ZZ(den)) % int(p)
                except Exception:
                    try:
                        if hasattr(den, "numerator") and hasattr(den, "denominator"):
                            num = den.numerator()
                            deno = den.denominator()
                            if hasattr(num, "__call__"):
                                num = num(0)
                            if hasattr(deno, "__call__"):
                                deno = deno(0)
                            num = ZZ(num)
                            deno = ZZ(deno)
                            den_modp = int(num * pow(deno, -1, int(p)) % int(p))
                        else:
                            den_modp = int(GF(int(p))(den))
                    except Exception:
                        raise
            
            if den_modp == 0:
                roots_by_rhs.append(roots_for_rhs)
                continue

            # Compute numerator of (Pm[0]/Pm[2] - rhs_p)
            num_expr = (Pm[0] / Pm[2] - rhs_p).numerator()

            try:
                if num_expr.is_zero():
                    roots_by_rhs.append(roots_for_rhs)
                    continue
            except Exception:
                raise

            local_modular_checks += 1

            # Find roots in GF(p)
            try:
                Fp = GF(p)
                raw_roots = num_expr.roots(ring=Fp, multiplicities=False)
            except Exception:
                try:
                    num_modp = num_expr.change_ring(Fp)
                    raw_roots = [r for r, _ in num_modp.roots(multiplicities=True)]
                except Exception:
                    raise

            # Normalize to integers
            normalized_raw_roots = []
            for r in raw_roots:
                try:
                    normalized_raw_roots.append(int(r))
                except Exception:
                    try:
                        normalized_raw_roots.append(int(r[0]))
                    except Exception:
                        raise

            if not normalized_raw_roots:
                roots_by_rhs.append(roots_for_rhs)
                continue

            # Hensel simple-root filtering via derivative
            simple_roots = set()
            deriv = None
            
            try:
                if hasattr(num_expr, 'numerator'):
                    deriv = num_expr.numerator().derivative()
                else:
                    deriv = num_expr.derivative()
            except Exception:
                raise

            for r in normalized_raw_roots:
                keep_root = True
                
                if HENSEL_STRICT and deriv is not None:
                    try:
                        # Evaluate derivative at r modulo p
                        try:
                            deriv_modp = deriv.change_ring(Fp)
                            dval = int(deriv_modp(Fp(r)))
                        except Exception:
                            try:
                                dval = int(deriv(r))
                            except Exception:
                                raise
                        
                        if dval % int(p) == 0:
                            keep_root = False
                    except Exception:
                        keep_root = not HENSEL_STRICT
                        raise

                if keep_root:
                    simple_roots.add(int(r))

            # Decide what to keep
            if simple_roots:
                roots_for_rhs.update(simple_roots)
            else:
                if HENSEL_ALLOW_WEAK:
                    roots_for_rhs.update(normalized_raw_roots)

            # Debug output
            if DEBUG and normalized_raw_roots:
                rejected = len(normalized_raw_roots) - len(simple_roots)
                if rejected > 0:
                    print(f"[hensel p={p}, rhs={i_rhs}, v={v_orig_tuple[:2]}...] "
                          f"raw={len(normalized_raw_roots)}, simple={len(simple_roots)}, "
                          f"rejected={rejected}, kept={len(roots_for_rhs)}")

            roots_by_rhs.append(roots_for_rhs)

        result_for_p[v_orig_tuple] = roots_by_rhs

    return p, result_for_p, local_modular_checks



# Patched helper functions for search_lll pipeline
# Place this file into your repo and run your dedup.sh to merge/replace duplicates.
# These functions are defensive and intentionally accept optional callbacks so they
# can be dropped into your existing code with minimal change.

from sage.all import QQ, GF, PolynomialRing, gcd, EllipticCurve, Zmod
import math

# ---------------------------------------------------------------------------
# candidate_passes_extra_primes
# ---------------------------------------------------------------------------

# ---------------------------------------------------------------------------
# detect_fiber_collision
# ---------------------------------------------------------------------------

def hensel_lift_y(E, m_mod_p, x_mod_p, p, k=3):
    """
    Given an elliptic-curve polynomial model E in variables (m), attempt a
    simple Hensel-lift for y solving y^2 = x^3 + a4(m)*x + a6(m) modulo p^k.

    This is a *cheap* p-adic consistency check: it will return True if the
    congruence can be lifted to modulo p^k using standard Hensel/Newton when
    the derivative (2*y) is invertible along the lift.

    Requirements/assumptions:
      - E.change_ring(GF(p)) and the evaluation of a4,a6 at m_mod_p is available
      - x_mod_p is an integer in GF(p)

    Returns
    -------
    bool
        True if a Hensel lift to modulus p^k was successful, False otherwise.
    """
    # Quick checks
    if p == 2:
        # Hensel lifting for p=2 is more delicate; skip and fail conservatively.
        return False

    try:
        Fp = GF(p)
        E_mod = E.change_ring(Fp)
    except Exception:
        return False

    # evaluate RHS at provided x and m
    try:
        # a4(m) and a6(m) may be polynomials in m. We try to evaluate them.
        a4 = E.a4()
        a6 = E.a6()
        # If a4, a6 are functions of m (sage expressions), user must have
        # substituted m -> m_mod_p before calling this function.
        rhs_mod_p = (x_mod_p**3 + a4 * x_mod_p + a6) % p
    except Exception:
        # fallback: try the curve eval method if available
        try:
            rhs_mod_p = E_mod.right_hand_side(x_mod_p)
        except Exception:
            return False

    # Is rhs a quadratic residue mod p?
    try:
        rhs_int = int(rhs_mod_p)
    except Exception:
        # maybe an element of GF(p)
        rhs_int = int(rhs_mod_p.integer_representation())

    if rhs_int % p == 0:
        y0 = 0
    else:
        # try to find square root modulo p
        try:
            y0 = pow(rhs_int, (p + 1) // 4, p) if (p % 4 == 3) else None
            if y0 is None:
                # brute force small p
                y0 = next((t for t in range(p) if (t * t - rhs_int) % p == 0), None)
        except Exception:
            y0 = None

    if y0 is None:
        return False

    # Now do Newton lifting to modulus p^k
    p_pow = p
    y = y0
    for i in range(1, k):
        p_pow *= p
        # Evaluate F(y) = y^2 - rhs (mod p_pow)
        F_y = (y * y - rhs_int) % p_pow
        dF = (2 * y) % p_pow
        if math.gcd(dF, p) != 1:
            # derivative not invertible, cannot apply Newton
            return False
        # dF inverse modulo p_pow
        try:
            inv = pow(int(dF), -1, p_pow)
        except ValueError:
            return False
        y = (y - (F_y * inv)) % p_pow
    return True


# ---------------------------------------------------------------------------
# is_congruent_to_known_sections
# ---------------------------------------------------------------------------

def is_congruent_to_known_sections(candidate, known_sections, primes, E=None,
                                   threshold=3, logger=None):
    """
    Check whether `candidate` is congruent to any of the `known_sections`
    modulo at least `threshold` primes from `primes`.

    Parameters
    ----------
    candidate : object
        Candidate point representation; the caller must provide a callable
        `reduce_candidate_mod_p(candidate, p)` if candidate is not a simple
        tuple.
    known_sections : iterable
        Known global sections (points). Each should be reducible with
        `reduce_point_hom(., ., p)` above.
    primes : iterable of ints
        Primes to test reductions against.
    E : optional
        EllipticCurve model used for reductions (passed to reduce_point_hom).
    threshold : int
        Number of primes where congruence must hold to declare candidate a
        reduction of a known section.

    Returns
    -------
    (is_reduction, witness_info)
        is_reduction: bool
        witness_info: dict {section_index: count_of_primes_matched}
    """
    if E is None:
        # reduction helpers require a curve model; caller should provide one.
        raise ValueError("E (elliptic curve model) is required for reductions")

    counts = {i: 0 for i in range(len(known_sections))}

    for p in primes:
        # reduce candidate and known sections
        c_red = reduce_point_hom(E, candidate, p)
        if c_red is None:
            continue
        for i, s in enumerate(known_sections):
            s_red = reduce_point_hom(E, s, p)
            if s_red is None:
                continue
            # Compare points in the group law: use equality on reduced points
            try:
                if c_red == s_red:
                    counts[i] += 1
            except Exception:
                # fallback to coordinate comparison if necessary
                try:
                    if tuple(c_red) == tuple(s_red):
                        counts[i] += 1
                except Exception:
                    continue

    # Find if any section matched across `threshold` or more primes
    for i, c in counts.items():
        if c >= threshold:
            return True, {i: c}

    return False, counts


# End of patched helper functions
# -------------------------------
# Notes for integration:
# - candidate_passes_extra_primes now accepts an optional per_prime_test callback
#   so you can reuse your original per-prime logic without changing calls.
# - reduce_point_hom returns `None` when reduction fails (denominator non-invertible
#   or other issues). Callers should treat None explicitly.
# - detect_fiber_collision flags primes where Delta and its derivative share
#   a factor (colliding fibers). Use this to force extra inspection of such primes.
# - hensel_lift_y is a cheap p-adic consistency test to reduce false positives.
# - is_congruent_to_known_sections provides a multi-prime consistency test to
#   filter out candidates that are just reductions of global sections.

def compute_all_mults_for_section(Pi, E_mod_p, p, logger=None):
    """
    Compute small multiples of a reduced section Pi on the reduced curve E_mod_p.

    Returns:
      - dict {n: n*Pi} on success
      - None if Pi is None (reduction failed) or on unexpected error.

    This function is intentionally conservative: it returns None when Pi is None
    so callers can treat the prime as unusable for modular LLL data.
    """
    if Pi is None:
        if logger:
            logger(f"[compute_all_mults_for_section] Pi is None (reduction failed) at p={p}")
        return None

    try:
        # Try to get a sensible upper bound on how many multiples to compute.
        # If group order is cheap to compute for this prime, use it (bounded).
        max_mult = 20
        try:
            # E_mod_p.count_points may exist and be fast for small p
            order = int(E_mod_p.count_points()) if hasattr(E_mod_p, "count_points") else None
            if order is not None and order > 0:
                max_mult = min(max_mult, max(5, min(order, 200)))
        except Exception:
            # ignore expensive/failed count_points attempt
            pass

        computed = {}
        # identity: prefer E_mod_p(0) but be defensive
        try:
            identity = E_mod_p(0)
        except Exception:
            try:
                identity = E_mod_p(0, 1, 0)
            except Exception:
                identity = None

        computed[0] = identity
        computed[1] = Pi

        for n in range(2, max_mult + 1):
            try:
                computed[n] = n * Pi
            except Exception as e:
                # stop at first failure to multiply further
                if logger:
                    logger(f"[compute_all_mults_for_section] multiply failed n={n} at p={p}: {e}")
                break

        return computed

    except Exception as e:
        if logger:
            logger(f"[compute_all_mults_for_section] unexpected error at p={p}: {e}")
        return None



    def make_E_mod_p(p):
        """Attempt several strategies to obtain a reduced elliptic curve at p."""
        # Strategy 1: try cd.change_ring(GF(p))
        try:
            E_mod = cd.change_ring(GF(p))
            return E_mod
        except Exception:
            pass
        # Strategy 2: try Zmod
        try:
            E_mod = cd.change_ring(Zmod(p))
            return E_mod
        except Exception:
            pass
        # Strategy 3: if cd is a callable factory cd(p) (user-specific), try that
        try:
            if callable(cd):
                E_mod = cd(p)
                return E_mod
        except Exception:
            pass
        # If none worked, return None
        return None

    def safe_eval_rhs_mod_p(rhs_expr, p):
        """
        Evaluate an RHS rational function modulo p and return a reduced representation.
        Expect rhs_expr to be a sage expression or a callable that can be evaluated
        after substituting m->m0 or as symbol; adapt as needed for your code.
        """
        try:
            # If rhs_expr has a .change_ring or .mod or .subs method, try common patterns:
            if hasattr(rhs_expr, "change_ring"):
                return rhs_expr.change_ring(GF(p))
            # If it's a sage polynomial/expression in m, try to coerce coefficients mod p:
            # best-effort: rely on str-based fallback evaluation if necessary
            return rhs_expr  # caller may substitute per-m later
        except Exception:
            return None

    for p in prime_pool:
        # skip tiny primes known bad (optional)
        try:
            E_mod_p = make_E_mod_p(p)
            if E_mod_p is None:
                if logger:
                    logger(f"[prepare_modular_data_lll] could not form E_mod at p={p}; skipping")
                rejected_primes.append((p, "E_mod_missing"))
                continue
        except Exception as e:
            if logger:
                logger(f"[prepare_modular_data_lll] exception forming E_mod at p={p}: {e}; skipping")
            rejected_primes.append((p, f"E_mod_exception:{e}"))
            continue

        # Reduce each known global section; if any reduction fails, reject prime
        reduced_sections = []
        reduction_failed = False
        for i_sec, S in enumerate(current_sections):
            try:
                Pi_red = reduce_point_hom(E_mod_p, S, p, logger=logger)
            except Exception as e:
                Pi_red = None
                if logger:
                    logger(f"[prepare_modular_data_lll] reduce_point_hom raised for section #{i_sec} at p={p}: {e}")

            if Pi_red is None:
                # couldn't reduce this known section => prime unusable for modular LLL
                reduction_failed = True
                if logger:
                    logger(f"[prepare_modular_data_lll] section #{i_sec} reduction failed at p={p}; rejecting prime")
                break
            reduced_sections.append(Pi_red)

        if reduction_failed:
            rejected_primes.append((p, "section_reduction_failed"))
            continue

        # For each reduced section compute multiples
        mults_for_prime = {}
        any_mult_error = False
        for i_sec, Pi_red in enumerate(reduced_sections):
            mults = compute_all_mults_for_section(Pi_red, E_mod_p, p, logger=logger)
            if mults is None:
                any_mult_error = True
                if logger:
                    logger(f"[prepare_modular_data_lll] compute_all_mults_for_section failed for section #{i_sec} at p={p}")
                break
            mults_for_prime[i_sec] = mults

        if any_mult_error:
            rejected_primes.append((p, "mults_failed"))
            continue

        # Prepare RHS reductions (best-effort; many RHS are symbolic in m and
        # may only be fully evaluated at particular m-values later).
        rhs_reduced = []
        for rhs in rhs_list:
            try:
                rhs_r = safe_eval_rhs_mod_p(rhs, p)
                rhs_reduced.append(rhs_r)
            except Exception as e:
                rhs_reduced.append(None)
                if logger:
                    logger(f"[prepare_modular_data_lll] rhs eval failed at p={p}: {e}")

        # Attempt to reduce vecs if they are numeric/vectors; otherwise keep as-is
        vecs_reduced = []
        for v in vecs:
            try:
                # Try elementwise integer reduction if v is sequence-like
                if hasattr(v, "__iter__") and not isinstance(v, (str, bytes)):
                    reduced_v = []
                    ok_v = True
                    for comp in v:
                        try:
                            # If comp looks rational, ensure denominator invertible mod p
                            if hasattr(comp, "numerator") and hasattr(comp, "denominator"):
                                if int(comp.denominator()) % p == 0:
                                    ok_v = False
                                    break
                                val = (int(comp.numerator()) * pow(int(comp.denominator()), -1, p)) % p
                                reduced_v.append(val)
                            else:
                                reduced_v.append(int(comp) % p)
                        except Exception:
                            ok_v = False
                            break
                    if ok_v:
                        vecs_reduced.append(tuple(reduced_v))
                    else:
                        vecs_reduced.append(None)
                else:
                    # scalar
                    try:
                        vecs_reduced.append(int(v) % p)
                    except Exception:
                        vecs_reduced.append(None)
                # not fatal â€” we can still use this prime
            except Exception as e:
                vecs_reduced.append(None)
                if logger:
                    logger(f"[prepare_modular_data_lll] vec reduction error at p={p}: {e}")

        # All checks passed for this prime: record prepared objects
        Ep_dict[p] = E_mod_p
        rhs_modp_list[p] = rhs_reduced
        mult_lll[p] = mults_for_prime
        vecs_lll[p] = vecs_reduced

    # update stats about rejected primes, if requested
    if stats is not None:
        stats.setdefault("rejected_primes", []).extend(rejected_primes)
        stats.setdefault("prepared_primes", []).extend(sorted(Ep_dict.keys()))

    return Ep_dict, rhs_modp_list, mult_lll, vecs_lll



# Complete fixed functions for search_lll.py
# Copy-paste these to replace the broken versions

def prepare_modular_data_lll(cd, current_sections, prime_pool, rhs_list, vecs, 
                             stats=None, search_primes=None, logger=None):
    """
    Prepare modular data for LLL search. Now properly handles section reduction 
    failures by skipping primes where any section fails to reduce.
    
    Returns: Ep_dict, rhs_modp_list, mult_lll, vecs_lll
    """
    from sage.all import GF, Zmod, lcm
    
    Ep_dict = {}
    rhs_modp_list = {i: {} for i in range(len(rhs_list))}
    mult_lll = {}
    vecs_lll = {}
    rejected_primes = []
    
def reduce_point_hom(E_mod_p, P, p, logger=None):
    """
    Reduce a projective/affine point P on curve E modulo prime p.
    
    Returns:
        - The reduced point on E_mod_p on success
        - None if reduction fails (denominator non-invertible, bad coords, etc.)
    
    Returning None (not Ep(0)) allows callers to distinguish "couldn't reduce"
    from "reduced to identity".
    """
    from sage.all import GF, ZZ
    
    def log(msg):
        if logger:
            logger(msg)
        elif DEBUG:
            print(msg)
    
    def reduce_rational_to_Fp(r, p):
        """Reduce rational r to GF(p), return None if denominator not invertible."""
        try:
            num = int(ZZ(r.numerator()))
            den = int(ZZ(r.denominator()))
        except Exception:
            try:
                return GF(p)(int(r))
            except Exception:
                return None
        
        if den % p == 0:
            return None
        
        try:
            return GF(p)(num) * GF(p)(pow(den, -1, p))
        except Exception:
            return None
    
    try:
        coords = tuple(P)
    except Exception:
        log("[reduce_point_hom] point not iterable")
        return None
    
    # Projective (X, Y, Z)
    if len(coords) == 3:
        X, Y, Z = coords
        Xr = reduce_rational_to_Fp(X, p)
        Yr = reduce_rational_to_Fp(Y, p)
        Zr = reduce_rational_to_Fp(Z, p)
        
        if Xr is None or Yr is None or Zr is None:
            return None
        
        try:
            return E_mod_p([Xr, Yr, Zr])
        except Exception:
            # Try affine conversion
            if Zr == 0:
                return None
            try:
                x_aff = Xr / Zr
                y_aff = Yr / Zr
                return E_mod_p(x_aff, y_aff)
            except Exception:
                return None
    
    # Affine (x, y)
    if len(coords) == 2:
        x, y = coords
        xr = reduce_rational_to_Fp(x, p)
        yr = reduce_rational_to_Fp(y, p)
        
        if xr is None or yr is None:
            return None
        
        try:
            return E_mod_p(xr, yr)
        except Exception:
            return None
    
    log("[reduce_point_hom] unsupported coordinate shape")
    return None


def candidate_passes_extra_primes(m0, M, residue_map, extra_primes_for_filtering, 
                                 tmax=TMAX, verbose=False):
    """
    Quick O(1) filter for CRT candidates using extra primes.
    
    Args:
        m0: CRT residue
        M: CRT modulus
        residue_map: dict mapping prime -> set of allowed residues
        extra_primes_for_filtering: list of extra primes to check
        tmax: max |t| to check
        verbose: print diagnostics
    
    Returns:
        bool: True if candidate could be valid, False to reject
    """
    if not extra_primes_for_filtering:
        return True
    
    m0 = int(m0)
    M = int(M)
    
    for q in extra_primes_for_filtering:
        allowed_residues = residue_map.get(q)
        
        if not allowed_residues:
            # No allowed residues at this prime - reject
            if verbose:
                print(f"[extra_prime_filter] q={q}: no allowed residues")
            return False
        
        # Check if ANY t in [-tmax, tmax] gives an allowed residue
        found_valid_t = False
        for t in range(-tmax, tmax + 1):
            m_cand = (m0 + t * M) % q
            if m_cand in allowed_residues:
                found_valid_t = True
                break
        
        if not found_valid_t:
            if verbose:
                print(f"[extra_prime_filter] q={q}: no valid t found")
            return False
    
    return True


def detect_fiber_collision(Delta_poly, p, debug=DEBUG):
    """
    Detect if discriminant Delta(m) has repeated roots mod p.
    Returns (has_collision, gcd_poly).
    """
    from sage.all import GF, PolynomialRing, gcd
    
    try:
        Fp = GF(p)
        R = PolynomialRing(Fp, 'm')
        
        # Reduce Delta mod p
        Delta_modp = R([int(c) % p for c in Delta_poly.list()])
        dDelta = Delta_modp.derivative()
        
        g = gcd(Delta_modp, dDelta)
        has_collision = (g.degree() >= 1)
        
        if has_collision and debug:
            print(f"âš ï¸  Fiber collision detected at p={p}: gcd degree {g.degree()}")
        
        return has_collision, g
        
    except Exception as e:
        if debug:
            print(f"[detect_fiber_collision] p={p}: error {e}")
        return False, None


def lll_reduce_basis_modp(p, sections, curve_modp,
                          truncate_deg=TRUNCATE_MAX_DEG,
                          lll_delta=LLL_DELTA, bkz_block=BKZ_BLOCK,
                          max_k_abs=MAX_K_ABS):
    """
    LLL/BKZ reduction with proper handling of single-section case.
    """
    from sage.all import ZZ, identity_matrix, diagonal_matrix
    
    r = len(sections)
    if r == 0:
        return [], identity_matrix(ZZ, 0)

    poly_coords = []
    max_deg = 0

    for P in sections:
        Pp = reduce_point_hom(curve_modp, P, p)
        if Pp is None:
            poly_coords.append(([0], [0], [1]))
            continue

        Xr, Yr, Zr = Pp[0], Pp[1], Pp[2]
        den = lcm([Xr.denominator(), Yr.denominator(), Zr.denominator()])
        Xp = Xr.numerator() * (den // Xr.denominator())
        Yp = Yr.numerator() * (den // Yr.denominator())
        Zp = Zr.numerator() * (den // Zr.denominator())

        xc, dx = _get_coeff_data(Xp)
        yc, dy = _get_coeff_data(Yp)
        zc, dz = _get_coeff_data(Zp)

        xc = _trim_poly_coeffs(xc, truncate_deg)
        yc = _trim_poly_coeffs(yc, truncate_deg)
        zc = _trim_poly_coeffs(zc, truncate_deg)

        poly_coords.append((xc, yc, zc))
        max_deg = max(max_deg, len(xc)-1, len(yc)-1, len(zc)-1)

    poly_len = max_deg + 1
    coeff_vecs = []
    for xc, yc, zc in poly_coords:
        xc_padded = list(xc) + [0] * (poly_len - len(xc))
        yc_padded = list(yc) + [0] * (poly_len - len(yc))
        zc_padded = list(zc) + [0] * (poly_len - len(zc))
        row = [ZZ(int(c)) for c in (xc_padded + yc_padded + zc_padded)]
        coeff_vecs.append(vector(ZZ, row))

    if not coeff_vecs or all(v.is_zero() for v in coeff_vecs):
        if DEBUG: 
            print("All coefficient vectors are zero or truncated away, using identity transformation")
        return [curve_modp(0) for _ in range(r)], identity_matrix(ZZ, r)

    M = matrix(ZZ, coeff_vecs)
    
    if M.nrows() <= 1:
        Uinv = identity_matrix(ZZ, r)
        reduced_sections_mod_p = []
        for P in sections:
            P_red = reduce_point_hom(curve_modp, P, p)
            if P_red is not None:
                reduced_sections_mod_p.append(P_red)
        return reduced_sections_mod_p, Uinv

    if M.rank() < min(M.nrows(), M.ncols()):
        if DEBUG: 
            print("Matrix is rank-deficient (after trimming), continuing but may affect LLL")

    if M.ncols() > 5 * M.nrows():
        if DEBUG:
            print(f"[LLL] Matrix too wide ({M.nrows()}x{M.ncols()}), skipping LLL for this prime")
        reduced_sections_mod_p = []
        for P in sections:
            P_red = reduce_point_hom(curve_modp, P, p)
            if P_red is not None:
                reduced_sections_mod_p.append(P_red)
        return reduced_sections_mod_p, identity_matrix(ZZ, r)

    try:
        scales = _compute_integer_scales_for_columns(M)
        M_scaled, D = _scale_matrix_columns_int(M, scales)
    except Exception as e:
        if DEBUG: 
            print("Column scaling failed, proceeding without scaling:", e)
        M_scaled = M
        D = diagonal_matrix([1]*M.ncols())

    U = None
    B = None
    try:
        if hasattr(M_scaled, "BKZ"):
            block = min(bkz_block, max(2, M_scaled.ncols()//2))
            U, B = M_scaled.BKZ(block_size=block, transformation=True)
        else:
            U, B = M_scaled.LLL(transformation=True, delta=float(lll_delta))
    except (TypeError, ValueError):
        try:
            U, B = M_scaled.LLL(transformation=True)
        except Exception as e:
            if DEBUG:
                print("LLL/BKZ reduction failed, falling back to identity:", e)
            U = identity_matrix(ZZ, r)
            B = M_scaled.copy()

    if DEBUG:
        print(f"[LLL_DEBUG] p={p}, r={r}, M_scaled shape: {M_scaled.nrows()}x{M_scaled.ncols()}")
        print(f"[LLL_DEBUG] U type: {type(U).__name__}, has nrows: {hasattr(U, 'nrows')}")
        if hasattr(U, 'nrows'):
            print(f"[LLL_DEBUG] U shape: {U.nrows()}x{U.ncols()}")
        else:
            print(f"[LLL_DEBUG] U is not a matrix. repr (first 100 chars): {repr(U)[:100]}")
        print(f"[LLL_DEBUG] B type: {type(B).__name__}")

    Uinv = None
    assert U.nrows() == U.ncols(), "LLL transform U must be square"
    detU = int(U.det())
    assert abs(detU) == 1, "LLL transform U not unimodular; det = " + str(detU)
    Uinv = U.inverse()
    
    reduced_sections_mod_p = []
    for P in sections:
        P_red = reduce_point_hom(curve_modp, P, p)
        if P_red is not None:
            reduced_sections_mod_p.append(P_red)
    
    new_basis = [sum(U[i, j] * reduced_sections_mod_p[j] for j in range(len(reduced_sections_mod_p))) 
                 for i in range(r)]

    return new_basis, Uinv



# Complete fixed functions for search_lll.py
# Copy-paste these to replace the broken versions

def prepare_modular_data_lll(cd, current_sections, prime_pool, rhs_list, vecs, 
                             stats=None, search_primes=None, logger=None):
    """
    Prepare modular data for LLL search. Now properly handles section reduction 
    failures by skipping primes where any section fails to reduce.
    
    Returns: Ep_dict, rhs_modp_list, mult_lll, vecs_lll
    """
    from sage.all import GF, Zmod, lcm
    
    # Initialize all return values first
    Ep_dict = {}
    rhs_modp_list = {i: {} for i in range(len(rhs_list))}
    mult_lll = {}
    vecs_lll = {}
    rejected_primes = []
    
    def log(msg):
        if logger:
            logger(msg)
        elif DEBUG:
            print(msg)
    
    try:
        for p in prime_pool:
            try:
                # Build E_mod_p
                try:
                    E_mod_p = cd.change_ring(GF(p))
                except Exception:
                    try:
                        E_mod_p = cd.change_ring(Zmod(p))
                    except Exception:
                        log(f"[prepare_modular] p={p}: couldn't construct E_mod_p")
                        rejected_primes.append((p, "E_mod_construction_failed"))
                        continue
                
                # Reduce all sections - if ANY fail, skip this prime
                reduced_sections = []
                reduction_failed = False
                
                for i_sec, S in enumerate(current_sections):
                    Pi_red = reduce_point_hom(E_mod_p, S, p, logger=logger)
                    if Pi_red is None:
                        log(f"[prepare_modular] p={p}: section {i_sec} reduction failed (None)")
                        reduction_failed = True
                        break
                    reduced_sections.append(Pi_red)
                
                if reduction_failed:
                    rejected_primes.append((p, "section_reduction_returned_None"))
                    continue
                
                # Compute multiples for each reduced section
                mults_for_prime = {}
                any_mult_error = False
                
                for i_sec, Pi_red in enumerate(reduced_sections):
                    mults = compute_all_mults_for_section(Pi_red, E_mod_p, p, logger=logger)
                    if mults is None:
                        log(f"[prepare_modular] p={p}: mults computation failed for section {i_sec}")
                        any_mult_error = True
                        break
                    mults_for_prime[i_sec] = mults
                
                if any_mult_error:
                    rejected_primes.append((p, "mults_computation_failed"))
                    continue
                
                # Reduce RHS functions
                for i_rhs, rhs in enumerate(rhs_list):
                    try:
                        # RHS is typically a rational function in m
                        # We need to reduce its coefficients mod p
                        if hasattr(rhs, 'change_ring'):
                            rhs_modp = rhs.change_ring(GF(p))
                        else:
                            # Fallback: keep as symbolic and reduce later per-m
                            rhs_modp = rhs
                        rhs_modp_list[i_rhs][p] = rhs_modp
                    except Exception as e:
                        log(f"[prepare_modular] p={p}: RHS {i_rhs} reduction failed: {e}")
                        rhs_modp_list[i_rhs][p] = None
                
                # Reduce vecs (LLL-transformed vectors)
                vecs_reduced = []
                for v in vecs:
                    try:
                        v_tuple = tuple(v)
                        vecs_reduced.append(v_tuple)
                    except Exception:
                        vecs_reduced.append(tuple([0] * len(current_sections)))
                
                # All checks passed - record this prime
                Ep_dict[p] = E_mod_p
                mult_lll[p] = mults_for_prime
                vecs_lll[p] = vecs_reduced
                
            except Exception as e:
                log(f"[prepare_modular] p={p}: unexpected error: {e}")
                rejected_primes.append((p, f"unexpected_error:{e}"))
                continue
        
        # Update stats
        if stats is not None:
            if hasattr(stats, 'counters'):
                stats.counters['primes_rejected_in_prep'] = len(rejected_primes)
                stats.counters['primes_usable_after_prep'] = len(Ep_dict)
        
        if DEBUG:
            print(f"[prepare_modular] Rejected {len(rejected_primes)} primes, using {len(Ep_dict)}")
            if rejected_primes:
                by_reason = {}
                for p, reason in rejected_primes:
                    by_reason.setdefault(reason, []).append(p)
                for reason, primes in by_reason.items():
                    print(f"  {reason}: {len(primes)} primes")
    
    except Exception as e:
        print(f"[prepare_modular_data_lll] FATAL ERROR: {e}")
        import traceback
        traceback.print_exc()
        # Return empty dicts so caller doesn't crash
        return {}, {i: {} for i in range(len(rhs_list))}, {}, {}
    
    return Ep_dict, rhs_modp_list, mult_lll, vecs_lll


def reduce_point_hom(E_mod_p, P, p, logger=None):
    """
    Reduce a projective/affine point P on curve E modulo prime p.
    
    Returns:
        - The reduced point on E_mod_p on success
        - None if reduction fails (denominator non-invertible, bad coords, etc.)
    
    Returning None (not Ep(0)) allows callers to distinguish "couldn't reduce"
    from "reduced to identity".
    """
    from sage.all import GF, ZZ
    
    def log(msg):
        if logger:
            logger(msg)
        elif DEBUG:
            print(msg)
    
    def reduce_rational_to_Fp(r, p):
        """Reduce rational r to GF(p), return None if denominator not invertible."""
        try:
            num = int(ZZ(r.numerator()))
            den = int(ZZ(r.denominator()))
        except Exception:
            try:
                return GF(p)(int(r))
            except Exception:
                return None
        
        if den % p == 0:
            return None
        
        try:
            return GF(p)(num) * GF(p)(pow(den, -1, p))
        except Exception:
            return None
    
    try:
        coords = tuple(P)
    except Exception:
        log("[reduce_point_hom] point not iterable")
        return None
    
    # Projective (X, Y, Z)
    if len(coords) == 3:
        X, Y, Z = coords
        Xr = reduce_rational_to_Fp(X, p)
        Yr = reduce_rational_to_Fp(Y, p)
        Zr = reduce_rational_to_Fp(Z, p)
        
        if Xr is None or Yr is None or Zr is None:
            return None
        
        try:
            return E_mod_p([Xr, Yr, Zr])
        except Exception:
            # Try affine conversion
            if Zr == 0:
                return None
            try:
                x_aff = Xr / Zr
                y_aff = Yr / Zr
                return E_mod_p(x_aff, y_aff)
            except Exception:
                return None
    
    # Affine (x, y)
    if len(coords) == 2:
        x, y = coords
        xr = reduce_rational_to_Fp(x, p)
        yr = reduce_rational_to_Fp(y, p)
        
        if xr is None or yr is None:
            return None
        
        try:
            return E_mod_p(xr, yr)
        except Exception:
            return None
    
    log("[reduce_point_hom] unsupported coordinate shape")
    return None


def candidate_passes_extra_primes(m0, M, residue_map, extra_primes_for_filtering, 
                                 tmax=TMAX, verbose=False):
    """
    Quick O(1) filter for CRT candidates using extra primes.
    
    Args:
        m0: CRT residue
        M: CRT modulus
        residue_map: dict mapping prime -> set of allowed residues
        extra_primes_for_filtering: list of extra primes to check
        tmax: max |t| to check
        verbose: print diagnostics
    
    Returns:
        bool: True if candidate could be valid, False to reject
    """
    if not extra_primes_for_filtering:
        return True
    
    m0 = int(m0)
    M = int(M)
    
    for q in extra_primes_for_filtering:
        allowed_residues = residue_map.get(q)
        
        if not allowed_residues:
            # No allowed residues at this prime - reject
            if verbose:
                print(f"[extra_prime_filter] q={q}: no allowed residues")
            return False
        
        # Check if ANY t in [-tmax, tmax] gives an allowed residue
        found_valid_t = False
        for t in range(-tmax, tmax + 1):
            m_cand = (m0 + t * M) % q
            if m_cand in allowed_residues:
                found_valid_t = True
                break
        
        if not found_valid_t:
            if verbose:
                print(f"[extra_prime_filter] q={q}: no valid t found")
            return False
    
    return True


def detect_fiber_collision(Delta_poly, p, debug=DEBUG):
    """
    Detect if discriminant Delta(m) has repeated roots mod p.
    Returns (has_collision, gcd_poly).
    """
    from sage.all import GF, PolynomialRing, gcd
    
    try:
        Fp = GF(p)
        R = PolynomialRing(Fp, 'm')
        
        # Reduce Delta mod p
        Delta_modp = R([int(c) % p for c in Delta_poly.list()])
        dDelta = Delta_modp.derivative()
        
        g = gcd(Delta_modp, dDelta)
        has_collision = (g.degree() >= 1)
        
        if has_collision and debug:
            print(f"âš ï¸  Fiber collision detected at p={p}: gcd degree {g.degree()}")
        
        return has_collision, g
        
    except Exception as e:
        if debug:
            print(f"[detect_fiber_collision] p={p}: error {e}")
        return False, None


def lll_reduce_basis_modp(p, sections, curve_modp,
                          truncate_deg=TRUNCATE_MAX_DEG,
                          lll_delta=LLL_DELTA, bkz_block=BKZ_BLOCK,
                          max_k_abs=MAX_K_ABS):
    """
    LLL/BKZ reduction with proper handling of single-section case.
    """
    from sage.all import ZZ, identity_matrix, diagonal_matrix
    
    r = len(sections)
    if r == 0:
        return [], identity_matrix(ZZ, 0)

    poly_coords = []
    max_deg = 0

    for P in sections:
        Pp = reduce_point_hom(curve_modp, P, p)
        if Pp is None:
            poly_coords.append(([0], [0], [1]))
            continue

        Xr, Yr, Zr = Pp[0], Pp[1], Pp[2]
        den = lcm([Xr.denominator(), Yr.denominator(), Zr.denominator()])
        Xp = Xr.numerator() * (den // Xr.denominator())
        Yp = Yr.numerator() * (den // Yr.denominator())
        Zp = Zr.numerator() * (den // Zr.denominator())

        xc, dx = _get_coeff_data(Xp)
        yc, dy = _get_coeff_data(Yp)
        zc, dz = _get_coeff_data(Zp)

        xc = _trim_poly_coeffs(xc, truncate_deg)
        yc = _trim_poly_coeffs(yc, truncate_deg)
        zc = _trim_poly_coeffs(zc, truncate_deg)

        poly_coords.append((xc, yc, zc))
        max_deg = max(max_deg, len(xc)-1, len(yc)-1, len(zc)-1)

    poly_len = max_deg + 1
    coeff_vecs = []
    for xc, yc, zc in poly_coords:
        xc_padded = list(xc) + [0] * (poly_len - len(xc))
        yc_padded = list(yc) + [0] * (poly_len - len(yc))
        zc_padded = list(zc) + [0] * (poly_len - len(zc))
        row = [ZZ(int(c)) for c in (xc_padded + yc_padded + zc_padded)]
        coeff_vecs.append(vector(ZZ, row))

    if not coeff_vecs or all(v.is_zero() for v in coeff_vecs):
        if DEBUG: 
            print("All coefficient vectors are zero or truncated away, using identity transformation")
        return [curve_modp(0) for _ in range(r)], identity_matrix(ZZ, r)

    M = matrix(ZZ, coeff_vecs)
    
    if M.nrows() <= 1:
        Uinv = identity_matrix(ZZ, r)
        reduced_sections_mod_p = []
        for P in sections:
            P_red = reduce_point_hom(curve_modp, P, p)
            if P_red is not None:
                reduced_sections_mod_p.append(P_red)
        return reduced_sections_mod_p, Uinv

    if M.rank() < min(M.nrows(), M.ncols()):
        if DEBUG: 
            print("Matrix is rank-deficient (after trimming), continuing but may affect LLL")

    if M.ncols() > 5 * M.nrows():
        if DEBUG:
            print(f"[LLL] Matrix too wide ({M.nrows()}x{M.ncols()}), skipping LLL for this prime")
        reduced_sections_mod_p = []
        for P in sections:
            P_red = reduce_point_hom(curve_modp, P, p)
            if P_red is not None:
                reduced_sections_mod_p.append(P_red)
        return reduced_sections_mod_p, identity_matrix(ZZ, r)

    try:
        scales = _compute_integer_scales_for_columns(M)
        M_scaled, D = _scale_matrix_columns_int(M, scales)
    except Exception as e:
        if DEBUG: 
            print("Column scaling failed, proceeding without scaling:", e)
        M_scaled = M
        D = diagonal_matrix([1]*M.ncols())

    U = None
    B = None
    try:
        if hasattr(M_scaled, "BKZ"):
            block = min(bkz_block, max(2, M_scaled.ncols()//2))
            U, B = M_scaled.BKZ(block_size=block, transformation=True)
        else:
            U, B = M_scaled.LLL(transformation=True, delta=float(lll_delta))
    except (TypeError, ValueError):
        try:
            U, B = M_scaled.LLL(transformation=True)
        except Exception as e:
            if DEBUG:
                print("LLL/BKZ reduction failed, falling back to identity:", e)
            U = identity_matrix(ZZ, r)
            B = M_scaled.copy()

    if DEBUG:
        print(f"[LLL_DEBUG] p={p}, r={r}, M_scaled shape: {M_scaled.nrows()}x{M_scaled.ncols()}")
        print(f"[LLL_DEBUG] U type: {type(U).__name__}, has nrows: {hasattr(U, 'nrows')}")
        if hasattr(U, 'nrows'):
            print(f"[LLL_DEBUG] U shape: {U.nrows()}x{U.ncols()}")
        else:
            print(f"[LLL_DEBUG] U is not a matrix. repr (first 100 chars): {repr(U)[:100]}")
        print(f"[LLL_DEBUG] B type: {type(B).__name__}")

    Uinv = None
    assert U.nrows() == U.ncols(), "LLL transform U must be square"
    detU = int(U.det())
    assert abs(detU) == 1, "LLL transform U not unimodular; det = " + str(detU)
    Uinv = U.inverse()
    
    reduced_sections_mod_p = []
    for P in sections:
        P_red = reduce_point_hom(curve_modp, P, p)
        if P_red is not None:
            reduced_sections_mod_p.append(P_red)
    
    new_basis = [sum(U[i, j] * reduced_sections_mod_p[j] for j in range(len(reduced_sections_mod_p))) 
                 for i in range(r)]

    return new_basis, Uinv
