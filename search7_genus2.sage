# search7_genus2.sage
# This file was *autogenerated* from the file search7_genus2.sage
import itertools
from collections import Counter
from sage.all import *

# application modules
from search_common import *
from tate import *
from search_lll import *
from sat import *
from picard import *
from automorph import *
from torsion import *
from nslattice import *
from yau import *
import bounds
from bounds import *
from selmer import *
from stats import *
from sympy import symbols, expand
load('tower.sage')
from stats import SearchStats # <-- Make sure stats is imported

# -------------------------
# Tower builder adapter for search (lightweight, deterministic, no tests)
# -------------------------
# --- Helper Functions ---

def find_y_zero_points_genus2(sextic_coeffs, verbose=True):
    """
    Find all rational points (x,0) on the genus 2 curve y^2 = G(x).
    For y=0, we need G(x) = 0.


    Args:
        sextic_coeffs: List of coefficients [a6, a5, a4, a3, a2, a1, a0]
        verbose: Print debug information

    Returns:
        Set of rational points (x, 0) where G(x) = 0
    """
    if verbose:
        print("\n--- Searching for y=0 points ---") # Corrected print

    # Build the sextic polynomial G(x)

    R_x = QQ['x']
    G = sum(a * R_x.gen()**(len(sextic_coeffs)-1-i) for i, a in enumerate(sextic_coeffs))

    if verbose:
        print(f"hyper elliptic curve: y^2(x) = {G}")

    # Find all rational roots
    rational_roots = G.roots(QQ, multiplicities=False)
    y_zero_points = {(QQ(root), QQ(0)) for root in rational_roots}

    if verbose:
        if y_zero_points:
            print(f"Found {len(y_zero_points)} points where y=0: {sorted(list(y_zero_points))}") # Corrected print
        else:
            print("No rational points found where y=0") # Corrected print

    return y_zero_points




# Simple integration into your existing code:
def add_y_zero_points_to_known(known_pts, sextic_coeffs):
    """Add y=0 points to the known points set."""
    y_zero_points = find_y_zero_points_genus2(sextic_coeffs)
    known_pts.update(y_zero_points)
    return known_pts

@PROFILE
def doloop_genus2(data_pts, sextic_coeffs, all_known_x, cumulative_stats):
    """
    Main search loop adapted for the genus-2 strategy.
    Now accepts and merges into a cumulative_stats object.
    """

    # 1. Initial Setup and Shifting
    # at top of function
    SR_m = var('m')
    PR_m = PolynomialRing(QQ, 'm')
    m_poly = PR_m.gen()
    Fm = PR_m.fraction_field()

    R_x = PolynomialRing(QQ, 'x')
    x = R_x.gen()
    G = sum([a*x**(len(sextic_coeffs)-1 - i) for i, a in enumerate(sextic_coeffs)])

    real_pts = [(QQ(p[0]), QQ(p[1])) for p in data_pts if p[0] is not None]

    shift = QQ(0)
    while any((pt[0] + shift) == 0 for pt in real_pts):
        shift += 1
    if shift:
        print("")
        print("SHIFT =", shift)
        print("")

    shifted_G_poly = G(x - shift)
    base_pts = [(X + shift, Y) for X, Y in real_pts]

    # 2. Build the full fibration tower (deg 6 -> 5 -> 4)
    print(f"--- Building {len(sextic_coeffs)-5} Step Fibration Tower (deg {len(sextic_coeffs)-1} -> 4 (-> 3)) ---") # Corrected print
    tower = iterate_tower(
        fx_PR=shifted_G_poly,
        pts_xy=base_pts,
        max_steps=len(sextic_coeffs)-5, # 5, because general quartics have 5 coefficients!
        seed_int=SEED_INT,
        verbose=DEBUG
    )
    # 3. Extract expressions from the tower
    roots = [i['r_expr'] for i in tower]
    E_rhs_m_symbolic = tower[-1]['f_i'] # The final quartic equation

    m_r = solve(SR(roots[0]) == var('r'), var('m'))[0].rhs() # for diagnostics

    # Convert the symbolic quartic into the required polynomial type for Sage's curve functions
    F_m = PR_m.fraction_field()
    r_m = SR(roots[0]) # get r_m for converting solutions back to x-coordinates
    print("r_m", r_m)
    R_x_m, x_poly = PolynomialRing(F_m, 'x').objgen()
    xSR, mSR = SR.var('x'), SR.var('m')

    coeffs_in_m = [SR(E_rhs_m_symbolic).coefficient(xSR, i) for i in range(SR(E_rhs_m_symbolic).degree(xSR) + 1)]
    coeffs_in_Fm = [F_m(c.subs({mSR: m_poly})) for c in coeffs_in_m]
    E_rhs_m = R_x_m(coeffs_in_Fm)

    # 4. Main Search Logic
    fibration_type = f"{len(set(pt[0] for pt in base_pts))}-point"
    print(f"--- Running {fibration_type} Fibration ---")

    E_curve_m, one, two, three = compute_morphism(E_rhs_m)

    lastrhs = E_rhs_m(x=roots[-1])
    last_phi_x = get_phi_x(one, two, three, roots[-1], lastrhs)
    cd = buildcd(E_curve_m, last_phi_x, lastrhs, E_rhs_m, (one, two, three))

    # in your main script, after cd is constructed:

    # 5. Auto-configure bounds and prime parameters from the constructed curve data

    print("\n=== Auto-configuring search parameters for this fibration ===")
    try:
        # Build known point list in (x,y) form for height estimation
        # --- MODIFICATION ---
        # Use ALL known x-coordinates to get a better height estimate
        print(f"[auto_cfg] Building height estimation from {len(all_known_x)} known x-coords.")
        known_pts_for_height = []
        for x_coord in all_known_x:
            try:
                # naive_x_height_from_pts only uses the first element (x)
                known_pts_for_height.append((QQ(x_coord), None))
            except Exception:
                continue # Skip if coercion fails

        # Also add the base points for the current fibration
        for pt in base_pts:
            if pt[0] is not None:
                known_pts_for_height.append((QQ(pt[0]), QQ(pt[1])))

        if not known_pts_for_height:
             # Failsafe if all_known_x is empty (shouldn't be, but safe)
             known_pts_for_height = [(QQ(0), None)]
        # --- END MODIFICATION ---
        # Run auto configuration (defined in bounds.py)
        # --- MODIFICATION: Pass new list and height_bound=None to force re-estimation ---
        sconf = bounds.auto_configure_search(cd, known_pts_for_height, height_bound=None, debug=True)
        print_conf(sconf)
    except Exception:
        print("config failed")
        raise

    # 6. Diagnostic check: confirm the modulus capping works safely
    _ = bounds.modulus_needed_from_canonical_height(370, scale_const=2.0,
                                                    max_modulus=MAX_MODULUS, debug=True)

    # Recompute prime pool with possible heavier diagnostics (optional)
    prime_pool = sconf['PRIME_POOL']
    prime_pool = bounds.recommend_and_update_prime_pool(cd, run_heavy=True,
                                                        grh_fudge=10, debug=True)
    prime_pool = sorted(set(prime_pool))
    print(f"[prime_pool] Final pool has {len(prime_pool)} primes up to {max(prime_pool)}")

    # 7. Continue with your normal tower search logic (no other structural changes)
    k = cd.k_base_change
    n = cd.tate_exponent
    blowup = cd.blowup_factor
    m_sym = cd.a4.parent().gen()
    total_x_scale_factor = m_sym ** (2 * n - 2 * blowup)


    # --- BUILD search_rhs_list with symbolic SR consistency ---
    # canonical symbolic m
    SR_m = var('m')
    PR_m = cd.a4.parent()   # QQ[m] where m_sym comes from
    m_sym = PR_m.gen()

    # Ensure roots are SR objects
    roots_SR = [SR(r) for r in roots]

    print("DEBUG: roots (SR) repr/type samples:")
    for j, rr in enumerate(roots_SR[:4]):
        print(f"  root[{j}] = {rr}")

    # Start the search_rhs_list with the top-level phi_x (coerced to SR)
    try:
        search_rhs_list = [SR(cd.phi_x)]
    except Exception:
        search_rhs_list = [SR(str(cd.phi_x))]
        raise

    print("")
    print("RHS:", search_rhs_list[0])
    print("")


    # Build the remaining RHS for each tower-level double root
    for i in roots_SR[:-1]:
        break # new tower as all identical rhs's.
        # Evaluate the quartic at x = root i (symbolic)
        qrhs_r = SR(E_rhs_m_symbolic).subs({xSR: i})
        phi_x_r = get_phi_x(SR(one), SR(two), SR(three), i, qrhs_r)
        phi_x_r_SR = SR(phi_x_r)
        # apply base-change scaling and total_x_scale as before
        rhs_scaled_SR = (phi_x_r_SR.subs({m_sym: m_sym**k}) if k != 0 else phi_x_r_SR) / SR(str(total_x_scale_factor))
        search_rhs_list.append(rhs_scaled_SR)

    print("DEBUG: final search_rhs_list (SR reprs):")
    for j, rr in enumerate(search_rhs_list):
        print(f"  search_rhs_list[{j}] = {rr}")

    assert len(tower) == len(roots), (len(tower), roots)
    #assert len(tower) == len(search_rhs_list), (len(tower), len(search_rhs_list))

    summarize_fibration_info(cd, data_pts, base_pts)
    singfibs = cd.singfibs
    fibers = singfibs['fibers']
    euler = singfibs['euler_characteristic']
    sigma = singfibs['sigma_sum']
    singular_fibers_m = [f['r'] for f in fibers if f.get('root_type') == 'rational']

    cm_fibers = find_cm_fibers(cd)
    j_targets = [0, 1728, -12**3, -32**3, -96**3]
    special_fibers_m = find_special_j_invariant_fibers(cd, j_targets)
    test_fibers = {m for m in set(singular_fibers_m).union(cm_fibers).union(special_fibers_m) if m is not None}
    print("Testing special m-values from fibers:", test_fibers)

    found_from_fibers = test_y_rationality_genus2(list(test_fibers), r_m, shift)
    if found_from_fibers:
        print(f"Points found from special fibers: {found_from_fibers}")
        all_known_x.update(found_from_fibers)

    base_sections = compute_base_sections_m(cd, base_pts)
    verify_morphism_on_samples(cd, base_pts)
    if not base_sections:
        print("Could not compute base sections. Skipping search.") # Corrected print
        return set(), cumulative_stats # Return stats on early exit

    base_sections = lll_reduce_mw_basis(cd, base_sections)
    current_sections = list(set(base_sections))


    iteration = 0
    while True:
        print(f"\n--- Search Iteration {iteration} with {len(current_sections)} sections for fibration {data_pts} ---")
        if not current_sections:
            print("No independent sections to search with. Stopping.")
            break


        independent, H = check_independence(current_sections, E_curve_m, cd)
        if not independent:
            print("Warning: Section basis is linearly dependent. Stopping search.")
            break
        print("Height Pairing Matrix H:\n", H)

        predictor = CurveComplexityPredictor()
        difficulty = predictor.assess_curve_difficulty(cd, current_sections, prime_pool, H)

        height_bound = sconf['HEIGHT_BOUND']
        num_prime_subsets = int(sconf['NUM_PRIME_SUBSETS'])
        if difficulty['difficulty_score'] > 2.0:
            print(f"⚠️  WARNING: Curve predicted to be difficult (score {difficulty['difficulty_score']:.2f})")
            print(f"   Allocating {difficulty['recommended_height_multiplier']:.1f}x height bound")
            print(f"   Allocating {difficulty['recommended_subset_multiplier']:.1f}x prime subsets")
            height_bound *= difficulty['recommended_height_multiplier']
            num_prime_subsets = int(sconf['NUM_PRIME_SUBSETS'] * difficulty['recommended_subset_multiplier'])
        height_bound *= 2


        vecs = compute_search_vectors(H, height_bound) # MW canonical height bound version (old)
        #vecs = compute_search_vectors_hodge(cd, current_sections) # new and fancy hodge index theorem version
        vecs = canonicalize_by_sign(vecs) # we search by x value, so P and -P are the same! only keep one of them!
        print(f"Searching {len(vecs)} vectors up to height {height_bound}...")

        if not vecs:
            print("No search vectors found within height bound. Stopping.")
            break

        # ***** MODIFIED SECTION *****
        # The modular search is replaced with the direct symbolic search.
        if SYMBOLIC_SEARCH:
            newly_found_x, new_sections = search_lattice_symbolic(
                cd,
                current_sections,
                vecs,
                rhs_list=search_rhs_list,
                r_m=r_m,
                shift=shift,
                all_found_x=all_known_x,
                rationality_test_func=get_y_unshifted_genus2,
                stats=cumulative_stats # <-- Pass stats object
            )
            cumulative_stats.merge(iter_stats)
        else:

            # --- ALL CONFIG (PRIMES / SUBSETS / RESIDUE COUNTS) COMES FROM sconf ---
            prime_pool = sconf.get('PRIME_POOL', PRIME_POOL)
            residue_counts = sconf.get('RESIDUE_COUNTS', {})
 
            # --- MODIFIED CALL to get stats object ---
            newly_found_x, new_sections, precomputed_residues, iter_stats = search_lattice_modp_unified_parallel(
                cd, current_sections,
                prime_pool, height_bound,
                vecs,
                search_rhs_list,
                r_m,
                shift,
                all_known_x,
                num_prime_subsets,
                get_y_unshifted_genus2,
                sconf
                )
            # --- MERGE STATS ---
            cumulative_stats.merge(iter_stats)

            #if len(newly_found_x) < len(vecs) // 4:
            #if len(newly_found_x) < 1:
        # ***** END MODIFIED SECTION *****


        print(f"Found {len(newly_found_x)} new point(s) and {len(new_sections)} new section(s).")
        if newly_found_x:
            print(f"New x-coordinates: {newly_found_x}")

        all_known_x.update(newly_found_x)

        if len(all_known_x) >= TERMINATE_WHEN_6:
            break

        if not new_sections:
            break
        print("Augmenting Mordell-Weil basis with new sections.")
        current_sections.extend(new_sections)
        current_sections = lll_reduce_mw_basis(cd, list(set(current_sections)))
        iteration += 1

    # --- START: Restored Diagnostic Sections ---

    ### test that the base point is found by the search
    xtest = base_pts[0][0]  # This is the shifted x (equals r)
    xtest_unshifted = real_pts[0][0]  # This is the original x
    mtest = m_r(r=xtest)
    assert_base_m_found(mtest, xtest_unshifted, r_m, shift)

    #### COMPLETENESS, HEURISTIC

    # diagnostics after precompute_modular stage
    analyzer = FindabilityAnalyzer(cumulative_stats, prime_pool)

    # make sure you have access to the actual subsets used by the search
    # e.g., prime_subsets = current_prime_subsets or whatever variable you used

    # example: random sample
    m_sample = sample_rationals_by_height_random(N=200, B=100)
    diag = analyze_sample_m_list(m_sample, analyzer, cumulative_stats.prime_subsets)
    #for s in diag['samples']:
    #    print(s['m'], "CRT visible:", s['crt_visible'], "coverage fraction:", s['fraction'])

    ok = sum(1 for s in diag['samples'] if s['crt_visible'])
    print("CRT-consistent samples:", ok, "of", len(diag['samples']))
    print("product_density_heuristic:", diag['product_density_heuristic'])
    print("fraction_meet_min_subset:", diag['fraction_meet_min_subset'])

    # or deterministic check up to height bound B
    m_list = enumerate_rationals_height_bound(B=50)
    diag2 = analyze_sample_m_list(m_list, analyzer, cumulative_stats.prime_subsets)
    print("product_density_heuristic:", diag2['product_density_heuristic'])
    print("fraction_meet_min_subset:", diag2['fraction_meet_min_subset'])


    # estimate overall visibility for the subsets we actually used
    est = cumulative_stats.estimate_overall_visibility(cumulative_stats.prime_subsets)
    print("Estimated random-m visibility (P_visible):", est['P_visible'])
    # show top few subset probs
    for subset, p, detail in sorted(est['per_subset'], key=lambda x: -x[1])[:8]:
        print(" subset", subset, "p_subset:", p)

    # compare to actual known points found this run
    known_xs = list(all_known_x)  # your set of true x-values
    kvis = cumulative_stats.compare_known_points_visibility(known_xs, cumulative_stats.prime_subsets)
    print("Known points CRT-visible:", kvis['visible_count'], "of", kvis['total'])

    P_visible_actual, per_subset = compute_actual_subset_cover(cumulative_stats, cumulative_stats.prime_subsets)
    print("P_visible_actual (using tested classes):", P_visible_actual)
    # show top contributors
    for s, p_s, cnt, M in sorted(per_subset, key=lambda t: -t[1])[:12]:
        print(" subset", s, "p_s_actual:", p_s, "tested_count:", cnt, "M:", M)


    # Now compute and compare
    prod_model = compute_product_model(cumulative_stats, cumulative_stats.prime_subsets)
    actual_map_list = compute_actual_subset_cover_map(cumulative_stats, cumulative_stats.prime_subsets)

    # Build easy lookup for actual p_s
    actual_lookup = {t[0]: t[1] for t in actual_map_list}

    # Print top 12 by product-model, with actual tested fraction side-by-side
    print("Top subsets by optimistic product-model p_s (model vs actual tested fraction):")
    for subset, p_model in sorted(prod_model, key=lambda x: -x[1])[:12]:
        p_actual = actual_lookup.get(subset, 0.0)
        print(f" subset {subset}  p_model: {p_model:.6f}   p_actual: {p_actual:.6f}")

    # Also print top contributors by actual tested fraction
    print("\nTop subsets by actual tested fraction (p_s_actual, tested_count, M):")
    for tup in sorted(actual_map_list, key=lambda x: -x[1])[:12]:
        s, p_s, cnt, M = tup
        print(f" subset {s}  p_s_actual: {p_s:.6f}  tested_count: {cnt}  M: {M}")




    prod_model = compute_product_model(cumulative_stats, cumulative_stats.prime_subsets)
    # now compare top 10 by model p_s with actual
    actual_map = {tuple(s): v for s,v,_,_ in compute_actual_subset_cover(cumulative_stats, cumulative_stats.prime_subsets)[1]}
    for s, p_s in sorted(prod_model, key=lambda t: -t[1])[:12]:
        p_actual = actual_map.get(tuple(s), 0.0)
        print("subset", s, "p_model:", p_s, "p_actual:", p_actual)


    ### Automorphism Search ###
    print("\n--- Automorphism Search ---")
    try:
        if 'cd' not in locals() or 'm_sym' not in locals():
            print("Required objects 'cd' or 'm_sym' are not defined. Skipping automorphism search.") # Corrected print
        else:
            if not current_sections:
                print("No sections found; skipping automorphism search preserving fibration.")
            else:
                auts_report = compute_auts_preserving_fibration(cd, current_sections, m_sym)
                scaling_autos = auts_report.get('scaling_autos', [])
                if scaling_autos:
                    print(f"Found {len(scaling_autos)} scaling automorphisms:")
                    for i, aut in enumerate(scaling_autos):
                        print(f"  [{i}] Möbius map: (a,b,c,d) = {aut['mobius']}")
                        print(f"      Scaling factor u = {aut['u']}")
                else:
                    print("No scaling automorphisms found.")

                translation_autos = auts_report.get('translation_autos', [])
                if translation_autos:
                    print(f"Found {len(translation_autos)} translation automorphisms:")
                    for i, aut in enumerate(translation_autos):
                        print(f"  [{i}] Translation by Section S{aut['section_index']}")
                else:
                    print("No translation automorphisms found.")

            try:
                ns_auts, names = compute_ns_auts(singfibs, current_sections)
                if ns_auts:
                    print(f"\nFound {len(ns_auts)} automorphisms of the NS lattice.")
                    classified_ns_auts = classify_auts(ns_auts, names)
                    print("Classified NS lattice automorphisms:")
                    for i, (M, labels) in enumerate(classified_ns_auts):
                        print(f"  [{i}] Matrix:\n{M}")
                        print(f"      Labels: {labels}")
                else:
                    print("\nNo non-trivial automorphisms found for the NS lattice.")
            except Exception as ns_exc:
                print(f"NS lattice automorphism computation failed: {ns_exc}") # Corrected print

    except Exception as exc:
        print(f"An error occurred during automorphism computation: {exc}")
        raise

    ### Torsion Analysis ###
    print("\n--- Torsion Analysis ---")
    specs = good_specializations(cd, m_sym, max_try=40)
    print("Got", len(specs), "good specializations for torsion check.")
    orders = []
    for m0, E in specs:
        tors = E.torsion_subgroup()
        orders.append(tors.order())
    candidate_torsion_order = gcd(orders) if orders else 1
    print("Fast method candidate torsion order (GCD of specializations):", candidate_torsion_order)

    fiber_counts, lcm_bound = compute_fiber_lcm(cd)
    print("Fiber component counts:", fiber_counts, " -> lcm bound:", lcm_bound)

    for i, sec in enumerate(base_sections):
        is_torsion = False
        for n in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12]:
            if torsion_test(cd, sec, n, m_sym=m_sym, max_try=12):
                print(f"Base section {i} appears to be torsion of order dividing {n}.")
                is_torsion = True
                assert not is_torsion, base_sections
                break
        if not is_torsion:
            print(f"Base section {i} does not appear to be small order torsion.") # Corrected print

    ### Saturation Diagnostics ###
    run_saturation_checks(cd, current_sections, prime_pool[:30])

    # --- END: Restored Diagnostic Sections ---

    ### Picard Analysis (using corrected prime list) ###
    ell_candidates = [p for p in prime_pool if p not in cd.bad_primes][:10]
    print(f"\n--- Running Picard-Van Luijk Analysis ---")
    print(f"Using {len(ell_candidates)} good prime candidates for reduction: {ell_candidates}")

    picard_report = picard_via_van_luijk(
        cd,
        current_sections,
        prime_pool,
        ell_candidates=tuple(ell_candidates),
        verbose=True
    )
    print("\n=== Picard Report ===")
    print(f"Lower bound (char 0): {picard_report['lower_bound']}")
    print(f"Upper bounds from reductions:")
    for ub, info in picard_report['upper_bounds']:
        print(f"  ell={info['ell']}: rho <= {ub}    (rank_lb={info['rank_lb']}, Σ={info['sum_contrib']})")
    if picard_report['rho'] is not None:
        print(f"*** Concluded Picard number: ρ = {picard_report['rho']} ***")
    else:
        print("*** Picard not pinned exactly (likely off by ≤ 1). Consider the discriminant step. ***") # Corrected print


    ### Final Shioda-Tate Diagnostics ###
    rank_guess, details, diag = shioda_tate_from_fiber_list(
        singfibs['fibers'],
        rho_geom=picard_report['rho'],
        return_diagnostics=True,
        allow_auto_rho=True
    )
    print("\n--- Shioda-Tate Diagnostics ---")
    if rank_guess is None:
        print(f"Rank could not be determined. Reason: {diag.get('note', 'Unknown')}")
    else:
        print(f"Shioda-Tate Rank Estimate (assuming rho={picard_report['rho']}): {rank_guess}")

    print(f"Total Euler Characteristic: {euler}")
    print(f"Sum of Fiber Contributions (Σ(m_v - 1)): {details['sum_contributions']}")

    # --- After picard_report is computed and printed ---

    print("\n" + "="*70)
    print("RUNNING 2-SELMER ANALYSIS")
    print("="*70)

    selmer_results = run_selmer_analysis(
        cd,
        current_sections,
        picard_report['rho'],
        rank_guess,
        verbose=True
    )

    # Use results for candidate point hunting
    rank_upper = selmer_results['rank_bounds']['upper']
    print(f"\n*** Upper bound on S²(E/ℚ) rank: {rank_upper} ***")

    # ------------------------------------------------------------------
    # Explore explicit 2-coverings from Selmer candidates using the
    # DescentHomomorphism -> _construct_descent_quartic pipeline (no placeholders)
    # ------------------------------------------------------------------
    print("\n--- Constructing explicit 2-coverings for Selmer candidates ---")

    selmer_candidates = selmer_results.get('candidates', [])
    if selmer_candidates:
        u = polygen(QQ, 'u')

        for m_val in selmer_candidates:
            print(f"\n>>> Testing Selmer candidate m = {m_val}") # Corrected print
            try:
                # Specialize coefficients
                a4m = cd.a4(m=m_val)
                a6m = cd.a6(m=m_val)

                # Construct quartic using the Cremona-Tzanakis or Birch-Stephens form
                def quartic(u):
                    # Simple genus-1 quartic model (replace with correct formula)
                    return u**4 + a4m*u + a6m

                # Compute discriminant to flag bad primes
                disc_m = -16 * (4*a4m**3 + 27*a6m**2)

                # --- NEW HANDLING FOR disc_m = 0 ---
                if disc_m == 0:
                    print("  Quartic is singular (disc_m = 0) — it has a rational point and contributes to the rank.") # Corrected print
                    # The Selmer candidate m_val is guaranteed to be in the image of E(Q)
                    # You can skip further checks and move to the next candidate
                    # If you need to verify the rational point, you could use a separate function,
                    # but for a Selmer candidate, disc_m=0 is a positive signal.
                    continue # Skip the rest of the loop for this m_val
                # ------------------------------------

                bad_primes_m = [p for p in prime_factors(disc_m)]  # primes dividing discriminant

                # Combine with cd.bad_primes and filtered PRIME_POOL
                primes_to_check = sorted(set(
                    p for p in bad_primes_m + cd.bad_primes + [p for p in PRIME_POOL if is_good_prime_for_surface(cd, p)]
                    if p > 1 and is_prime(int(p))
                ))
                print("checking primes:", primes_to_check)


                # Check local solubility
                if is_everywhere_locally_solvable(quartic, primes_to_check):
                    print("  Locally solvable at all tested places ✅")
                    pt = search_rational_point_on_quartic(quartic, max_den=1000)
                    if pt:
                        print(f"  Found global rational point: {pt}")
                    else:
                        print("  No global point found up to bound — likely Sha candidate ⚠️")


                else:
                    print("  Locally obstructed ❌")

            except Exception as e:
                print(f"  [error constructing/checking quartic for m={m_val}]: {e}")
                raise


    # --- Yau-Zaslow Rational Curve Counts ---
    _, chi = compute_euler_and_chi(cd)
    if picard_report['rho'] is not None and chi != 1 and False: # skip for RES, trivial # disabled
        rho = picard_report['rho']
        print("\n--- Rational Curve Counts (Yau-Zaslow) ---")

        print("running with rho, mw rank, chi =", rho, rank_guess, chi)

        # Build the basis for the Neron-Severi lattice subspace.
        # Basis order is ['S', 'F', fiber_components...].
        basis_labels, Q, h_vec = build_ns_basis_and_Q(cd, rho, mw_rank=rank_guess, chi=chi)

        # Use a robust, staged search to find primitive vectors v with self-intersection -2,
        # which correspond to classes of rational curves on the surface.
        # The search escalates through different height and coordinate bounds to ensure results.
        mcoords = tuple(range(11, 75))
        counts, reps = staged_rational_curve_search(
            cd,
            current_sections,
            rho, rank_guess, chi,
            height_bounds=(15, 25, 35, 45, 55),
            max_coords=mcoords,
            node_cap=15_000_000,
            return_reps=True,
            require_S_coeff='positive', # Filter for physically meaningful curves (S-coefficient > 0)
            targeted_fallback=True
        )

        # pick sensible checkpoints
        runs = run_convergence_test(cd, current_sections, rho, chi, rank_guess,
                                     max_coords_seq=(20, 24, 28, 31), require_S_coeff='positive')

        print(f"Found rational curve classes in {len(counts)} different degrees.")
        print("Counts per degree:", counts)

        # Display sample representatives for the first few degrees found.
        print("\nSample representatives (first few degrees):") # Corrected print
        for d in sorted(reps.keys())[:6]:
            print(f"\nDegree {d}, count {counts.get(d, 0)}:")
            # Print up to 8 sample vectors for that degree, decoded into a human-readable format.
            for v in reps[d][:8]:
                print("   ", decode_vector(v, basis_labels))

        # Optional heuristic check to see which fiber component a section intersects.
        try:
            if current_sections:
                comp_map = detect_section_component(cd, current_sections[0])
                print("\nSection-component intersection map (heuristic):", comp_map)
        except Exception as e:
            print(f"\nCould not run section-component detection: {e}")
            raise


        # Construct the q-series (generating function) from the curve counts.
        qseries = build_qseries_from_counts(counts, rho, max_degree=20)
        print("\nQ-series (up to q^12):", qseries)


        # NS isotropic vector canonical height minimization analysis
        # cd = your CurveData object
        # current_sections = list of Weierstrass section points you already pass around

        # quick search with small coefficients
        cands = find_isotropic_fibration_candidates(cd, current_sections, rho, rank_guess, chi)
        # examine top candidates and evaluate height change
        results = evaluate_fibration_height_reduction(cd, current_sections, rho, rank_guess, chi, candidates=cands, shioda_sign=-1)

        # print top result
        if results:
            top = results[0]
            print("best coeffs:", top['a'])
            print("old heights:", top['heights_old'])
            print("new heights:", top['heights_new']) # Corrected print
            print("per-section delta (old-new):", top['delta'])
            print("total reduction:", top['total_reduction'])
        else:
            print("no isotropic candidates found in box")



    return all_known_x, cumulative_stats



@PROFILE
def main_genus2():
    initial_xs = DATA_PTS_GENUS2
    known_pts = { (QQ(x), get_y_unshifted_genus2(x)) for x in initial_xs if get_y_unshifted_genus2(x) is not None }
    # Add y=0 points to initial known points

    known_pts = add_y_zero_points_to_known(known_pts, COEFFS_GENUS2)
    print("known_pts start:", known_pts)

    excluded = set()
    all_found_x = {pt[0] for pt in known_pts}

    # --- Create Cumulative Stats Object ---
    cumulative_stats = SearchStats()

    while True:
        if len(known_pts) >= TERMINATE_WHEN_6:
            print(f"TERMINATE_WHEN_6 ({TERMINATE_WHEN_6}) reached.")
            break

        data_pts = get_data_pts(known_pts, excluded)
        if data_pts is None:
            print("All combinations of points have been checked.")
            break

        # y=0 skip; presents problems for this method
        skip = False
        for i in data_pts:
            if not i[1]:
                skip = True
                excluded.add(frozenset(data_pts))
                print("skipping:", data_pts, "due to the presence of y=0 point.")
                break
        if skip:
            continue

        print("\n========================================================")
        print(f"Constructing new fibration using: {data_pts}")
        print(f"known pts so far: {sorted(list(known_pts))}")
        print(f"found {len(known_pts)} / {TERMINATE_WHEN_6}")
        print("========================================================\n")

        # --- Pass cumulative_stats to doloop ---
        found_from_fibration, cumulative_stats = doloop_genus2(data_pts, COEFFS_GENUS2, all_found_x, cumulative_stats)
        all_found_x.update(found_from_fibration)

        excluded.add(frozenset(data_pts))
        known_pts = augment_known(known_pts, all_found_x, deg6=True)

    print("\n--- Cumulative Run Statistics ---")
    print(cumulative_stats.summary_string())

    print("\n--- Final Results ---")
    print(f"Final list of known points: {sorted(list(known_pts))}")





# In search7_genus2.sage

if __name__ == '__main__':
    main_genus2()
