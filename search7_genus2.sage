# search7_genus2.sage
# This file was *autogenerated* from the file search7_genus2.sage
import itertools
from collections import Counter
from sage.all import *

# application modules
from search_common import *
from tate import *
from search_lll import *
from sat import *
from picard import *
from automorph import *
from torsion import *
from nslattice import *
from yau import *
import bounds
from bounds import *
from selmer import *
from stats import *
from sympy import symbols, expand
load('tower.sage')
from stats import SearchStats # <-- Make sure stats is imported

# -------------------------
# Tower builder adapter for search (lightweight, deterministic, no tests)
# -------------------------
# --- Helper Functions ---

def find_y_zero_points_genus2(sextic_coeffs, verbose=True):
    """
    Find all rational points (x,0) on the genus 2 curve y^2 = G(x).
    For y=0, we need G(x) = 0.


    Args:
        sextic_coeffs: List of coefficients [a6, a5, a4, a3, a2, a1, a0]
        verbose: Print debug information

    Returns:
        Set of rational points (x, 0) where G(x) = 0
    """
    if verbose:
        print("\n--- Searching for y=0 points ---") # Corrected print

    # Build the sextic polynomial G(x)

    R_x = QQ['x']
    G = sum(a * R_x.gen()**(len(sextic_coeffs)-1-i) for i, a in enumerate(sextic_coeffs))

    if verbose:
        print(f"hyper elliptic curve: y^2(x) = {G}")

    # Find all rational roots
    rational_roots = G.roots(QQ, multiplicities=False)
    y_zero_points = {(QQ(root), QQ(0)) for root in rational_roots}

    if verbose:
        if y_zero_points:
            print(f"Found {len(y_zero_points)} points where y=0: {sorted(list(y_zero_points))}") # Corrected print
        else:
            print("No rational points found where y=0") # Corrected print

    return y_zero_points




# Simple integration into your existing code:
def add_y_zero_points_to_known(known_pts, sextic_coeffs):
    """Add y=0 points to the known points set."""
    y_zero_points = find_y_zero_points_genus2(sextic_coeffs)
    known_pts.update(y_zero_points)
    return known_pts

@PROFILE
def doloop_genus2(data_pts, sextic_coeffs, all_known_x, cumulative_stats):
    """
    Main search loop adapted for the genus-2 strategy.
    Now accepts and merges into a cumulative_stats object.
    """

    # 1. Initial Setup and Shifting
    # at top of function
    SR_m = var('m')
    PR_m = PolynomialRing(QQ, 'm')
    m_poly = PR_m.gen()
    Fm = PR_m.fraction_field()

    R_x = PolynomialRing(QQ, 'x')
    x = R_x.gen()
    G = sum([a*x**(len(sextic_coeffs)-1 - i) for i, a in enumerate(sextic_coeffs)])

    real_pts = [(QQ(p[0]), QQ(p[1])) for p in data_pts if p[0] is not None]

    shift = QQ(0)
    while any((pt[0] + shift) == 0 for pt in real_pts):
        shift += 1
    if shift:
        print("")
        print("SHIFT =", shift)
        print("")

    shifted_G_poly = G(x - shift)
    base_pts = [(X + shift, Y) for X, Y in real_pts]

    # 2. Build the full fibration tower (deg 6 -> 5 -> 4)
    print(f"--- Building {len(sextic_coeffs)-5} Step Fibration Tower (deg {len(sextic_coeffs)-1} -> 4 (-> 3)) ---") # Corrected print
    tower = iterate_tower(
        fx_PR=shifted_G_poly,
        pts_xy=base_pts,
        max_steps=len(sextic_coeffs)-5, # 5, because general quartics have 5 coefficients!
        seed_int=SEED_INT,
        verbose=DEBUG
    )
    # 3. Extract expressions from the tower
    roots = [i['r_expr'] for i in tower]
    E_rhs_m_symbolic = tower[-1]['f_i'] # The final quartic equation

    m_r = solve(SR(roots[0]) == var('r'), var('m'))[0].rhs() # for diagnostics

    # Convert the symbolic quartic into the required polynomial type for Sage's curve functions
    F_m = PR_m.fraction_field()
    r_m = SR(roots[0]) # get r_m for converting solutions back to x-coordinates
    print("r_m", r_m)
    R_x_m, x_poly = PolynomialRing(F_m, 'x').objgen()
    xSR, mSR = SR.var('x'), SR.var('m')

    coeffs_in_m = [SR(E_rhs_m_symbolic).coefficient(xSR, i) for i in range(SR(E_rhs_m_symbolic).degree(xSR) + 1)]
    coeffs_in_Fm = [F_m(c.subs({mSR: m_poly})) for c in coeffs_in_m]
    E_rhs_m = R_x_m(coeffs_in_Fm)

    # 4. Main Search Logic
    fibration_type = f"{len(set(pt[0] for pt in base_pts))}-point"
    print(f"--- Running {fibration_type} Fibration ---")

    E_curve_m, one, two, three = compute_morphism(E_rhs_m)

    lastrhs = E_rhs_m(x=roots[-1])
    last_phi_x = get_phi_x(one, two, three, roots[-1], lastrhs)
    cd = buildcd(E_curve_m, last_phi_x, lastrhs, E_rhs_m, (one, two, three))

    # in your main script, after cd is constructed:

    # 5. Auto-configure bounds and prime parameters from the constructed curve data

    print("\n=== Auto-configuring search parameters for this fibration ===")
    try:
        # Build known point list in (x,y) form for height estimation
        # --- MODIFICATION ---
        # Use ALL known x-coordinates to get a better height estimate
        print(f"[auto_cfg] Building height estimation from {len(all_known_x)} known x-coords.")
        known_pts_for_height = []
        for x_coord in all_known_x:
            try:
                # naive_x_height_from_pts only uses the first element (x)
                known_pts_for_height.append((QQ(x_coord), None))
            except Exception:
                continue # Skip if coercion fails

        # Also add the base points for the current fibration
        for pt in base_pts:
            if pt[0] is not None:
                known_pts_for_height.append((QQ(pt[0]), QQ(pt[1])))

        if not known_pts_for_height:
             # Failsafe if all_known_x is empty (shouldn't be, but safe)
             known_pts_for_height = [(QQ(0), None)]
        # --- END MODIFICATION ---
        # Run auto configuration (defined in bounds.py)
        # --- MODIFICATION: Pass new list and height_bound=None to force re-estimation ---
        sconf = bounds.auto_configure_search(cd, known_pts_for_height, height_bound=None, debug=True)
        print_conf(sconf)
    except Exception:
        print("config failed")
        raise

    # 6. Diagnostic check: confirm the modulus capping works safely
    _ = bounds.modulus_needed_from_canonical_height(370, scale_const=2.0,
                                                    max_modulus=MAX_MODULUS, debug=True)

    # Recompute prime pool with possible heavier diagnostics (optional)
    prime_pool = sconf['PRIME_POOL']
    prime_pool = bounds.recommend_and_update_prime_pool(cd, run_heavy=True,
                                                        grh_fudge=10, debug=True)
    prime_pool = sorted(set(prime_pool))
    print(f"[prime_pool] Final pool has {len(prime_pool)} primes up to {max(prime_pool)}")

    # 7. Continue with your normal tower search logic (no other structural changes)
    k = cd.k_base_change
    n = cd.tate_exponent
    blowup = cd.blowup_factor
    m_sym = cd.a4.parent().gen()
    total_x_scale_factor = m_sym ** (2 * n - 2 * blowup)


    # --- BUILD search_rhs_list with symbolic SR consistency ---
    # canonical symbolic m
    SR_m = var('m')
    PR_m = cd.a4.parent()   # QQ[m] where m_sym comes from
    m_sym = PR_m.gen()

    # Ensure roots are SR objects
    roots_SR = [SR(r) for r in roots]

    print("DEBUG: roots (SR) repr/type samples:")
    for j, rr in enumerate(roots_SR[:4]):
        print(f"  root[{j}] = {rr}")

    # Start the search_rhs_list with the top-level phi_x (coerced to SR)
    try:
        search_rhs_list = [SR(cd.phi_x)]
    except Exception:
        search_rhs_list = [SR(str(cd.phi_x))]
        raise

    print("")
    print("RHS:", search_rhs_list[0])
    print("")


    # Build the remaining RHS for each tower-level double root
    for i in roots_SR[:-1]:
        break # new tower as all identical rhs's.
        # Evaluate the quartic at x = root i (symbolic)
        qrhs_r = SR(E_rhs_m_symbolic).subs({xSR: i})
        phi_x_r = get_phi_x(SR(one), SR(two), SR(three), i, qrhs_r)
        phi_x_r_SR = SR(phi_x_r)
        # apply base-change scaling and total_x_scale as before
        rhs_scaled_SR = (phi_x_r_SR.subs({m_sym: m_sym**k}) if k != 0 else phi_x_r_SR) / SR(str(total_x_scale_factor))
        search_rhs_list.append(rhs_scaled_SR)

    print("DEBUG: final search_rhs_list (SR reprs):")
    for j, rr in enumerate(search_rhs_list):
        print(f"  search_rhs_list[{j}] = {rr}")

    assert len(tower) == len(roots), (len(tower), roots)
    #assert len(tower) == len(search_rhs_list), (len(tower), len(search_rhs_list))

    summarize_fibration_info(cd, data_pts, base_pts)
    singfibs = cd.singfibs
    fibers = singfibs['fibers']
    euler = singfibs['euler_characteristic']
    sigma = singfibs['sigma_sum']
    singular_fibers_m = [f['r'] for f in fibers if f.get('root_type') == 'rational']

    cm_fibers = find_cm_fibers(cd)
    j_targets = [0, 1728, -12**3, -32**3, -96**3]
    special_fibers_m = find_special_j_invariant_fibers(cd, j_targets)
    test_fibers = {m for m in set(singular_fibers_m).union(cm_fibers).union(special_fibers_m) if m is not None}
    print("Testing special m-values from fibers:", test_fibers)

    found_from_fibers = test_y_rationality_genus2(list(test_fibers), r_m, shift)
    if found_from_fibers:
        print(f"Points found from special fibers: {found_from_fibers}")
        all_known_x.update(found_from_fibers)

    base_sections = compute_base_sections_m(cd, base_pts)
    verify_morphism_on_samples(cd, base_pts)
    if not base_sections:
        print("Could not compute base sections. Skipping search.") # Corrected print
        return set(), cumulative_stats # Return stats on early exit

    base_sections = lll_reduce_mw_basis(cd, base_sections)
    current_sections = list(set(base_sections))


    iteration = 0
    while True:
        print(f"\n--- Search Iteration {iteration} with {len(current_sections)} sections for fibration {data_pts} ---")
        if not current_sections:
            print("No independent sections to search with. Stopping.")
            break


        independent, H = check_independence(current_sections, E_curve_m, cd)
        if not independent:
            print("Warning: Section basis is linearly dependent. Stopping search.")
            break
        print("Height Pairing Matrix H:\n", H)

        predictor = CurveComplexityPredictor()
        difficulty = predictor.assess_curve_difficulty(cd, current_sections, prime_pool, H)

        height_bound = sconf['HEIGHT_BOUND']
        num_prime_subsets = int(sconf['NUM_PRIME_SUBSETS'])
        if difficulty['difficulty_score'] > 2.0:
            print(f"⚠️  WARNING: Curve predicted to be difficult (score {difficulty['difficulty_score']:.2f})")
            print(f"   Allocating {difficulty['recommended_height_multiplier']:.1f}x height bound")
            print(f"   Allocating {difficulty['recommended_subset_multiplier']:.1f}x prime subsets")
            height_bound *= difficulty['recommended_height_multiplier']
            num_prime_subsets = int(sconf['NUM_PRIME_SUBSETS'] * difficulty['recommended_subset_multiplier'])
        height_bound *= 2


        vecs = compute_search_vectors(H, height_bound) # MW canonical height bound version (old)
        #vecs = compute_search_vectors_hodge(cd, current_sections) # new and fancy hodge index theorem version
        vecs = canonicalize_by_sign(vecs) # we search by x value, so P and -P are the same! only keep one of them!
        print(f"Searching {len(vecs)} vectors up to height {height_bound}...")

        if not vecs:
            print("No search vectors found within height bound. Stopping.")
            break

        # ***** MODIFIED SECTION *****
        # The modular search is replaced with the direct symbolic search.
        if SYMBOLIC_SEARCH:
            newly_found_x, new_sections = search_lattice_symbolic(
                cd,
                current_sections,
                vecs,
                rhs_list=search_rhs_list,
                r_m=r_m,
                shift=shift,
                all_found_x=all_known_x,
                rationality_test_func=get_y_unshifted_genus2,
                stats=cumulative_stats # <-- Pass stats object
            )
            cumulative_stats.merge(iter_stats)
        else:

            # --- ALL CONFIG (PRIMES / SUBSETS / RESIDUE COUNTS) COMES FROM sconf ---
            prime_pool = sconf.get('PRIME_POOL', PRIME_POOL)
            residue_counts = sconf.get('RESIDUE_COUNTS', {})
 
            # --- MODIFIED CALL to get stats object ---
            newly_found_x, new_sections, precomputed_residues, iter_stats = search_lattice_modp_unified_parallel(
                cd, current_sections,
                prime_pool, height_bound,
                vecs,
                search_rhs_list,
                r_m,
                shift,
                all_known_x,
                num_prime_subsets,
                get_y_unshifted_genus2,
                sconf
                )
            # --- MERGE STATS ---
            cumulative_stats.merge(iter_stats)

            #if len(newly_found_x) < len(vecs) // 4:
            #if len(newly_found_x) < 1:
        # ***** END MODIFIED SECTION *****


        print(f"Found {len(newly_found_x)} new point(s) and {len(new_sections)} new section(s).")
        if newly_found_x:
            print(f"New x-coordinates: {newly_found_x}")

        all_known_x.update(newly_found_x)

        if len(all_known_x) >= TERMINATE_WHEN_6:
            break

        if not new_sections:
            break
        print("Augmenting Mordell-Weil basis with new sections.")
        current_sections.extend(new_sections)
        current_sections = lll_reduce_mw_basis(cd, list(set(current_sections)))
        iteration += 1

    # --- START: Restored Diagnostic Sections ---

    ### test that the base point is found by the search
    xtest = base_pts[0][0]  # This is the shifted x (equals r)
    xtest_unshifted = real_pts[0][0]  # This is the original x
    mtest = m_r(r=xtest)
    assert_base_m_found(mtest, xtest_unshifted, r_m, shift)

    # Replace the diagnostic section in search7_genus2.sage (after search completes)

    #### COMPLETENESS, HEURISTIC - CLEANED UP VERSION

    # REMOVE all the old scattered diagnostics and replace with this:

    # =====================================================================
    # PRIMARY COMPLETENESS REPORT
    # =====================================================================
    
   
    from stats import CompletenessAnalyzer, print_unified_completeness_report
    
    found_x_list = list(all_known_x)
    
    # This function now calls the REPAIRED CompletenessAnalyzer
    print_unified_completeness_report(
        stats=cumulative_stats,
        prime_pool=prime_pool,
        prime_subsets=cumulative_stats.prime_subsets,
        height_bound=height_bound,
        found_xs=found_x_list,
        r_m=r_m,
        shift=shift
    )


    # --- Run new unified diagnostics (empirical bootstrap + MI + subset checks) ---
    from stats import FindabilityAnalyzer, print_unified_diagnostics

    analyzer = FindabilityAnalyzer(cumulative_stats, prime_pool)
    # Choose bootstrap sampling parameters reasonably for your height_bound:
    # - small quick run: bootstrap_N=2000, max_num=2000, max_den=2000
    # - fuller run: bootstrap_N=5000..20000, max_num=10**4..10**5
    diag = print_unified_diagnostics(
        findability_analyzer=analyzer,
        prime_pool=prime_pool,
        prime_subsets=cumulative_stats.prime_subsets if hasattr(cumulative_stats, 'prime_subsets') else None,
        height_bound=height_bound,
        bootstrap_N=5000,
        bootstrap_max_num=min(10**4, max(10**3, int(2**(height_bound//2)))),
        bootstrap_max_den=min(10**4, max(10**3, int(2**(height_bound//3)))),
        mi_primes_limit=40,
        mi_N=2000
    )

    # diag contains raw bootstrap/mi/subset results if you want to further process them.



    # =====================================================================
    # OPTIONAL: DETAILED DIAGNOSTICS (for debugging only)
    # =====================================================================
    
    # --- DELETED Faulty Diagnostic Block ---
    # The block starting with "if DEBUG and False:" was here.
    # It was removed because it relied on the broken
    # compute_actual_subset_cover and compute_product_model functions.
    # The unified report above replaces it.
    
    # =====================================================================
    # POINT-SPECIFIC DIAGNOSTICS (if you suspect missing a specific point)
    # =====================================================================
    
    if TARGETED_X:
        print("\n" + "="*70)
 
        print(f"TARGETED POINT ANALYSIS: x = {TARGETED_X}")
        print("="*70)
        
        # We need a FindabilityAnalyzer instance. We can create one.
        from stats import FindabilityAnalyzer
        analyzer = FindabilityAnalyzer(cumulative_stats, prime_pool)

        # Compute target m from target x
        const = r_m(m=QQ(0))
        target_m = QQ(-1) * TARGETED_X + const
        
        print(f"Target m-value: {target_m}")
        
        
        # Visibility analysis
        sig = analyzer.visibility_signature(target_m)
        print(f"\nPer-prime visibility: {sig['matched']}/{sig['usable']} primes matched ({sig['fraction']:.1%})")
        
        # --- MODIFIED CHECK ---
        # Instead of checking 'crt_visible' (which was broken),
        # we check the 'fraction' of matching primes.
        if sig['fraction'] < 0.5: # Heuristic threshold
            print("\n⚠️  TARGET POINT HAS LOW VISIBILITY")
            print(f"This point is only visible at {sig['fraction']:.1%} of usable primes.")
 
            print("\nMatched primes:", [p for p, (r, ok) in sig['per_prime'].items() if ok])
            print("Missing primes (sample):", [p for p, (r, ok) in sig['per_prime'].items() if not ok][:10])
            
            # Suggest a targeted subset
            matched_primes = [p for p, (r, ok) in sig['per_prime'].items() if ok]
            if len(matched_primes) >= 4:
 
                suggested = matched_primes[:6]
                print(f"\nSuggested targeted subset: {suggested}")
                print("Add this to prime_subsets_to_process and re-run.")
        else:
            print(f"\n✓ Target point has high visibility ({sig['fraction']:.1%})")
            print("It should have been found. Check rationality_test_func or height bounds.")
    
    # =====================================================================
    # FINAL SUMMARY
    # =====================================================================
    
    print("\n" + "="*70)
    print("SEARCH SUMMARY")
    print("="*70)
    print(f"Total points found: {len(all_known_x)}")
    print(f"Total time: {float(cumulative_stats.summary()['elapsed']):.1f}s")
    print(f"CRT candidates tested: {cumulative_stats.counters['crt_candidates_found']:,}")
    print(f"Rationality tests: {cumulative_stats.counters['rationality_tests_total']:,}")
    hit_rate = float(cumulative_stats.counters['rationality_tests_success']) / max(1, float(cumulative_stats.counters['rationality_tests_total']))
    print(f"Hit rate: {100*hit_rate:.2f}%")
    print("="*70)


    # ---------- Insert this into search7_genus2.sage after you run the unified diagnostics ----------
    # It computes an arithmetic-informed prior and then produces posterior probabilities
    # for T = true number of rational points (T == k means "we found all").

    from stats import prior_from_arithmetic, completeness_posterior_geometric, bootstrap_visibility
    import math

    # Found / simple stats
    try:
        # found_x_list was used earlier when calling print_unified_completeness_report
        k_found = len(found_x_list)
    except Exception:
        # fallback to all_known_x if present
        try:
            k_found = len(all_known_x)
        except Exception:
            k_found = 0

    # try to extract bootstrap visibility p from diagnostic 'diag' if present,
    # otherwise run a small quick bootstrap (fast; adjustable).
    p_visibility = None
    if 'diag' in locals() and isinstance(diag, dict):
        try:
            p_visibility = float(diag['bootstrap']['avg_fraction'])
        except Exception:
            p_visibility = None

    if p_visibility is None:
        # run small quick bootstrap (keeps runtime minimal)
        try:
            analyzer = FindabilityAnalyzer(cumulative_stats, prime_pool)
            quick_boot = bootstrap_visibility(analyzer, N_samples=2000, max_num=2000, max_den=2000, seed=42)
            p_visibility = float(quick_boot['avg_fraction'])
            print(f"[fallback bootstrap] avg_fraction (p) = {p_visibility:.4f}")
        except Exception as e:
            print("Could not compute bootstrap visibility (fallback). Setting p_visibility = 0.0")
            p_visibility = 0.0

    # Gather arithmetic signals (pass None if missing; prior_from_arithmetic handles it)
    selmer_dim = None
    r_found = None
    # Try to pick up selmer info if present (your pipeline computes selmer later; if available use it)
    if 'selmer_results' in locals() and isinstance(selmer_results, dict):
        # try common keys
        try:
            r_found = len([_ for _ in base_sections]) if 'base_sections' in locals() else None
            selmer_dim = selmer_results.get('selmer_dim', None) or selmer_results.get('rank_bounds', {}).get('upper', None)
        except Exception:
            selmer_dim = None

    # CRT / rationality counters from cumulative_stats if available
    crt_candidates_found = None
    rationality_tests_success = None
    try:
        cnts = cumulative_stats.counters
        crt_candidates_found = cnts.get('crt_candidates_found', cnts.get('crt_candidates_tested', None))
        rationality_tests_success = cnts.get('rationality_tests_success', cnts.get('rationality_tests_total_success', None))
    except Exception:
        crt_candidates_found = None
        rationality_tests_success = None

    # Height info (optional). If you computed canonical heights for found non-torsion sections,
    # you can plug them in; otherwise leave None.
    h_max = None
    known_heights = None
    try:
        # If your completeness report computed height_bound earlier, reuse it
        if 'height_bound' in locals():
            h_max = float(height_bound)
        # Attempt to derive known canonical heights list if you tracked them (best-effort)
        if 'current_sections' in locals() and current_sections:
            # best-effort: attempt to compute naive heights if function exists
            try:
                known_heights = [float(cd.naive_height(sec)) for sec in current_sections]  # replace with your real function if different
            except Exception:
                known_heights = None
    except Exception:
        h_max = None
        known_heights = None

    # Call arithmetic-prior helper (your function in stats.py)
    prior = prior_from_arithmetic(
        k_found=k_found,
        p_visibility=p_visibility,
        selmer_dim=selmer_dim,
        r_found=r_found,
        crt_candidates_found=crt_candidates_found,
        rationality_tests_success=rationality_tests_success,
        h_max=h_max,
        known_heights=known_heights
    )

    # Print the derived prior components (transparent)
    print("\n--- Arithmetic-informed prior (components) ---")
    print(f"mu_selmer:   {prior['mu_selmer']:.4g}")
    print(f"mu_local:    {prior['mu_local']:.4g}")
    print(f"mu_height:   {prior['mu_height']:.4g}")
    print(f"mu_bootstrap:{prior['mu_bootstrap']:.4g}")
    print(f"--> mu_combined = {prior['mu_combined']:.4g}  (q = {prior['q']:.6f})")

    # Now compute posterior probabilities under the geometric prior
    # call completeness posterior function with a reasonable m_max (tune if you expect large tails)
    m_max = 200
    post = completeness_posterior_geometric(k=k_found, p=p_visibility, q=prior['q'], m_max=m_max)

    # Nicely formatted posterior summary
    P_all = post['P_all']
    P_all_but_1 = post['P_all_but_1']
    P_all_but_2 = post['P_all_but_2']
    mean_T = post['posterior_mean_T']

    print("\n--- Posterior summary (geometric prior from arithmetic signals) ---")
    print(f"Observed points (k) = {k_found}")
    print(f"Estimated detection probability p = {p_visibility:.3f}")
    print(f"P(true T == k)         = {P_all:.3%}   (probability we found all points)")
    print(f"P(true T <= k+1)       = {P_all_but_1:.3%}   (probability we missed ≤ 1 point)")
    print(f"P(true T <= k+2)       = {P_all_but_2:.3%}   (probability we missed ≤ 2 points)")
    print(f"Posterior mean of T    = {mean_T:.3f}")
    print("Note: results depend on prior (mu_combined) computed from arithmetic signals above.\n")

    # For auditability, optionally print the top posterior mass (T values with significant mass)
    top_items = sorted(post['posterior'].items(), key=lambda t: -t[1])[:8]
    print("Top posterior mass (T, prob):", ", ".join([f"{int(T)}:{prob:.3%}" for T,prob in top_items]))
    # --------------------------------------------------------------------------


    ### Automorphism Search ###
    print("\n--- Automorphism Search ---")
    try:
        if 'cd' not in locals() or 'm_sym' not in locals():
            print("Required objects 'cd' or 'm_sym' are not defined. Skipping automorphism search.") # Corrected print
        else:
            if not current_sections:
                print("No sections found; skipping automorphism search preserving fibration.")
            else:
                auts_report = compute_auts_preserving_fibration(cd, current_sections, m_sym)
                scaling_autos = auts_report.get('scaling_autos', [])
                if scaling_autos:
                    print(f"Found {len(scaling_autos)} scaling automorphisms:")
                    for i, aut in enumerate(scaling_autos):
                        print(f"  [{i}] Möbius map: (a,b,c,d) = {aut['mobius']}")
                        print(f"      Scaling factor u = {aut['u']}")
                else:
                    print("No scaling automorphisms found.")

                translation_autos = auts_report.get('translation_autos', [])
                if translation_autos:
                    print(f"Found {len(translation_autos)} translation automorphisms:")
                    for i, aut in enumerate(translation_autos):
                        print(f"  [{i}] Translation by Section S{aut['section_index']}")
                else:
                    print("No translation automorphisms found.")

            try:
                ns_auts, names = compute_ns_auts(singfibs, current_sections)
                if ns_auts:
                    print(f"\nFound {len(ns_auts)} automorphisms of the NS lattice.")
                    classified_ns_auts = classify_auts(ns_auts, names)
                    print("Classified NS lattice automorphisms:")
                    for i, (M, labels) in enumerate(classified_ns_auts):
                        print(f"  [{i}] Matrix:\n{M}")
                        print(f"      Labels: {labels}")
                else:
                    print("\nNo non-trivial automorphisms found for the NS lattice.")
            except Exception as ns_exc:
                print(f"NS lattice automorphism computation failed: {ns_exc}") # Corrected print

    except Exception as exc:
        print(f"An error occurred during automorphism computation: {exc}")
        raise

    ### Torsion Analysis ###
    print("\n--- Torsion Analysis ---")
    specs = good_specializations(cd, m_sym, max_try=40)
    print("Got", len(specs), "good specializations for torsion check.")
    orders = []
    for m0, E in specs:
        tors = E.torsion_subgroup()
        orders.append(tors.order())
    candidate_torsion_order = gcd(orders) if orders else 1
    print("Fast method candidate torsion order (GCD of specializations):", candidate_torsion_order)

    fiber_counts, lcm_bound = compute_fiber_lcm(cd)
    print("Fiber component counts:", fiber_counts, " -> lcm bound:", lcm_bound)

    for i, sec in enumerate(base_sections):
        is_torsion = False
        for n in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12]:
            if torsion_test(cd, sec, n, m_sym=m_sym, max_try=12):
                print(f"Base section {i} appears to be torsion of order dividing {n}.")
                is_torsion = True
                assert not is_torsion, base_sections
                break
        if not is_torsion:
            print(f"Base section {i} does not appear to be small order torsion.") # Corrected print

    ### Saturation Diagnostics ###
    run_saturation_checks(cd, current_sections, prime_pool[:30])

    # --- END: Restored Diagnostic Sections ---

    ### Picard Analysis (using corrected prime list) ###
    ell_candidates = [p for p in prime_pool if p not in cd.bad_primes][:10]
    print(f"\n--- Running Picard-Van Luijk Analysis ---")
    print(f"Using {len(ell_candidates)} good prime candidates for reduction: {ell_candidates}")

    picard_report = picard_via_van_luijk(
        cd,
        current_sections,
        prime_pool,
        ell_candidates=tuple(ell_candidates),
        verbose=True
    )
    print("\n=== Picard Report ===")
    print(f"Lower bound (char 0): {picard_report['lower_bound']}")
    print(f"Upper bounds from reductions:")
    for ub, info in picard_report['upper_bounds']:
        print(f"  ell={info['ell']}: rho <= {ub}    (rank_lb={info['rank_lb']}, Σ={info['sum_contrib']})")
    if picard_report['rho'] is not None:
        print(f"*** Concluded Picard number: ρ = {picard_report['rho']} ***")
    else:
        print("*** Picard not pinned exactly (likely off by ≤ 1). Consider the discriminant step. ***") # Corrected print


    ### Final Shioda-Tate Diagnostics ###
    rank_guess, details, diag = shioda_tate_from_fiber_list(
        singfibs['fibers'],
        rho_geom=picard_report['rho'],
        return_diagnostics=True,
        allow_auto_rho=True
    )
    print("\n--- Shioda-Tate Diagnostics ---")
    if rank_guess is None:
        print(f"Rank could not be determined. Reason: {diag.get('note', 'Unknown')}")
    else:
        print(f"Shioda-Tate Rank Estimate (assuming rho={picard_report['rho']}): {rank_guess}")

    print(f"Total Euler Characteristic: {euler}")
    print(f"Sum of Fiber Contributions (Σ(m_v - 1)): {details['sum_contributions']}")

    # --- After picard_report is computed and printed ---

    print("\n" + "="*70)
    print("RUNNING 2-SELMER ANALYSIS")
    print("="*70)

    selmer_results = run_selmer_analysis(
        cd,
        current_sections,
        picard_report['rho'],
        rank_guess,
        verbose=True
    )

    # Use results for candidate point hunting
    rank_upper = selmer_results['rank_bounds']['upper']
    print(f"\n*** Upper bound on S²(E/ℚ) rank: {rank_upper} ***")

    # ------------------------------------------------------------------
    # Explore explicit 2-coverings from Selmer candidates using the
    # DescentHomomorphism -> _construct_descent_quartic pipeline (no placeholders)
    # ------------------------------------------------------------------
    print("\n--- Constructing explicit 2-coverings for Selmer candidates ---")

    selmer_candidates = selmer_results.get('candidates', [])
    if selmer_candidates:
        u = polygen(QQ, 'u')

        for m_val in selmer_candidates:
            print(f"\n>>> Testing Selmer candidate m = {m_val}") # Corrected print
            try:
                # Specialize coefficients
                a4m = cd.a4(m=m_val)
                a6m = cd.a6(m=m_val)

                # Construct quartic using the Cremona-Tzanakis or Birch-Stephens form
                def quartic(u):
                    # Simple genus-1 quartic model (replace with correct formula)
                    return u**4 + a4m*u + a6m

                # Compute discriminant to flag bad primes
                disc_m = -16 * (4*a4m**3 + 27*a6m**2)

                # --- NEW HANDLING FOR disc_m = 0 ---
                if disc_m == 0:
                    print("  Quartic is singular (disc_m = 0) — it has a rational point and contributes to the rank.") # Corrected print
                    # The Selmer candidate m_val is guaranteed to be in the image of E(Q)
                    # You can skip further checks and move to the next candidate
                    # If you need to verify the rational point, you could use a separate function,
                    # but for a Selmer candidate, disc_m=0 is a positive signal.
                    continue # Skip the rest of the loop for this m_val
                # ------------------------------------

                bad_primes_m = [p for p in prime_factors(disc_m)]  # primes dividing discriminant

                # Combine with cd.bad_primes and filtered PRIME_POOL
                primes_to_check = sorted(set(
                    p for p in bad_primes_m + cd.bad_primes + [p for p in PRIME_POOL if is_good_prime_for_surface(cd, p)]
                    if p > 1 and is_prime(int(p))
                ))
                print("checking primes:", primes_to_check)


                # Check local solubility
                if is_everywhere_locally_solvable(quartic, primes_to_check):
                    print("  Locally solvable at all tested places ✅")
                    pt = search_rational_point_on_quartic(quartic, max_den=1000)
                    if pt:
                        print(f"  Found global rational point: {pt}")
                    else:
                        print("  No global point found up to bound — likely Sha candidate ⚠️")


                else:
                    print("  Locally obstructed ❌")

            except Exception as e:
                print(f"  [error constructing/checking quartic for m={m_val}]: {e}")
                raise


    # --- Yau-Zaslow Rational Curve Counts ---
    _, chi = compute_euler_and_chi(cd)
    if picard_report['rho'] is not None and chi != 1 and False: # skip for RES, trivial # disabled
        rho = picard_report['rho']
        print("\n--- Rational Curve Counts (Yau-Zaslow) ---")

        print("running with rho, mw rank, chi =", rho, rank_guess, chi)

        # Build the basis for the Neron-Severi lattice subspace.
        # Basis order is ['S', 'F', fiber_components...].
        basis_labels, Q, h_vec = build_ns_basis_and_Q(cd, rho, mw_rank=rank_guess, chi=chi)

        # Use a robust, staged search to find primitive vectors v with self-intersection -2,
        # which correspond to classes of rational curves on the surface.
        # The search escalates through different height and coordinate bounds to ensure results.
        mcoords = tuple(range(11, 75))
        counts, reps = staged_rational_curve_search(
            cd,
            current_sections,
            rho, rank_guess, chi,
            height_bounds=(15, 25, 35, 45, 55),
            max_coords=mcoords,
            node_cap=15_000_000,
            return_reps=True,
            require_S_coeff='positive', # Filter for physically meaningful curves (S-coefficient > 0)
            targeted_fallback=True
        )

        # pick sensible checkpoints
        runs = run_convergence_test(cd, current_sections, rho, chi, rank_guess,
                                     max_coords_seq=(20, 24, 28, 31), require_S_coeff='positive')

        print(f"Found rational curve classes in {len(counts)} different degrees.")
        print("Counts per degree:", counts)

        # Display sample representatives for the first few degrees found.
        print("\nSample representatives (first few degrees):") # Corrected print
        for d in sorted(reps.keys())[:6]:
            print(f"\nDegree {d}, count {counts.get(d, 0)}:")
            # Print up to 8 sample vectors for that degree, decoded into a human-readable format.
            for v in reps[d][:8]:
                print("   ", decode_vector(v, basis_labels))

        # Optional heuristic check to see which fiber component a section intersects.
        try:
            if current_sections:
                comp_map = detect_section_component(cd, current_sections[0])
                print("\nSection-component intersection map (heuristic):", comp_map)
        except Exception as e:
            print(f"\nCould not run section-component detection: {e}")
            raise


        # Construct the q-series (generating function) from the curve counts.
        qseries = build_qseries_from_counts(counts, rho, max_degree=20)
        print("\nQ-series (up to q^12):", qseries)


        # NS isotropic vector canonical height minimization analysis
        # cd = your CurveData object
        # current_sections = list of Weierstrass section points you already pass around

        # quick search with small coefficients
        cands = find_isotropic_fibration_candidates(cd, current_sections, rho, rank_guess, chi)
        # examine top candidates and evaluate height change
        results = evaluate_fibration_height_reduction(cd, current_sections, rho, rank_guess, chi, candidates=cands, shioda_sign=-1)

        # print top result
        if results:
            top = results[0]
            print("best coeffs:", top['a'])
            print("old heights:", top['heights_old'])
            print("new heights:", top['heights_new']) # Corrected print
            print("per-section delta (old-new):", top['delta'])
            print("total reduction:", top['total_reduction'])
        else:
            print("no isotropic candidates found in box")



    return all_known_x, cumulative_stats



@PROFILE
def main_genus2():
    initial_xs = DATA_PTS_GENUS2
    known_pts = { (QQ(x), get_y_unshifted_genus2(x)) for x in initial_xs if get_y_unshifted_genus2(x) is not None }
    # Add y=0 points to initial known points

    known_pts = add_y_zero_points_to_known(known_pts, COEFFS_GENUS2)
    print("known_pts start:", known_pts)

    excluded = set()
    all_found_x = {pt[0] for pt in known_pts}

    # --- Create Cumulative Stats Object ---
    cumulative_stats = SearchStats()

    while True:
        if len(known_pts) >= TERMINATE_WHEN_6:
            print(f"TERMINATE_WHEN_6 ({TERMINATE_WHEN_6}) reached.")
            break

        data_pts = get_data_pts(known_pts, excluded)
        if data_pts is None:
            print("All combinations of points have been checked.")
            break

        # y=0 skip; presents problems for this method
        skip = False
        for i in data_pts:
            if not i[1]:
                skip = True
                excluded.add(frozenset(data_pts))
                print("skipping:", data_pts, "due to the presence of y=0 point.")
                break
        if skip:
            continue

        print("\n========================================================")
        print(f"Constructing new fibration using: {data_pts}")
        print(f"known pts so far: {sorted(list(known_pts))}")
        print(f"found {len(known_pts)} / {TERMINATE_WHEN_6}")
        print("========================================================\n")

        # --- Pass cumulative_stats to doloop ---
        found_from_fibration, cumulative_stats = doloop_genus2(data_pts, COEFFS_GENUS2, all_found_x, cumulative_stats)
        all_found_x.update(found_from_fibration)

        excluded.add(frozenset(data_pts))
        known_pts = augment_known(known_pts, all_found_x, deg6=True)

    print("\n--- Cumulative Run Statistics ---")
    print(cumulative_stats.summary_string())

    print("\n--- Final Results ---")
    print(f"Final list of known points: {sorted(list(known_pts))}")





# In search7_genus2.sage

if __name__ == '__main__':
    main_genus2()
