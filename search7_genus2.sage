# This file was *autogenerated* from the file search7_genus2.sage
import itertools
from sage.all import *

# application modules
from search_common import *
from tate import *
from search_lll import *
from sat import *
from picard import *
from automorph import *
from torsion import *
from nslattice import *
from yau import *
from bounds import *
from sympy import symbols, expand
load('tower.sage')

# -------------------------
# Tower builder adapter for search (lightweight, deterministic, no tests)
# -------------------------
# --- Helper Functions ---

def find_y_zero_points_genus2(sextic_coeffs, verbose=True):
    """
    Find all rational points (x,0) on the genus 2 curve y^2 = G(x).
    For y=0, we need G(x) = 0.
    
    Args:
        sextic_coeffs: List of coefficients [a6, a5, a4, a3, a2, a1, a0] 
        verbose: Print debug information
    
    Returns:
        Set of rational points (x, 0) where G(x) = 0
    """
    if verbose:
        print("\n--- Searching for y=0 points ---")
    
    # Build the sextic polynomial G(x)
    R.<x> = QQ[]
    G = sum(a * x^(len(sextic_coeffs)-1-i) for i, a in enumerate(sextic_coeffs))
    
    if verbose:
        print(f"hyper elliptic curve: y^2(x) = {G}")
    
    # Find all rational roots
    rational_roots = G.roots(QQ, multiplicities=False)
    y_zero_points = {(QQ(root), QQ(0)) for root in rational_roots}
    
    if verbose:
        if y_zero_points:
            print(f"Found {len(y_zero_points)} points where y=0: {sorted(list(y_zero_points))}")
        else:
            print("No rational points found where y=0")
    
    return y_zero_points




# Simple integration into your existing code:
def add_y_zero_points_to_known(known_pts, sextic_coeffs):
    """Add y=0 points to the known points set."""
    y_zero_points = find_y_zero_points_genus2(sextic_coeffs)
    known_pts.update(y_zero_points)
    return known_pts

@PROFILE
def doloop_genus2(data_pts, sextic_coeffs, all_known_x):
    """Main search loop adapted for the genus-2 strategy."""

    # 1. Initial Setup and Shifting
    # at top of function
    SR_m = var('m')
    PR_m = PolynomialRing(QQ, 'm')
    m_poly = PR_m.gen()
    Fm = PR_m.fraction_field()

    R_x = PolynomialRing(QQ, 'x')
    x = R_x.gen()
    G = sum([a*x**(len(sextic_coeffs)-1 - i) for i, a in enumerate(sextic_coeffs)])

    real_pts = [(QQ(p[0]), QQ(p[1])) for p in data_pts if p[0] is not None]

    shift = QQ(0)
    while any((pt[0] + shift) == 0 for pt in real_pts):
        shift += 1
    if shift:
        print("")
        print("SHIFT =", shift)
        print("")

    shifted_G_poly = G(x - shift)
    base_pts = [(X + shift, Y) for X, Y in real_pts]

    # 2. Build the full fibration tower (deg 6 -> 5 -> 4)
    print("--- Building", len(sextic_coeffs)-5, "Step Fibration Tower (deg ", len(sextic_coeffs)-1, " -> 4 (-> 3)) ---")
    tower = iterate_tower(
        fx_PR=shifted_G_poly,
        pts_xy=base_pts,
        max_steps=len(sextic_coeffs)-5, # 5, because general quartics have 5 coefficients!
        seed_int=SEED_INT,
        verbose=DEBUG
    )
    # 3. Extract expressions from the tower
    roots = [i['r_expr'] for i in tower]
    E_rhs_m_symbolic = tower[-1]['f_i'] # The final quartic equation

    m_r = solve(SR(roots[0]) == var('r'), var('m'))[0].rhs() # for diagnostics

    # Convert the symbolic quartic into the required polynomial type for Sage's curve functions
    F_m = PR_m.fraction_field()
    r_m = SR(roots[0]) # get r_m for converting solutions back to x-coordinates
    print("r_m", r_m)
    R_x_m, x_poly = PolynomialRing(F_m, 'x').objgen()
    xSR, mSR = SR.var('x'), SR.var('m')

    coeffs_in_m = [SR(E_rhs_m_symbolic).coefficient(xSR, i) for i in range(SR(E_rhs_m_symbolic).degree(xSR) + 1)]
    coeffs_in_Fm = [F_m(c.subs({mSR: m_poly})) for c in coeffs_in_m]
    E_rhs_m = R_x_m(coeffs_in_Fm)

    # 4. Main Search Logic
    fibration_type = f"{len(set(pt[0] for pt in base_pts))}-point"
    print(f"--- Running {fibration_type} Fibration ---")

    E_curve_m, one, two, three = compute_morphism(E_rhs_m)

    lastrhs = E_rhs_m(x=roots[-1])
    last_phi_x = get_phi_x(one, two, three, roots[-1], lastrhs)
    cd = buildcd(E_curve_m, last_phi_x, lastrhs, E_rhs_m, (one, two, three))

    # in your main script, after cd is constructed:

    # 5. Auto-configure bounds and prime parameters from the constructed curve data
    import bounds

    print("\n=== Auto-configuring search parameters for this fibration ===")
    try:
        # Build known point list in (x,y) form for height estimation
        known_pts = [(pt[0], pt[1]) for pt in base_pts if pt[0] is not None]

        # Run auto configuration (defined in bounds.py)
        sconf = bounds.auto_configure_search(cd, known_pts, height_bound=HEIGHT_BOUND, debug=True)
        #sconf = bounds.auto_configure_search(cd, known_pts, debug=True)

        # Extract parameters into local scope
        #height_bound = max(sconf['HEIGHT_BOUND'], HEIGHT_BOUND)
        height_bound = sconf['HEIGHT_BOUND']
        PRIME_POOL = sconf['PRIME_POOL']
        MIN_PRIME_SUBSET_SIZE = sconf['MIN_PRIME_SUBSET_SIZE']
        MIN_MAX_PRIME_SUBSET_SIZE = sconf['MIN_MAX_PRIME_SUBSET_SIZE']
        NUM_PRIME_SUBSETS = sconf['NUM_PRIME_SUBSETS']
        MAX_MODULUS = sconf['MAX_MODULUS']
        TMAX = sconf['TMAX']

        print(f"[auto_cfg summary] height_bound={height_bound}, MAX_MODULUS={MAX_MODULUS}, "
              f"NUM_PRIME_SUBSETS={NUM_PRIME_SUBSETS}, PRIME_POOL size={len(PRIME_POOL)}, "
              f"TMAX={TMAX}")

    except Exception as e:
        print("[auto-config] Failed, reverting to static defaults:", e)
        raise

    # 6. Diagnostic check: confirm the modulus capping works safely
    _ = bounds.modulus_needed_from_canonical_height(370, scale_const=2.0,
                                                    max_modulus=MAX_MODULUS, debug=True)

    # Recompute prime pool with possible heavier diagnostics (optional)
    prime_pool = PRIME_POOL
    prime_pool = bounds.recommend_and_update_prime_pool(cd, run_heavy=True,
                                                        grh_fudge=10, debug=True)
    # Always keep 3, idk why, magic.  2 is bad tho.
    if 3 not in prime_pool:
        prime_pool = [3] + prime_pool
    PRIME_POOL = sorted(set(prime_pool))
    prime_pool = PRIME_POOL
    print(f"[prime_pool] Final pool has {len(PRIME_POOL)} primes up to {max(PRIME_POOL)}")

    # 7. Continue with your normal tower search logic (no other structural changes)
    k = cd.k_base_change
    n = cd.tate_exponent
    blowup = cd.blowup_factor
    m_sym = cd.a4.parent().gen()
    total_x_scale_factor = m_sym ** (2 * n - 2 * blowup)

    # --- BUILD search_rhs_list with symbolic SR consistency ---
    # canonical symbolic m
    SR_m = var('m')
    PR_m = cd.a4.parent()   # QQ[m] where m_sym comes from
    m_sym = PR_m.gen()

    # Ensure roots are SR objects
    roots_SR = [SR(r) for r in roots]

    print("DEBUG: roots (SR) repr/type samples:")
    for j, rr in enumerate(roots_SR[:4]):
        print(f"  root[{j}] = {rr}")

    # Start the search_rhs_list with the top-level phi_x (coerced to SR)
    try:
        search_rhs_list = [SR(cd.phi_x)]
    except Exception:
        search_rhs_list = [SR(str(cd.phi_x))]
        raise

    print("")
    print("RHS:", search_rhs_list[0])
    print("")


    # Build the remaining RHS for each tower-level double root
    for i in roots_SR[:-1]:
        break # new tower as all identical rhs's.
        # Evaluate the quartic at x = root i (symbolic)
        qrhs_r = SR(E_rhs_m_symbolic).subs({xSR: i})
        phi_x_r = get_phi_x(SR(one), SR(two), SR(three), i, qrhs_r)
        phi_x_r_SR = SR(phi_x_r)
        # apply base-change scaling and total_x_scale as before
        rhs_scaled_SR = (phi_x_r_SR.subs({m_sym: m_sym**k}) if k != 0 else phi_x_r_SR) / SR(str(total_x_scale_factor))
        search_rhs_list.append(rhs_scaled_SR)

    print("DEBUG: final search_rhs_list (SR reprs):")
    for j, rr in enumerate(search_rhs_list):
        print(f"  search_rhs_list[{j}] = {rr}")

    assert len(tower) == len(roots), (len(tower), roots)
    #assert len(tower) == len(search_rhs_list), (len(tower), len(search_rhs_list))

    summarize_fibration_info(cd, data_pts, base_pts)
    singfibs = cd.singfibs
    fibers = singfibs['fibers']
    euler = singfibs['euler_characteristic']
    sigma = singfibs['sigma_sum']
    singular_fibers_m = [f['r'] for f in fibers if f.get('root_type') == 'rational']

    cm_fibers = find_cm_fibers(cd)
    j_targets = [0, 1728, -12**3, -32**3, -96**3]
    special_fibers_m = find_special_j_invariant_fibers(cd, j_targets)
    test_fibers = {m for m in set(singular_fibers_m).union(cm_fibers).union(special_fibers_m) if m is not None}
    print("Testing special m-values from fibers:", test_fibers)

    found_from_fibers = test_y_rationality_genus2(list(test_fibers), r_m, shift)
    if found_from_fibers:
        print(f"Points found from special fibers: {found_from_fibers}")
        all_known_x.update(found_from_fibers)

    base_sections = compute_base_sections_m(cd, base_pts)
    verify_morphism_on_samples(cd, base_pts)
    if not base_sections:
        print("Could not compute base sections. Skipping search.")
        return set()
    base_sections = lll_reduce_mw_basis(cd, base_sections)
    current_sections = list(set(base_sections))

    iteration = 0
    while True:
        print(f"\n--- Search Iteration {iteration} with {len(current_sections)} sections for fibration {data_pts} ---")
        if not current_sections:
            print("No independent sections to search with. Stopping.")
            break

        independent, H = check_independence(current_sections, E_curve_m, cd)
        if not independent:
            print("Warning: Section basis is linearly dependent. Stopping search.")
            break
        print("Height Pairing Matrix H:\n", H)

        vecs = compute_search_vectors(H, height_bound) # MW canonical height bound version (old)
        #vecs = compute_search_vectors_hodge(cd, current_sections) # new and fancy hodge index theorem version
        vecs = canonicalize_by_sign(vecs) # we search by x value, so P and -P are the same!  only keep one of them!
        print(f"Searching {len(vecs)} vectors up to height {height_bound}...")

        if not vecs:
            print("No search vectors found within height bound. Stopping.")
            break

        # ***** MODIFIED SECTION *****
        # The modular search is replaced with the direct symbolic search.
        # Note that `prime_subsets` and `F` are no longer needed.
        if SYMBOLIC_SEARCH:
            newly_found_x, new_sections = search_lattice_symbolic(
                cd,
                current_sections,
                vecs,
                rhs_list=search_rhs_list,
                r_m=r_m,
                shift=shift,
                all_found_x=all_known_x,
                rationality_test_func=get_y_unshifted_genus2
            )
        else:

            # --- ALL CONFIG (PRIMES / SUBSETS / RESIDUE COUNTS) COMES FROM sconf ---
            # sconf was created earlier and contains:
            #   PRIME_POOL, RESIDUE_COUNTS, SUBSET_PLAN, PRIME_SUBSETS, NUM_PRIME_SUBSETS, ...
            prime_pool = sconf.get('PRIME_POOL', PRIME_POOL)  # fallback to earlier var if any
            residue_counts = sconf.get('RESIDUE_COUNTS', {})
            prime_subsets = sconf.get('PRIME_SUBSETS', None)

            # Defensive fallback: if bounds failed to produce actual subsets, do a tiny local build
            if prime_subsets is None:
                print("[doloop] WARNING: sconf returned no PRIME_SUBSETS; building a small fallback list.")
                # deterministic simple fallback: contiguous chunks of prime_pool
                prime_subsets = []
                L = len(prime_pool)
                if L == 0:
                    raise RuntimeError("Empty prime pool in both sconf and fallback.")
                # produce up to 100 subsets by sliding window
                window_min = max(3, min(3, L))
                window_max = min(9, L)
                for w in range(window_min, window_max+1):
                    for start in range(0, min(L, 100)):
                        block = prime_pool[start:start+w]
                        if len(block) == w:
                            prime_subsets.append(block)
                    if len(prime_subsets) >= 200:
                        break
                # dedupe & sort
                prime_subsets = sorted({tuple(s) for s in prime_subsets}, key=lambda t: (len(t), t))
                prime_subsets = [list(t) for t in prime_subsets]

            # show summary (no recomputation of strategy)
            from collections import Counter
            size_dist = Counter(len(s) for s in prime_subsets)
            print(f"[bounds] Using {len(prime_subsets)} precomputed prime subsets from sconf. Size dist: {dict(sorted(size_dist.items()))}")

            # call the modular LLL/subset search with precomputed subsets
            newly_found_x, new_sections = search_lattice_modp_lll_subsets(
                cd,
                current_sections,
                prime_pool,
                vecs,
                search_rhs_list,
                r_m,
                shift,
                all_known_x,
                prime_subsets,
                get_y_unshifted_genus2,
                TMAX
            )
        # ***** END MODIFIED SECTION *****


        print(f"Found {len(newly_found_x)} new point(s) and {len(new_sections)} new section(s).")
        if newly_found_x:
            print(f"New x-coordinates: {newly_found_x}")

        all_known_x.update(newly_found_x)

        if len(all_known_x) >= TERMINATE_WHEN_6:
            break

        if not new_sections:
            break
        print("Augmenting Mordell-Weil basis with new sections.")
        current_sections.extend(new_sections)
        current_sections = lll_reduce_mw_basis(cd, list(set(current_sections)))
        iteration += 1

    # --- START: Restored Diagnostic Sections ---
    sys.exit()

    ### test that the base point is found by the search
    xtest = base_pts[0][0]  # This is the shifted x (equals r)
    xtest_unshifted = real_pts[0][0]  # This is the original x
    mtest = m_r(r=xtest)
    assert_base_m_found(mtest, xtest_unshifted, r_m, shift)


    ### Automorphism Search ###
    print("\n--- Automorphism Search ---")
    try:
        if 'cd' not in locals() or 'm_sym' not in locals():
            print("Required objects 'cd' or 'm_sym' are not defined. Skipping automorphism search.")
        else:
            if not current_sections:
                print("No sections found; skipping automorphism search preserving fibration.")
            else:
                auts_report = compute_auts_preserving_fibration(cd, current_sections, m_sym)
                scaling_autos = auts_report.get('scaling_autos', [])
                if scaling_autos:
                    print(f"Found {len(scaling_autos)} scaling automorphisms:")
                    for i, aut in enumerate(scaling_autos):
                        print(f"  [{i}] Möbius map: (a,b,c,d) = {aut['mobius']}")
                        print(f"      Scaling factor u = {aut['u']}")
                else:
                    print("No scaling automorphisms found.")

                translation_autos = auts_report.get('translation_autos', [])
                if translation_autos:
                    print(f"Found {len(translation_autos)} translation automorphisms:")
                    for i, aut in enumerate(translation_autos):
                        print(f"  [{i}] Translation by Section S{aut['section_index']}")
                else:
                    print("No translation automorphisms found.")

            try:
                ns_auts, names = compute_ns_auts(singfibs, current_sections)
                if ns_auts:
                    print(f"\nFound {len(ns_auts)} automorphisms of the NS lattice.")
                    classified_ns_auts = classify_auts(ns_auts, names)
                    print("Classified NS lattice automorphisms:")
                    for i, (M, labels) in enumerate(classified_ns_auts):
                        print(f"  [{i}] Matrix:\n{M}")
                        print(f"      Labels: {labels}")
                else:
                    print("\nNo non-trivial automorphisms found for the NS lattice.")
            except Exception as ns_exc:
                print(f"NS lattice automorphism computation failed: {ns_exc}")

    except Exception as exc:
        print(f"An error occurred during automorphism computation: {exc}")
        raise

    ### Torsion Analysis ###
    print("\n--- Torsion Analysis ---")
    specs = good_specializations(cd, m_sym, max_try=40)
    print("Got", len(specs), "good specializations for torsion check.")
    orders = []
    for m0, E in specs:
        tors = E.torsion_subgroup()
        orders.append(tors.order())
    candidate_torsion_order = gcd(orders) if orders else 1
    print("Fast method candidate torsion order (GCD of specializations):", candidate_torsion_order)

    fiber_counts, lcm_bound = compute_fiber_lcm(cd)
    print("Fiber component counts:", fiber_counts, " -> lcm bound:", lcm_bound)

    for i, sec in enumerate(base_sections):
        is_torsion = False
        for n in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12]:
            if torsion_test(cd, sec, n, m_sym=m_sym, max_try=12):
                print(f"Base section {i} appears to be torsion of order dividing {n}.")
                is_torsion = True
                assert not is_torsion, base_sections
                break
        if not is_torsion:
            print(f"Base section {i} does not appear to be small order torsion.")

    ### Saturation Diagnostics ###
    run_saturation_checks(cd, current_sections, prime_pool[:30])

    # --- END: Restored Diagnostic Sections ---

    ### Picard Analysis (using corrected prime list) ###
    ell_candidates = [p for p in prime_pool if p not in cd.bad_primes][:10]
    print(f"\n--- Running Picard-Van Luijk Analysis ---")
    print(f"Using {len(ell_candidates)} good prime candidates for reduction: {ell_candidates}")

    picard_report = picard_via_van_luijk(
        cd,
        current_sections,
        prime_pool,
        ell_candidates=tuple(ell_candidates),
        verbose=True
    )
    print("\n=== Picard Report ===")
    print(f"Lower bound (char 0): {picard_report['lower_bound']}")
    print(f"Upper bounds from reductions:")
    for ub, info in picard_report['upper_bounds']:
        print(f"  ell={info['ell']}: rho <= {ub}    (rank_lb={info['rank_lb']}, Σ={info['sum_contrib']})")
    if picard_report['rho'] is not None:
        print(f"*** Concluded Picard number: ρ = {picard_report['rho']} ***")
    else:
        print("*** Picard not pinned exactly (likely off by ≤ 1). Consider the discriminant step. ***")


    ### Final Shioda-Tate Diagnostics ###
    rank_guess, details, diag = shioda_tate_from_fiber_list(
        singfibs['fibers'],
        rho_geom=picard_report['rho'],
        return_diagnostics=True,
        allow_auto_rho=True
    )
    print("\n--- Shioda-Tate Diagnostics ---")
    if rank_guess is None:
        print(f"Rank could not be determined. Reason: {diag.get('note', 'Unknown')}")
    else:
        print(f"Shioda-Tate Rank Estimate (assuming rho={picard_report['rho']}): {rank_guess}")

    print(f"Total Euler Characteristic: {euler}")
    print(f"Sum of Fiber Contributions (Σ(m_v - 1)): {details['sum_contributions']}")


    # --- Yau-Zaslow Rational Curve Counts ---
    _, chi = compute_euler_and_chi(cd)
    if picard_report['rho'] is not None and chi != 1 and False: # skip for RES, trivial # disabled
        rho = picard_report['rho']
        print("\n--- Rational Curve Counts (Yau-Zaslow) ---")

        print("running with rho, mw rank, chi =", rho, rank_guess, chi)

        # Build the basis for the Neron-Severi lattice subspace.
        # Basis order is ['S', 'F', fiber_components...].
        basis_labels, Q, h_vec = build_ns_basis_and_Q(cd, rho, mw_rank=rank_guess, chi=chi)

        # Use a robust, staged search to find primitive vectors v with self-intersection -2,
        # which correspond to classes of rational curves on the surface.
        # The search escalates through different height and coordinate bounds to ensure results.
        mcoords = tuple(range(11, 75))
        counts, reps = staged_rational_curve_search(
            cd,
            current_sections,
            rho, rank_guess, chi,
            height_bounds=(15, 25, 35, 45, 55),
            max_coords=mcoords,
            node_cap=15_000_000,
            return_reps=True,
            require_S_coeff='positive', # Filter for physically meaningful curves (S-coefficient > 0)
            targeted_fallback=True
        )

        # pick sensible checkpoints
        runs = run_convergence_test(cd, current_sections, rho, chi, rank_guess,
                                    max_coords_seq=(20, 24, 28, 31), require_S_coeff='positive')

        print(f"Found rational curve classes in {len(counts)} different degrees.")
        print("Counts per degree:", counts)

        # Display sample representatives for the first few degrees found.
        print("\nSample representatives (first few degrees):")
        for d in sorted(reps.keys())[:6]:
            print(f"\nDegree {d}, count {counts.get(d, 0)}:")
            # Print up to 8 sample vectors for that degree, decoded into a human-readable format.
            for v in reps[d][:8]:
                print("   ", decode_vector(v, basis_labels))

        # Optional heuristic check to see which fiber component a section intersects.
        try:
            if current_sections:
                comp_map = detect_section_component(cd, current_sections[0])
                print("\nSection-component intersection map (heuristic):", comp_map)
        except Exception as e:
            print(f"\nCould not run section-component detection: {e}")
            raise

        # Construct the q-series (generating function) from the curve counts.
        qseries = build_qseries_from_counts(counts, rho, max_degree=20)
        print("\nQ-series (up to q^12):", qseries)

    
        # NS isotropic vector canonical height minimization analysis
        # cd = your CurveData object
        # current_sections = list of Weierstrass section points you already pass around

        # quick search with small coefficients
        cands = find_isotropic_fibration_candidates(cd, current_sections, rho, rank_guess, chi)
        # examine top candidates and evaluate height change
        results = evaluate_fibration_height_reduction(cd, current_sections, rho, rank_guess, chi, candidates=cands, shioda_sign=-1)

        # print top result
        if results:
            top = results[0]
            print("best coeffs:", top['a'])
            print("old heights:", top['heights_old'])
            print("new heights:", top['heights_new'])
            print("per-section delta (old-new):", top['delta'])
            print("total reduction:", top['total_reduction'])
        else:
            print("no isotropic candidates found in box")



    return all_known_x

@PROFILE
def main_genus2():
    initial_xs = DATA_PTS_GENUS2
    known_pts = { (QQ(x), get_y_unshifted_genus2(x)) for x in initial_xs if get_y_unshifted_genus2(x) is not None }
    # Add y=0 points to initial known points
    known_pts = add_y_zero_points_to_known(known_pts, COEFFS_GENUS2)
    print("known_pts start:", known_pts)

    excluded = set()
    all_found_x = {pt[0] for pt in known_pts}

    while True:
        if len(known_pts) >= TERMINATE_WHEN_6:
            print(f"TERMINATE_WHEN_6 ({TERMINATE_WHEN_6}) reached.")
            break
        
        data_pts = get_data_pts(known_pts, excluded)
        if data_pts is None:
            print("All combinations of points have been checked.")
            break

        # y=0 skip; presents problems for this method
        skip = False
        for i in data_pts:
            if not i[1]:
                skip = True
                excluded.add(frozenset(data_pts))
                print("skipping:", data_pts, "due to the presence of y=0 point.")
                break
        if skip:
            continue

        print("\n========================================================")
        print(f"Constructing new fibration using: {data_pts}")
        print(f"known pts so far: {sorted(list(known_pts))}")
        print(f"found {len(known_pts)} / {TERMINATE_WHEN_6}")
        print("========================================================\n")

        found_from_fibration = doloop_genus2(data_pts, COEFFS_GENUS2, all_found_x)
        all_found_x.update(found_from_fibration)

        excluded.add(frozenset(data_pts))
        known_pts = augment_known(known_pts, all_found_x, deg6=True)

    print("\n--- Final Results ---")
    print(f"Final list of known points: {sorted(list(known_pts))}")

if __name__ == '__main__':
    main_genus2()
